diff -uNr linux-3.10.0-327.36.3.el7.x86_64/arch/Kconfig linux-3.10.0-327.36.3.el7.toa.x86_64/arch/Kconfig
--- linux-3.10.0-327.36.3.el7.x86_64/arch/Kconfig	2016-12-13 21:54:56.549000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/arch/Kconfig	2016-12-13 21:55:12.004000000 +0800
@@ -500,4 +500,11 @@
 config COMPAT_OLD_SIGACTION
 	bool
 
+config HOOKERS
+	tristate "Hooker service"
+	default m
+	help
+	  Allow replacing and restore the function pointer in any order.
+	  See include/linux/hookers.h for details.
+
 source "kernel/gcov/Kconfig"
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/arch/Kconfig.orig linux-3.10.0-327.36.3.el7.toa.x86_64/arch/Kconfig.orig
--- linux-3.10.0-327.36.3.el7.x86_64/arch/Kconfig.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/arch/Kconfig.orig	2016-12-13 21:55:12.005000000 +0800
@@ -0,0 +1,503 @@
+#
+# General architecture dependent options
+#
+
+config OPROFILE
+	tristate "OProfile system profiling"
+	depends on PROFILING
+	depends on HAVE_OPROFILE
+	select RING_BUFFER
+	select RING_BUFFER_ALLOW_SWAP
+	help
+	  OProfile is a profiling system capable of profiling the
+	  whole system, include the kernel, kernel modules, libraries,
+	  and applications.
+
+	  If unsure, say N.
+
+config OPROFILE_EVENT_MULTIPLEX
+	bool "OProfile multiplexing support (EXPERIMENTAL)"
+	default n
+	depends on OPROFILE && X86
+	help
+	  The number of hardware counters is limited. The multiplexing
+	  feature enables OProfile to gather more events than counters
+	  are provided by the hardware. This is realized by switching
+	  between events at an user specified time interval.
+
+	  If unsure, say N.
+
+config HAVE_OPROFILE
+	bool
+
+config OPROFILE_NMI_TIMER
+	def_bool y
+	depends on PERF_EVENTS && HAVE_PERF_EVENTS_NMI
+
+config KPROBES
+	bool "Kprobes"
+	depends on MODULES
+	depends on HAVE_KPROBES
+	select KALLSYMS
+	help
+	  Kprobes allows you to trap at almost any kernel address and
+	  execute a callback function.  register_kprobe() establishes
+	  a probepoint and specifies the callback.  Kprobes is useful
+	  for kernel debugging, non-intrusive instrumentation and testing.
+	  If in doubt, say "N".
+
+config JUMP_LABEL
+       bool "Optimize very unlikely/likely branches"
+       depends on HAVE_ARCH_JUMP_LABEL
+       help
+         This option enables a transparent branch optimization that
+	 makes certain almost-always-true or almost-always-false branch
+	 conditions even cheaper to execute within the kernel.
+
+	 Certain performance-sensitive kernel code, such as trace points,
+	 scheduler functionality, networking code and KVM have such
+	 branches and include support for this optimization technique.
+
+         If it is detected that the compiler has support for "asm goto",
+	 the kernel will compile such branches with just a nop
+	 instruction. When the condition flag is toggled to true, the
+	 nop will be converted to a jump instruction to execute the
+	 conditional block of instructions.
+
+	 This technique lowers overhead and stress on the branch prediction
+	 of the processor and generally makes the kernel faster. The update
+	 of the condition is slower, but those are always very rare.
+
+	 ( On 32-bit x86, the necessary options added to the compiler
+	   flags may increase the size of the kernel slightly. )
+
+config OPTPROBES
+	def_bool y
+	depends on KPROBES && HAVE_OPTPROBES
+	depends on !PREEMPT
+
+config KPROBES_ON_FTRACE
+	def_bool y
+	depends on KPROBES && HAVE_KPROBES_ON_FTRACE
+	depends on DYNAMIC_FTRACE_WITH_REGS
+	help
+	 If function tracer is enabled and the arch supports full
+	 passing of pt_regs to function tracing, then kprobes can
+	 optimize on top of function tracing.
+
+config UPROBES
+	bool "Transparent user-space probes (EXPERIMENTAL)"
+	depends on UPROBE_EVENT && PERF_EVENTS
+	default n
+	select PERCPU_RWSEM
+	help
+	  Uprobes is the user-space counterpart to kprobes: they
+	  enable instrumentation applications (such as 'perf probe')
+	  to establish unintrusive probes in user-space binaries and
+	  libraries, by executing handler functions when the probes
+	  are hit by user-space applications.
+
+	  ( These probes come in the form of single-byte breakpoints,
+	    managed by the kernel and kept transparent to the probed
+	    application. )
+
+	  If in doubt, say "N".
+
+config HAVE_64BIT_ALIGNED_ACCESS
+	def_bool 64BIT && !HAVE_EFFICIENT_UNALIGNED_ACCESS
+	help
+	  Some architectures require 64 bit accesses to be 64 bit
+	  aligned, which also requires structs containing 64 bit values
+	  to be 64 bit aligned too. This includes some 32 bit
+	  architectures which can do 64 bit accesses, as well as 64 bit
+	  architectures without unaligned access.
+
+	  This symbol should be selected by an architecture if 64 bit
+	  accesses are required to be 64 bit aligned in this way even
+	  though it is not a 64 bit architecture.
+
+	  See Documentation/unaligned-memory-access.txt for more
+	  information on the topic of unaligned memory accesses.
+
+config HAVE_EFFICIENT_UNALIGNED_ACCESS
+	bool
+	help
+	  Some architectures are unable to perform unaligned accesses
+	  without the use of get_unaligned/put_unaligned. Others are
+	  unable to perform such accesses efficiently (e.g. trap on
+	  unaligned access and require fixing it up in the exception
+	  handler.)
+
+	  This symbol should be selected by an architecture if it can
+	  perform unaligned accesses efficiently to allow different
+	  code paths to be selected for these cases. Some network
+	  drivers, for example, could opt to not fix up alignment
+	  problems with received packets if doing so would not help
+	  much.
+
+	  See Documentation/unaligned-memory-access.txt for more
+	  information on the topic of unaligned memory accesses.
+
+config ARCH_USE_BUILTIN_BSWAP
+       bool
+       help
+	 Modern versions of GCC (since 4.4) have builtin functions
+	 for handling byte-swapping. Using these, instead of the old
+	 inline assembler that the architecture code provides in the
+	 __arch_bswapXX() macros, allows the compiler to see what's
+	 happening and offers more opportunity for optimisation. In
+	 particular, the compiler will be able to combine the byteswap
+	 with a nearby load or store and use load-and-swap or
+	 store-and-swap instructions if the architecture has them. It
+	 should almost *never* result in code which is worse than the
+	 hand-coded assembler in <asm/swab.h>.  But just in case it
+	 does, the use of the builtins is optional.
+
+	 Any architecture with load-and-swap or store-and-swap
+	 instructions should set this. And it shouldn't hurt to set it
+	 on architectures that don't have such instructions.
+
+config KRETPROBES
+	def_bool y
+	depends on KPROBES && HAVE_KRETPROBES
+
+config USER_RETURN_NOTIFIER
+	bool
+	depends on HAVE_USER_RETURN_NOTIFIER
+	help
+	  Provide a kernel-internal notification when a cpu is about to
+	  switch to user mode.
+
+config HAVE_IOREMAP_PROT
+	bool
+
+config HAVE_KPROBES
+	bool
+
+config HAVE_KRETPROBES
+	bool
+
+config HAVE_OPTPROBES
+	bool
+
+config HAVE_KPROBES_ON_FTRACE
+	bool
+
+config HAVE_NMI_WATCHDOG
+	bool
+#
+# An arch should select this if it provides all these things:
+#
+#	task_pt_regs()		in asm/processor.h or asm/ptrace.h
+#	arch_has_single_step()	if there is hardware single-step support
+#	arch_has_block_step()	if there is hardware block-step support
+#	asm/syscall.h		supplying asm-generic/syscall.h interface
+#	linux/regset.h		user_regset interfaces
+#	CORE_DUMP_USE_REGSET	#define'd in linux/elf.h
+#	TIF_SYSCALL_TRACE	calls tracehook_report_syscall_{entry,exit}
+#	TIF_NOTIFY_RESUME	calls tracehook_notify_resume()
+#	signal delivery		calls tracehook_signal_handler()
+#
+config HAVE_ARCH_TRACEHOOK
+	bool
+
+config HAVE_DMA_ATTRS
+	bool
+
+config HAVE_DMA_CONTIGUOUS
+	bool
+
+config USE_GENERIC_SMP_HELPERS
+	bool
+
+config GENERIC_SMP_IDLE_THREAD
+       bool
+
+config GENERIC_IDLE_POLL_SETUP
+       bool
+
+# Select if arch init_task initializer is different to init/init_task.c
+config ARCH_INIT_TASK
+       bool
+
+# Select if arch has its private alloc_task_struct() function
+config ARCH_TASK_STRUCT_ALLOCATOR
+	bool
+
+# Select if arch has its private alloc_thread_info() function
+config ARCH_THREAD_INFO_ALLOCATOR
+	bool
+
+config HAVE_REGS_AND_STACK_ACCESS_API
+	bool
+	help
+	  This symbol should be selected by an architecure if it supports
+	  the API needed to access registers and stack entries from pt_regs,
+	  declared in asm/ptrace.h
+	  For example the kprobes-based event tracer needs this API.
+
+config HAVE_CLK
+	bool
+	help
+	  The <linux/clk.h> calls support software clock gating and
+	  thus are a key power management tool on many systems.
+
+config HAVE_DMA_API_DEBUG
+	bool
+
+config HAVE_HW_BREAKPOINT
+	bool
+	depends on PERF_EVENTS
+
+config HAVE_MIXED_BREAKPOINTS_REGS
+	bool
+	depends on HAVE_HW_BREAKPOINT
+	help
+	  Depending on the arch implementation of hardware breakpoints,
+	  some of them have separate registers for data and instruction
+	  breakpoints addresses, others have mixed registers to store
+	  them but define the access type in a control register.
+	  Select this option if your arch implements breakpoints under the
+	  latter fashion.
+
+config HAVE_USER_RETURN_NOTIFIER
+	bool
+
+config HAVE_PERF_EVENTS_NMI
+	bool
+	help
+	  System hardware can generate an NMI using the perf event
+	  subsystem.  Also has support for calculating CPU cycle events
+	  to determine how many clock cycles in a given period.
+
+config HAVE_PERF_REGS
+	bool
+	help
+	  Support selective register dumps for perf events. This includes
+	  bit-mapping of each registers and a unique architecture id.
+
+config HAVE_PERF_USER_STACK_DUMP
+	bool
+	help
+	  Support user stack dumps for perf event samples. This needs
+	  access to the user stack pointer which is not unified across
+	  architectures.
+
+config HAVE_ARCH_JUMP_LABEL
+	bool
+
+config HAVE_RCU_TABLE_FREE
+	bool
+
+config ARCH_HAVE_NMI_SAFE_CMPXCHG
+	bool
+
+config HAVE_ALIGNED_STRUCT_PAGE
+	bool
+	help
+	  This makes sure that struct pages are double word aligned and that
+	  e.g. the SLUB allocator can perform double word atomic operations
+	  on a struct page for better performance. However selecting this
+	  might increase the size of a struct page by a word.
+
+config HAVE_CMPXCHG_LOCAL
+	bool
+
+config HAVE_CMPXCHG_DOUBLE
+	bool
+
+config ARCH_WANT_IPC_PARSE_VERSION
+	bool
+
+config ARCH_WANT_COMPAT_IPC_PARSE_VERSION
+	bool
+
+config ARCH_WANT_OLD_COMPAT_IPC
+	select ARCH_WANT_COMPAT_IPC_PARSE_VERSION
+	bool
+
+config HAVE_ARCH_SECCOMP_FILTER
+	bool
+	help
+	  An arch should select this symbol if it provides all of these things:
+	  - syscall_get_arch()
+	  - syscall_get_arguments()
+	  - syscall_rollback()
+	  - syscall_set_return_value()
+	  - SIGSYS siginfo_t support
+	  - secure_computing is called from a ptrace_event()-safe context
+	  - secure_computing return value is checked and a return value of -1
+	    results in the system call being skipped immediately.
+
+config SECCOMP_FILTER
+	def_bool y
+	depends on HAVE_ARCH_SECCOMP_FILTER && SECCOMP && NET
+	help
+	  Enable tasks to build secure computing environments defined
+	  in terms of Berkeley Packet Filter programs which implement
+	  task-defined system call filtering polices.
+
+	  See Documentation/prctl/seccomp_filter.txt for details.
+
+config HAVE_CC_STACKPROTECTOR
+	bool
+	help
+	  An arch should select this symbol if:
+	  - its compiler supports the -fstack-protector option
+	  - it has implemented a stack canary (e.g. __stack_chk_guard)
+
+config CC_STACKPROTECTOR
+	def_bool n
+	help
+	  Set when a stack-protector mode is enabled, so that the build
+	  can enable kernel-side support for the GCC feature.
+
+choice
+	prompt "Stack Protector buffer overflow detection"
+	depends on HAVE_CC_STACKPROTECTOR
+	default CC_STACKPROTECTOR_NONE
+	help
+	  This option turns on the "stack-protector" GCC feature. This
+	  feature puts, at the beginning of functions, a canary value on
+	  the stack just before the return address, and validates
+	  the value just before actually returning.  Stack based buffer
+	  overflows (that need to overwrite this return address) now also
+	  overwrite the canary, which gets detected and the attack is then
+	  neutralized via a kernel panic.
+
+config CC_STACKPROTECTOR_NONE
+	bool "None"
+	help
+	  Disable "stack-protector" GCC feature.
+
+config CC_STACKPROTECTOR_REGULAR
+	bool "Regular"
+	select CC_STACKPROTECTOR
+	help
+	  Functions will have the stack-protector canary logic added if they
+	  have an 8-byte or larger character array on the stack.
+
+	  This feature requires gcc version 4.2 or above, or a distribution
+	  gcc with the feature backported ("-fstack-protector").
+
+	  On an x86 "defconfig" build, this feature adds canary checks to
+	  about 3% of all kernel functions, which increases kernel code size
+	  by about 0.3%.
+
+config CC_STACKPROTECTOR_STRONG
+	bool "Strong"
+	select CC_STACKPROTECTOR
+	help
+	  Functions will have the stack-protector canary logic added in any
+	  of the following conditions:
+
+	  - local variable's address used as part of the right hand side of an
+	    assignment or function argument
+	  - local variable is an array (or union containing an array),
+	    regardless of array type or length
+	  - uses register local variables
+
+	  This feature requires gcc version 4.9 or above, or a distribution
+	  gcc with the feature backported ("-fstack-protector-strong").
+
+	  On an x86 "defconfig" build, this feature adds canary checks to
+	  about 20% of all kernel functions, which increases the kernel code
+	  size by about 2%.
+
+endchoice
+
+config HAVE_CONTEXT_TRACKING
+	bool
+	help
+	  Provide kernel/user boundaries probes necessary for subsystems
+	  that need it, such as userspace RCU extended quiescent state.
+	  Syscalls need to be wrapped inside user_exit()-user_enter() through
+	  the slow path using TIF_NOHZ flag. Exceptions handlers must be
+	  wrapped as well. Irqs are already protected inside
+	  rcu_irq_enter/rcu_irq_exit() but preemption or signal handling on
+	  irq exit still need to be protected.
+
+config HAVE_VIRT_CPU_ACCOUNTING
+	bool
+
+config HAVE_IRQ_TIME_ACCOUNTING
+	bool
+	help
+	  Archs need to ensure they use a high enough resolution clock to
+	  support irq time accounting and then call enable_sched_clock_irqtime().
+
+config HAVE_ARCH_TRANSPARENT_HUGEPAGE
+	bool
+
+config HAVE_MOD_ARCH_SPECIFIC
+	bool
+	help
+	  The arch uses struct mod_arch_specific to store data.  Many arches
+	  just need a simple module loader without arch specific data - those
+	  should not enable this.
+
+config MODULES_USE_ELF_RELA
+	bool
+	help
+	  Modules only use ELF RELA relocations.  Modules with ELF REL
+	  relocations will give an error.
+
+config MODULES_USE_ELF_REL
+	bool
+	help
+	  Modules only use ELF REL relocations.  Modules with ELF RELA
+	  relocations will give an error.
+
+config HAVE_UNDERSCORE_SYMBOL_PREFIX
+	bool
+	help
+	  Some architectures generate an _ in front of C symbols; things like
+	  module loading and assembly files need to know about this.
+
+#
+# ABI hall of shame
+#
+config CLONE_BACKWARDS
+	bool
+	help
+	  Architecture has tls passed as the 4th argument of clone(2),
+	  not the 5th one.
+
+config CLONE_BACKWARDS2
+	bool
+	help
+	  Architecture has the first two arguments of clone(2) swapped.
+
+config CLONE_BACKWARDS3
+	bool
+	help
+	  Architecture has tls passed as the 3rd argument of clone(2),
+	  not the 5th one.
+
+config ODD_RT_SIGACTION
+	bool
+	help
+	  Architecture has unusual rt_sigaction(2) arguments
+
+config OLD_SIGSUSPEND
+	bool
+	help
+	  Architecture has old sigsuspend(2) syscall, of one-argument variety
+
+config OLD_SIGSUSPEND3
+	bool
+	help
+	  Even weirder antique ABI - three-argument sigsuspend(2)
+
+config OLD_SIGACTION
+	bool
+	help
+	  Architecture has old sigaction(2) syscall.  Nope, not the same
+	  as OLD_SIGSUSPEND | OLD_SIGSUSPEND3 - alpha has sigsuspend(2),
+	  but fairly different variant of sigaction(2), thanks to OSF/1
+	  compatibility...
+
+config COMPAT_OLD_SIGACTION
+	bool
+
+source "kernel/gcov/Kconfig"
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_conf.c linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_conf.c
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_conf.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_conf.c	2016-12-13 21:55:12.007000000 +0800
@@ -0,0 +1,1738 @@
+/****************************************************************************
+ *  flashcache_conf.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+struct cache_c *cache_list_head = NULL;
+struct work_struct _kcached_wq;
+u_int64_t size_hist[33];
+
+struct kmem_cache *_job_cache;
+mempool_t *_job_pool;
+struct kmem_cache *_pending_job_cache;
+mempool_t *_pending_job_pool;
+
+atomic_t nr_cache_jobs;
+atomic_t nr_pending_jobs;
+
+extern struct list_head *_pending_jobs;
+extern struct list_head *_io_jobs;
+extern struct list_head *_md_io_jobs;
+extern struct list_head *_md_complete_jobs;
+
+struct flashcache_control_s {
+	unsigned long synch_flags;
+};
+
+struct flashcache_control_s *flashcache_control;
+
+/* Bit offsets for wait_on_bit_lock() */
+#define FLASHCACHE_UPDATE_LIST		0
+
+static int flashcache_notify_reboot(struct notifier_block *this,
+				    unsigned long code, void *x);
+static void flashcache_sync_for_remove(struct cache_c *dmc);
+
+extern char *flashcache_sw_version;
+
+static int
+flashcache_wait_schedule(void *unused)
+{
+	schedule();
+	return 0;
+}
+
+static int 
+flashcache_jobs_init(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL, NULL);
+#else
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL);
+#endif
+	if (!_job_cache)
+		return -ENOMEM;
+
+	_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+	                           mempool_free_slab, _job_cache);
+	if (!_job_pool) {
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL, NULL);
+#else
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL);
+#endif
+	if (!_pending_job_cache) {
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	_pending_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+					   mempool_free_slab, _pending_job_cache);
+	if (!_pending_job_pool) {
+		kmem_cache_destroy(_pending_job_cache);
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void 
+flashcache_jobs_exit(void)
+{
+	VERIFY(flashcache_pending_empty());
+	VERIFY(flashcache_io_empty());
+	VERIFY(flashcache_md_io_empty());
+	VERIFY(flashcache_md_complete_empty());
+
+	mempool_destroy(_job_pool);
+	kmem_cache_destroy(_job_cache);
+	_job_pool = NULL;
+	_job_cache = NULL;
+	mempool_destroy(_pending_job_pool);
+	kmem_cache_destroy(_pending_job_cache);
+	_pending_job_pool = NULL;
+	_pending_job_cache = NULL;
+}
+
+static int 
+flashcache_kcached_init(struct cache_c *dmc)
+{
+	init_waitqueue_head(&dmc->destroyq);
+	atomic_set(&dmc->nr_jobs, 0);
+	atomic_set(&dmc->remove_in_prog, 0);
+	return 0;
+}
+
+/*
+ * Write out the metadata one sector at a time.
+ * Then dump out the superblock.
+ */
+static int 
+flashcache_writeback_md_store(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	int num_valid = 0, num_dirty = 0;
+	int error;
+	int write_errors = 0;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_writeback_md_store: Unable to allocate memory");
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata !");
+		return 1;
+	}	
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			num_valid++;
+		if (dmc->cache[i].cache_state & DIRTY)
+			num_dirty++;
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state & 
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/* 
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * 
+					MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+				if (error) {
+					write_errors++;
+					DMERR("flashcache_writeback_md_store: Could not write out cache metadata block %lu error %d !",
+					      where.sector, error);
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+		if (error) {
+			write_errors++;
+				DMERR("flashcache_writeback_md_store: Could not write out cache metadata block %lu error %d !",
+				      where.sector, error);
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_writeback_md_store" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_writeback_md_store: sector mismatch\n");
+	}
+
+	vfree((void *)meta_data_cacheblock);
+
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_writeback_md_store: Unable to allocate memory");
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata !");
+		return 1;
+	}	
+	memset(header, 0, MD_BLOCK_BYTES(dmc));
+	
+	/* Write the header out last */
+	if (write_errors == 0) {
+		if (num_dirty == 0)
+			header->cache_sb_state = CACHE_MD_STATE_CLEAN;
+		else
+			header->cache_sb_state = CACHE_MD_STATE_FASTCLEAN;			
+	} else
+		header->cache_sb_state = CACHE_MD_STATE_UNSTABLE;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+	
+	DPRINTK("Store metadata to disk: block size(%u), md block size(%u), cache size(%llu)" \
+	        "associativity(%u)",
+	        header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		write_errors++;
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata superblock %lu error %d !",
+		      where.sector, error);
+	}
+
+	vfree((void *)header);
+
+	if (write_errors == 0)
+		DMINFO("Cache metadata saved to disk");
+	else {
+		DMINFO("CRITICAL : There were %d errors in saving cache metadata saved to disk", 
+		       write_errors);
+		if (num_dirty)
+			DMINFO("CRITICAL : You have likely lost %d dirty blocks", num_dirty);
+	}
+
+	DMINFO("flashcache_writeback_md_store: valid blocks = %d dirty blocks = %d md_sectors = %d\n", 
+	       num_valid, num_dirty, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+
+	return 0;
+}
+
+static int 
+flashcache_writethrough_create(struct cache_c *dmc)
+{
+	sector_t cache_size, dev_size;
+	sector_t order;
+	int i;
+	
+	/* 
+	 * Convert size (in sectors) to blocks.
+	 * Then round size (in blocks now) down to a multiple of associativity 
+	 */
+	dmc->size /= dmc->block_size;
+	dmc->size = (dmc->size / dmc->assoc) * dmc->assoc;
+
+	/* Check cache size against device size */
+	dev_size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	cache_size = dmc->size * dmc->block_size;
+	if (cache_size > dev_size) {
+		DMERR("Requested cache size exeeds the cache device's capacity" \
+		      "(%lu>%lu)",
+  		      cache_size, dev_size);
+		return 1;
+	}
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%luB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       cache_size >> (20-SECTOR_SHIFT), dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		DMERR("flashcache_writethrough_create: Unable to allocate cache md");
+		return 1;
+	}
+	/* Initialize the cache structs */
+	for (i = 0; i < dmc->size ; i++) {
+		dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		dmc->cache[i].checksum = 0;
+#endif
+		dmc->cache[i].cache_state = INVALID;
+		dmc->cache[i].nr_queued = 0;
+	}
+	dmc->md_blocks = 0;
+	return 0;
+}
+
+static int 
+flashcache_writeback_create(struct cache_c *dmc, int force)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j, error;
+	sector_t cache_size, dev_size;
+	sector_t order;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+	
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_writeback_create: Unable to allocate sector");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Could not read cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;
+	}
+	if (!force &&
+	    ((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Existing Cache Detected, use force to re-create");
+		return 1;
+	}
+	/* Compute the size of the metadata, including header. 
+	   Note dmc->size is in raw sectors */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size / dmc->block_size) + 1 + 1;
+	dmc->size -= dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc);	/* total sectors available for cache */
+	dmc->size /= dmc->block_size;
+	dmc->size = (dmc->size / dmc->assoc) * dmc->assoc;	
+	/* Recompute since dmc->size was possibly trunc'ed down */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_writeback_create: md_blocks = %d, md_sectors = %d\n", 
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+	dev_size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	cache_size = dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + (dmc->size * dmc->block_size);
+	if (cache_size > dev_size) {
+		DMERR("Requested cache size exceeds the cache device's capacity" \
+		      "(%lu>%lu)",
+  		      cache_size, dev_size);
+		vfree((void *)header);
+		return 1;
+	}
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%luB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       cache_size >> (20-SECTOR_SHIFT), dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Unable to allocate cache md");
+		return 1;
+	}
+	/* Initialize the cache structs */
+	for (i = 0; i < dmc->size ; i++) {
+		dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		dmc->cache[i].checksum = 0;
+#endif
+		dmc->cache[i].cache_state = INVALID;
+		dmc->cache[i].nr_queued = 0;
+	}
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_writeback_create: Unable to allocate memory");
+		DMERR("flashcache_writeback_create: Could not write out cache metadata !");
+		return 1;
+	}	
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state & 
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/* 
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, 
+								 meta_data_cacheblock);
+				if (error) {
+					vfree((void *)header);
+					vfree((void *)meta_data_cacheblock);
+					vfree(dmc->cache);
+					DMERR("flashcache_writeback_create: Could not write cache metadata block %lu error %d !",
+					      where.sector, error);
+					return 1;
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next metadata block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+		if (error) {
+			vfree((void *)header);
+			vfree((void *)meta_data_cacheblock);
+			vfree(dmc->cache);
+			DMERR("flashcache_writeback_create: Could not write cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;		
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_writeback_create" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_writeback_create: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/* Write the header */
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	dmc->on_ssd_version = header->cache_version = FLASHCACHE_VERSION;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	
+	printk("flashcache-dbg: cachedev check - %s %s", header->cache_devname,
+				dmc->dm_vdevname);
+	
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_create: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;		
+	}
+	vfree((void *)header);
+	return 0;
+}
+
+static int 
+flashcache_writeback_load(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	u_int64_t size, slots_read;
+	int clean_shutdown;
+	int dirty_loaded = 0;
+	sector_t order, data_size;
+	int num_valid = 0;
+	int error;
+	int sectors_read = 0, sectors_expected = 0;	/* Debug */
+
+	/* 
+	 * We don't know what the preferred block size is, just read off 
+	 * the default md blocksize.
+	 */
+	header = (struct flash_superblock *)vmalloc(DEFAULT_MD_BLOCK_SIZE);
+	if (!header) {
+		DMERR("flashcache_writeback_load: Unable to allocate memory");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = DEFAULT_MD_BLOCK_SIZE;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Could not read cache superblock %lu error %d!",
+		      where.sector, error);
+		return 1;
+	}
+
+	if (header->cache_version == 1) {
+		/* Backwards compatibility, md was 512 bytes always in V1.0 */
+		header->md_block_size = 1;
+	} else if (header->cache_version > FLASHCACHE_VERSION) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Unknown version %d found in superblock!", header->cache_version);
+		return 1;
+	}
+	dmc->on_ssd_version = header->cache_version;
+		
+	DPRINTK("Loaded cache conf: version(%d), block size(%u), md block size(%u), cache size(%llu), " \
+	        "associativity(%u)",
+	        header->cache_version, header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+	if (!((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Corrupt Cache Superblock");
+		return 1;
+	}
+	if (header->cache_sb_state == CACHE_MD_STATE_DIRTY) {
+		DMINFO("Unclean Shutdown Detected");
+		printk(KERN_ALERT "Only DIRTY blocks exist in cache");
+		clean_shutdown = 0;
+	} else if (header->cache_sb_state == CACHE_MD_STATE_CLEAN) {
+		DMINFO("Slow (clean) Shutdown Detected");
+		printk(KERN_ALERT "Only CLEAN blocks exist in cache");
+		clean_shutdown = 1;
+	} else {
+		DMINFO("Fast (clean) Shutdown Detected");
+		printk(KERN_ALERT "Both CLEAN and DIRTY blocks exist in cache");
+		clean_shutdown = 1;
+	}
+	dmc->block_size = header->block_size;
+	dmc->md_block_size = header->md_block_size;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+	dmc->size = header->size;
+	dmc->assoc = header->assoc;
+	dmc->assoc_shift = ffs(dmc->assoc) - 1;
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_writeback_load: md_blocks = %d, md_sectors = %d, md_block_size = %d\n", 
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc), dmc->md_block_size);
+	data_size = dmc->size * dmc->block_size;
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%ldB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       (dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + data_size) >> (20-SECTOR_SHIFT), 
+	       dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		DMERR("load_metadata: Unable to allocate memory");
+		vfree((void *)header);
+		return 1;
+	}
+	/* Read the metadata in large blocks and populate incore state */
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_load: Unable to allocate memory");
+		return 1;
+	}
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	size = dmc->size;
+	i = 0;
+	while (size > 0) {
+		slots_read = min(size, (u_int64_t)(MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)));
+		if (slots_read % MD_SLOTS_PER_BLOCK(dmc))
+			where.count = (1 + (slots_read / MD_SLOTS_PER_BLOCK(dmc))) * MD_SECTORS_PER_BLOCK(dmc);
+		else
+			where.count = (slots_read / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		sectors_read += where.count;	/* Debug */
+		error = flashcache_dm_io_sync_vm(dmc, &where, READ, meta_data_cacheblock);
+		if (error) {
+			vfree((void *)header);
+			vfree(dmc->cache);
+			vfree((void *)meta_data_cacheblock);
+			DMERR("flashcache_writeback_load: Could not read cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;
+		}
+		where.sector += where.count;
+		next_ptr = meta_data_cacheblock;
+		for (j = 0 ; j < slots_read ; j++) {
+			/*
+			 * XXX - Now that we force each on-ssd metadata cache slot to be a ^2, where
+			 * we are guaranteed that the slots will exactly fit within a sector (and 
+			 * a metadata block), we can simplify this logic. We don't need this next test.
+			 */
+			if ((j % MD_SLOTS_PER_BLOCK(dmc)) == 0) {
+				/* Move onto next block */
+				next_ptr = (struct flash_cacheblock *)
+					((caddr_t)meta_data_cacheblock + MD_BLOCK_BYTES(dmc) * (j / MD_SLOTS_PER_BLOCK(dmc)));
+			}
+			dmc->cache[i].nr_queued = 0;
+			/* 
+			 * If unclean shutdown, only the DIRTY blocks are loaded.
+			 */
+			if (clean_shutdown || (next_ptr->cache_state & DIRTY)) {
+				if (next_ptr->cache_state & DIRTY)
+					dirty_loaded++;
+				dmc->cache[i].cache_state = next_ptr->cache_state;
+				VERIFY((dmc->cache[i].cache_state & (VALID | INVALID)) 
+				       != (VALID | INVALID));
+				if (dmc->cache[i].cache_state & VALID)
+					num_valid++;
+				dmc->cache[i].dbn = next_ptr->dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				if (clean_shutdown)
+					dmc->cache[i].checksum = next_ptr->checksum;
+				else {
+					error = flashcache_read_compute_checksum(dmc, i, block);
+					if (error) {
+						vfree((void *)header);
+						vfree(dmc->cache);
+						vfree((void *)meta_data_cacheblock);
+						DMERR("flashcache_writeback_load: Could not read cache metadata block %lu error %d !",
+						      dmc->cache[i].dbn, error);
+						return 1;				
+					}						
+				}
+#endif
+			} else {
+				dmc->cache[i].cache_state = INVALID;
+				dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				dmc->cache[i].checksum = 0;
+#endif
+			}
+			next_ptr++;
+			i++;
+		}
+		size -= slots_read;
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_read) {
+		printk("flashcache_writeback_load" "Sector Mismatch ! sectors_expected=%d, sectors_read=%d\n",
+		       sectors_expected, sectors_read);
+		panic("flashcache_writeback_load: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/*
+	 * For writing the superblock out, use the preferred blocksize that 
+	 * we read from the superblock above.
+	 */
+	if (DEFAULT_MD_BLOCK_SIZE != dmc->md_block_size) {
+		vfree((void *)header);
+		header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+		if (!header) {
+			DMERR("flashcache_writeback_load: Unable to allocate memory");
+			return 1;
+		}
+	}	
+	/* Before we finish loading, we need to dirty the superblock and 
+	   write it out */
+	header->size = dmc->size;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->assoc = dmc->assoc;
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_load: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;		
+	}
+	vfree((void *)header);
+	DMINFO("flashcache_writeback_load: Cache metadata loaded from disk with %d valid %d DIRTY blocks", 
+	       num_valid, dirty_loaded);
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+static void
+flashcache_clean_all_sets(void *data)
+{
+	struct cache_c *dmc = (struct cache_c *)data;
+#else
+static void
+flashcache_clean_all_sets(struct work_struct *work)
+{
+	struct cache_c *dmc = container_of(work, struct cache_c, 
+					   delayed_clean.work);
+#endif
+	int i;
+	
+	for (i = 0 ; i < dmc->num_sets ; i++)
+		flashcache_clean_set(dmc, i);
+}
+
+static int inline
+flashcache_get_dev(struct dm_target *ti, char *pth, struct dm_dev **dmd,
+		   char *dmc_dname, sector_t tilen)
+{
+	int rc;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34)
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else
+#if defined(RHEL_MAJOR) && RHEL_MAJOR == 6
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else 
+	rc = dm_get_device(ti, pth, 0, tilen,
+			   dm_table_get_mode(ti->table), dmd);
+#endif
+#endif
+	if (!rc)
+		strncpy(dmc_dname, pth, DEV_PATHLEN);
+	return rc;
+}
+
+/*
+ * Construct a cache mapping.
+ *  arg[0]: path to source device
+ *  arg[1]: path to cache device
+ *  arg[2]: md virtual device name
+ *  arg[3]: cache mode (from flashcache.h)
+ *  arg[4]: cache persistence (if set, cache conf is loaded from disk)
+ * Cache configuration parameters (if not set, default values are used.
+ *  arg[5]: cache block size (in sectors)
+ *  arg[6]: cache size (in blocks)
+ *  arg[7]: cache associativity
+ *  arg[8]: md block size (in sectors)
+ */
+int 
+flashcache_ctr(struct dm_target *ti, unsigned int argc, char **argv)
+{
+	struct cache_c *dmc;
+	sector_t i, order;
+	int r = -EINVAL;
+	int persistence = 0;
+	
+	if (argc < 3) {
+		ti->error = "flashcache: Need at least 3 arguments";
+		goto bad;
+	}
+
+	dmc = kzalloc(sizeof(*dmc), GFP_KERNEL);
+	if (dmc == NULL) {
+		ti->error = "flashcache: Failed to allocate cache context";
+		r = ENOMEM;
+		goto bad;
+	}
+
+	dmc->tgt = ti;
+	if ((r = flashcache_get_dev(ti, argv[0], &dmc->disk_dev, 
+				    dmc->disk_devname, ti->len))) {
+		if (r == -EBUSY)
+			ti->error = "flashcache: Disk device is busy, cannot create cache";
+		else
+			ti->error = "flashcache: Disk device lookup failed";
+		goto bad1;
+	}
+	if ((r = flashcache_get_dev(ti, argv[1], &dmc->cache_dev,
+				    dmc->cache_devname, 0))) {
+		if (r == -EBUSY)
+			ti->error = "flashcache: Cache device is busy, cannot create cache";
+		else
+			ti->error = "flashcache: Cache device lookup failed";
+		goto bad2;
+	}
+
+	if (sscanf(argv[2], "%s", (char *)&dmc->dm_vdevname) != 1) {
+		ti->error = "flashcache: Virtual device name lookup failed";
+		goto bad3;
+	}
+	
+	r = flashcache_kcached_init(dmc);
+	if (r) {
+		ti->error = "Failed to initialize kcached";
+		goto bad3;
+	}
+
+	if (sscanf(argv[3], "%u", &dmc->cache_mode) != 1) {
+		ti->error = "flashcache: sscanf failed, invalid cache mode";
+		r = -EINVAL;
+		goto bad3;
+	}
+	if (dmc->cache_mode < FLASHCACHE_WRITE_BACK || 
+	    dmc->cache_mode > FLASHCACHE_WRITE_AROUND) {
+		DMERR("cache_mode = %d", dmc->cache_mode);
+		ti->error = "flashcache: Invalid cache mode";
+		r = -EINVAL;
+		goto bad3;
+	}
+	
+	/* 
+	 * XXX - Persistence is totally ignored for write through and write around.
+	 * Maybe this should really be moved to the end of the param list ?
+	 */
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		if (argc >= 5) {
+			if (sscanf(argv[4], "%u", &persistence) != 1) {
+				ti->error = "flashcache: sscanf failed, invalid cache persistence";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (persistence < CACHE_RELOAD || persistence > CACHE_FORCECREATE) {
+				DMERR("persistence = %d", persistence);
+				ti->error = "flashcache: Invalid cache persistence";
+				r = -EINVAL;
+				goto bad3;
+			}			
+		}
+		if (persistence == CACHE_RELOAD) {
+			if (flashcache_writeback_load(dmc)) {
+				ti->error = "flashcache: Cache reload failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+			goto init; /* Skip reading cache parameters from command line */
+		}
+	} else
+		persistence = CACHE_CREATE;
+
+	if (argc >= 6) {
+		if (sscanf(argv[5], "%u", &dmc->block_size) != 1) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad3;
+		}
+		if (!dmc->block_size || (dmc->block_size & (dmc->block_size - 1))) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+	
+	if (!dmc->block_size)
+		dmc->block_size = DEFAULT_BLOCK_SIZE;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+
+	/* dmc->size is specified in sectors here, and converted to blocks later */
+	if (argc >= 7) {
+		if (sscanf(argv[6], "%lu", &dmc->size) != 1) {
+			ti->error = "flashcache: Invalid cache size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+	
+	if (!dmc->size)
+		dmc->size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+
+	if (argc >= 8) {
+		if (sscanf(argv[7], "%u", &dmc->assoc) != 1) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad3;
+		}
+		if (!dmc->assoc || (dmc->assoc & (dmc->assoc - 1)) ||
+		    dmc->assoc > FLASHCACHE_MAX_ASSOC ||
+		    dmc->assoc < FLASHCACHE_MIN_ASSOC ||
+		    dmc->size < dmc->assoc) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+
+	if (!dmc->assoc)
+		dmc->assoc = DEFAULT_CACHE_ASSOC;
+	dmc->assoc_shift = ffs(dmc->assoc) - 1;
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		if (argc >= 9) {
+			if (sscanf(argv[8], "%u", &dmc->md_block_size) != 1) {
+				ti->error = "flashcache: Invalid metadata block size";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (!dmc->md_block_size || (dmc->md_block_size & (dmc->md_block_size - 1)) ||
+			    dmc->md_block_size > FLASHCACHE_MAX_MD_BLOCK_SIZE) {
+				ti->error = "flashcache: Invalid metadata block size";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (dmc->assoc < 
+			    (dmc->md_block_size * 512 / sizeof(struct flash_cacheblock))) {
+				ti->error = "flashcache: Please choose a smaller metadata block size or larger assoc";
+				r = -EINVAL;
+				goto bad3;
+			}
+		}
+
+		if (!dmc->md_block_size)
+			dmc->md_block_size = DEFAULT_MD_BLOCK_SIZE;
+
+		if (dmc->md_block_size * 512 < dmc->cache_dev->bdev->bd_block_size) {
+			ti->error = "flashcache: Metadata block size must be >= cache device sector size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {	
+		if (persistence == CACHE_CREATE) {
+			if (flashcache_writeback_create(dmc, 0)) {
+				ti->error = "flashcache: Cache Create Failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+		} else {
+			if (flashcache_writeback_create(dmc, 1)) {
+				ti->error = "flashcache: Cache Force Create Failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+		}
+	} else
+		flashcache_writethrough_create(dmc);
+
+init:
+	dmc->num_sets = dmc->size >> dmc->assoc_shift;
+	order = dmc->num_sets * sizeof(struct cache_set);
+	dmc->cache_sets = (struct cache_set *)vmalloc(order);
+	if (!dmc->cache_sets) {
+		ti->error = "Unable to allocate memory";
+		r = -ENOMEM;
+		vfree((void *)dmc->cache);
+		goto bad3;
+	}				
+
+	for (i = 0 ; i < dmc->num_sets ; i++) {
+		dmc->cache_sets[i].set_fifo_next = i * dmc->assoc;
+		dmc->cache_sets[i].set_clean_next = i * dmc->assoc;
+		dmc->cache_sets[i].nr_dirty = 0;
+		dmc->cache_sets[i].clean_inprog = 0;
+		dmc->cache_sets[i].dirty_fallow = 0;
+		dmc->cache_sets[i].fallow_tstamp = jiffies;
+		dmc->cache_sets[i].fallow_next_cleaning = jiffies;
+		dmc->cache_sets[i].lru_tail = FLASHCACHE_LRU_NULL;
+		dmc->cache_sets[i].lru_head = FLASHCACHE_LRU_NULL;
+	}
+
+	/* Push all blocks into the set specific LRUs */
+	for (i = 0 ; i < dmc->size ; i++) {
+		dmc->cache[i].lru_prev = FLASHCACHE_LRU_NULL;
+		dmc->cache[i].lru_next = FLASHCACHE_LRU_NULL;
+		flashcache_reclaim_lru_movetail(dmc, i);
+	}
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		order = (dmc->md_blocks - 1) * sizeof(struct cache_md_block_head);
+		dmc->md_blocks_buf = (struct cache_md_block_head *)vmalloc(order);
+		if (!dmc->md_blocks_buf) {
+			ti->error = "Unable to allocate memory";
+			r = -ENOMEM;
+			vfree((void *)dmc->cache);
+			vfree((void *)dmc->cache_sets);
+			goto bad3;
+		}		
+
+		for (i = 0 ; i < dmc->md_blocks - 1 ; i++) {
+			dmc->md_blocks_buf[i].nr_in_prog = 0;
+			dmc->md_blocks_buf[i].queued_updates = NULL;
+		}
+	}
+
+	spin_lock_init(&dmc->cache_spin_lock);
+
+	dmc->sync_index = 0;
+	dmc->clean_inprog = 0;
+
+	ti->split_io = dmc->block_size;
+	ti->private = dmc;
+
+	/* Cleaning Thresholds */
+	dmc->sysctl_dirty_thresh = DIRTY_THRESH_DEF;
+	dmc->dirty_thresh_set = (dmc->assoc * dmc->sysctl_dirty_thresh) / 100;
+	dmc->max_clean_ios_total = MAX_CLEAN_IOS_TOTAL;
+	dmc->max_clean_ios_set = MAX_CLEAN_IOS_SET;
+
+	/* Other sysctl defaults */
+	dmc->sysctl_io_latency_hist = 0;
+	dmc->sysctl_do_sync = 0;
+	dmc->sysctl_stop_sync = 0;
+	dmc->sysctl_pid_do_expiry = 0;
+	dmc->sysctl_max_pids = MAX_PIDS;
+	dmc->sysctl_pid_expiry_secs = PID_EXPIRY_SECS;
+	dmc->sysctl_reclaim_policy = FLASHCACHE_FIFO;
+	dmc->sysctl_zerostats = 0;
+	dmc->sysctl_error_inject = 0;
+	dmc->sysctl_fast_remove = 0;
+	dmc->sysctl_cache_all = 1;
+	dmc->sysctl_fallow_clean_speed = FALLOW_CLEAN_SPEED;
+	dmc->sysctl_fallow_delay = FALLOW_DELAY;
+	dmc->sysctl_skip_seq_thresh_kb = SKIP_SEQUENTIAL_THRESHOLD;
+
+	/* Sequential i/o spotting */	
+	for (i = 0; i < SEQUENTIAL_TRACKER_QUEUE_DEPTH; i++) {
+		dmc->seq_recent_ios[i].most_recent_sector = 0;
+		dmc->seq_recent_ios[i].sequential_count = 0;
+		dmc->seq_recent_ios[i].prev = (struct sequential_io *)NULL;
+		dmc->seq_recent_ios[i].next = (struct sequential_io *)NULL;
+		seq_io_move_to_lruhead(dmc, &dmc->seq_recent_ios[i]);
+	}
+	dmc->seq_io_tail = &dmc->seq_recent_ios[0];
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, TASK_UNINTERRUPTIBLE);
+	dmc->next_cache = cache_list_head;
+	cache_list_head = dmc;
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			dmc->cached_blocks++;
+		if (dmc->cache[i].cache_state & DIRTY) {
+			dmc->cache_sets[i / dmc->assoc].nr_dirty++;
+			dmc->nr_dirty++;
+		}
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&dmc->delayed_clean, flashcache_clean_all_sets, dmc);
+#else
+	INIT_DELAYED_WORK(&dmc->delayed_clean, flashcache_clean_all_sets);
+#endif
+
+	dmc->whitelist_head = NULL;
+	dmc->whitelist_tail = NULL;
+	dmc->blacklist_head = NULL;
+	dmc->blacklist_tail = NULL;
+	dmc->num_whitelist_pids = 0;
+	dmc->num_blacklist_pids = 0;
+
+	flashcache_ctr_procfs(dmc);
+
+	return 0;
+
+bad3:
+	dm_put_device(ti, dmc->cache_dev);
+bad2:
+	dm_put_device(ti, dmc->disk_dev);
+bad1:
+	kfree(dmc);
+bad:
+	return r;
+}
+
+static void
+flashcache_dtr_stats_print(struct cache_c *dmc)
+{
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+	u_int64_t  cache_pct, dirty_pct;
+	char *cache_mode;
+	int i;
+	
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	
+	DMINFO("stats: \n\treads(%lu), writes(%lu)", stats->reads, stats->writes);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" \
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n" ,
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->dirty_write_hits, dirty_write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tmetadata dirties(%lu), metadata cleans(%lu)\n" \
+		       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+		       "\tcleanings(%lu) fallow cleanings(%lu)\n"	\
+		       "\tno room(%lu) front merge(%lu) back merge(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->md_write_dirty, stats->md_write_clean,
+		       stats->md_write_batch, stats->md_ssd_writes,
+		       stats->cleanings, stats->fallow_cleanings, 
+		       stats->noroom, stats->front_merge, stats->back_merge);
+	} else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\treplacement(%lu)\n"				\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->replace,
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	} else 	{	/* WRITE_AROUND */
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\treplacement(%lu)\n"				\
+		       "\tinvalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->replace,
+		       stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	}
+	/* All modes */
+        DMINFO("\tdisk reads(%lu), disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+               "\tuncached reads(%lu), uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tuncached sequential reads(%lu), uncached sequential writes(%lu)\n" \
+               "\tpid_adds(%lu), pid_dels(%lu), pid_drops(%lu) pid_expiry(%lu)",
+               stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+               stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->uncached_sequential_reads, stats->uncached_sequential_writes,
+               stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	if (dmc->size > 0) {
+		dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+		cache_pct = ((u_int64_t)dmc->cached_blocks * 100) / dmc->size;
+	} else {
+		cache_pct = 0;
+		dirty_pct = 0;
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		cache_mode = "WRITE_BACK";
+	else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+		cache_mode = "WRITE_THROUGH";
+	else
+		cache_mode = "WRITE_AROUND";
+	DMINFO("conf:\n"						\
+	       "\tvirt dev (%s), ssd dev (%s), disk dev (%s) cache mode(%s)\n"		\
+	       "\tcapacity(%luM), associativity(%u), data block size(%uK) metadata block size(%ub)\n" \
+	       "\tskip sequential thresh(%uK)\n" \
+	       "\ttotal blocks(%lu), cached blocks(%lu), cache percent(%d)\n" \
+	       "\tdirty blocks(%d), dirty percent(%d)\n",
+	       dmc->dm_vdevname, dmc->cache_devname, dmc->disk_devname,
+	       cache_mode,
+	       dmc->size*dmc->block_size>>11, dmc->assoc,
+	       dmc->block_size>>(10-SECTOR_SHIFT), 
+	       dmc->md_block_size * 512, 
+	       dmc->sysctl_skip_seq_thresh_kb,
+	       dmc->size, dmc->cached_blocks, 
+	       (int)cache_pct, dmc->nr_dirty, (int)dirty_pct);
+	DMINFO("\tnr_queued(%lu)\n", dmc->pending_jobs_count);
+	DMINFO("Size Hist: ");
+	for (i = 1 ; i <= 32 ; i++) {
+		if (size_hist[i] > 0)
+			DMINFO("%d:%llu ", i*512, size_hist[i]);
+	}
+}
+
+/*
+ * Destroy the cache mapping.
+ */
+void 
+flashcache_dtr(struct dm_target *ti)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct cache_c **nodepp;
+	int i;
+	int nr_queued = 0;
+
+	flashcache_dtr_procfs(dmc);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		flashcache_sync_for_remove(dmc);
+		flashcache_writeback_md_store(dmc);
+	}
+	if (!dmc->sysctl_fast_remove && dmc->nr_dirty > 0)
+		DMERR("Could not sync %d blocks to disk, cache still dirty", 
+		      dmc->nr_dirty);
+	DMINFO("cache jobs %d, pending jobs %d", atomic_read(&nr_cache_jobs), 
+	       atomic_read(&nr_pending_jobs));
+	for (i = 0 ; i < dmc->size ; i++)
+		nr_queued += dmc->cache[i].nr_queued;
+	DMINFO("cache queued jobs %d", nr_queued);	
+	flashcache_dtr_stats_print(dmc);
+
+	vfree((void *)dmc->cache);
+	vfree((void *)dmc->cache_sets);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		vfree((void *)dmc->md_blocks_buf);
+	flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 1);
+	flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 1);
+	VERIFY(dmc->num_whitelist_pids == 0);
+	VERIFY(dmc->num_blacklist_pids == 0);
+	dm_put_device(ti, dmc->disk_dev);
+	dm_put_device(ti, dmc->cache_dev);
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, 
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, 
+			       TASK_UNINTERRUPTIBLE);
+	nodepp = &cache_list_head;
+	while (*nodepp != NULL) {
+		if (*nodepp == dmc) {
+			*nodepp = dmc->next_cache;
+			break;
+		}
+		nodepp = &((*nodepp)->next_cache);
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	kfree(dmc);
+}
+
+void
+flashcache_status_info(struct cache_c *dmc, status_type_t type,
+		       char *result, unsigned int maxlen)
+{
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+	int sz = 0; /* DMEMIT */
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+
+	
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	DMEMIT("stats: \n\treads(%lu), writes(%lu)\n", 
+	       stats->reads, stats->writes);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" \
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->dirty_write_hits, dirty_write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tmetadata dirties(%lu), metadata cleans(%lu)\n" \
+		       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+		       "\tcleanings(%lu) fallow cleanings(%lu)\n"	\
+		       "\tno room(%lu) front merge(%lu) back merge(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->md_write_dirty, stats->md_write_clean,
+		       stats->md_write_batch, stats->md_ssd_writes,
+		       stats->cleanings, stats->fallow_cleanings, 
+		       stats->noroom, stats->front_merge, stats->back_merge);
+	} else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	} else {	/* WRITE_AROUND */
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\tinvalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	}
+	/* All modes */
+	DMEMIT("\tdisk reads(%lu), disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+	       "\tuncached reads(%lu), uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tuncached sequential reads(%lu), uncached sequential writes(%lu)\n" \
+	       "\tpid_adds(%lu), pid_dels(%lu), pid_drops(%lu) pid_expiry(%lu)",
+	       stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+	       stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->uncached_sequential_reads, stats->uncached_sequential_writes,
+	       stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	if (dmc->sysctl_io_latency_hist) {
+		int i;
+		
+		DMEMIT("\nIO Latency Histogram: \n");
+		for (i = 1 ; i <= IO_LATENCY_BUCKETS ; i++) {
+			DMEMIT("< %d\tusecs : %lu\n", i * IO_LATENCY_GRAN_USECS, dmc->latency_hist[i - 1]);
+		}
+		DMEMIT("> 10\tmsecs : %lu", dmc->latency_hist_10ms);		
+	}
+}
+
+static void
+flashcache_status_table(struct cache_c *dmc, status_type_t type,
+			     char *result, unsigned int maxlen)
+{
+	u_int64_t  cache_pct, dirty_pct;
+	int i;
+	int sz = 0; /* DMEMIT */
+	char *cache_mode;
+	
+
+	if (dmc->size > 0) {
+		dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+		cache_pct = ((u_int64_t)dmc->cached_blocks * 100) / dmc->size;
+	} else {
+		cache_pct = 0;
+		dirty_pct = 0;
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		cache_mode = "WRITE_BACK";
+	else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+		cache_mode = "WRITE_THROUGH";
+	else
+		cache_mode = "WRITE_AROUND";
+	DMEMIT("conf:\n");
+	DMEMIT("\tssd dev (%s), disk dev (%s) cache mode(%s)\n",
+	       dmc->cache_devname, dmc->disk_devname,
+	       cache_mode);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tcapacity(%luM), associativity(%u), data block size(%uK) metadata block size(%ub)\n",
+		       dmc->size*dmc->block_size>>11, dmc->assoc,
+		       dmc->block_size>>(10-SECTOR_SHIFT), 
+		       dmc->md_block_size * 512);
+	} else {
+		DMEMIT("\tcapacity(%luM), associativity(%u), data block size(%uK)\n",
+		       dmc->size*dmc->block_size>>11, dmc->assoc,
+		       dmc->block_size>>(10-SECTOR_SHIFT));
+	}
+	DMEMIT("\tskip sequential thresh(%uK)\n",
+	       dmc->sysctl_skip_seq_thresh_kb);
+	DMEMIT("\ttotal blocks(%lu), cached blocks(%lu), cache percent(%d)\n",
+	       dmc->size, dmc->cached_blocks,
+	       (int)cache_pct);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tdirty blocks(%d), dirty percent(%d)\n",
+		       dmc->nr_dirty, (int)dirty_pct);
+	}
+	DMEMIT("\tnr_queued(%lu)\n", dmc->pending_jobs_count);
+	DMEMIT("Size Hist: ");
+	for (i = 1 ; i <= 32 ; i++) {
+		if (size_hist[i] > 0)
+			DMEMIT("%d:%llu ", i*512, size_hist[i]);
+	}
+}
+
+/*
+ * Report cache status:
+ *  Output cache stats upon request of device status;
+ *  Output cache configuration upon request of table status.
+ */
+int 
+flashcache_status(struct dm_target *ti, status_type_t type,
+	     char *result, unsigned int maxlen)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	
+	switch (type) {
+	case STATUSTYPE_INFO:
+		flashcache_status_info(dmc, type, result, maxlen);
+		break;
+	case STATUSTYPE_TABLE:
+		flashcache_status_table(dmc, type, result, maxlen);
+		break;
+	}
+	return 0;
+}
+
+static struct target_type flashcache_target = {
+	.name   = "flashcache",
+	.version= {1, 0, 3},
+	.module = THIS_MODULE,
+	.ctr    = flashcache_ctr,
+	.dtr    = flashcache_dtr,
+	.map    = flashcache_map,
+	.status = flashcache_status,
+	.ioctl 	= flashcache_ioctl,
+};
+
+static void
+flashcache_sync_for_remove(struct cache_c *dmc)
+{
+	do {
+		atomic_set(&dmc->remove_in_prog, SLOW_REMOVE); /* Stop cleaning of sets */
+		if (!dmc->sysctl_fast_remove) {
+			/* 
+			 * Kick off cache cleaning. client_destroy will wait for cleanings
+			 * to finish.
+			 */
+			printk(KERN_ALERT "Cleaning %d blocks please WAIT", dmc->nr_dirty);
+			/* Tune up the cleaning parameters to clean very aggressively */
+			dmc->max_clean_ios_total = 20;
+			dmc->max_clean_ios_set = 10;
+			flashcache_sync_all(dmc);
+		} else {
+			/* Needed to abort any in-progress cleanings, leave blocks DIRTY */
+			atomic_set(&dmc->remove_in_prog, FAST_REMOVE);
+			printk(KERN_ALERT "Fast flashcache remove Skipping cleaning of %d blocks", 
+			       dmc->nr_dirty);
+		}
+		/* 
+		 * We've prevented new cleanings from starting (for the fast remove case)
+		 * and we will wait for all in progress cleanings to exit.
+		 * Wait a few seconds for everything to quiesce before writing out the 
+		 * cache metadata.
+		 */
+		msleep(FLASHCACHE_SYNC_REMOVE_DELAY);
+		/* Wait for all the dirty blocks to get written out, and any other IOs */
+		wait_event(dmc->destroyq, !atomic_read(&dmc->nr_jobs));
+		cancel_delayed_work(&dmc->delayed_clean);
+		flush_scheduled_work();
+	} while (!dmc->sysctl_fast_remove && dmc->nr_dirty > 0);
+}
+
+static int 
+flashcache_notify_reboot(struct notifier_block *this,
+			 unsigned long code, void *x)
+{
+	struct cache_c *dmc;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, 
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, 
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ; 
+	     dmc != NULL ; 
+	     dmc = dmc->next_cache) {
+		if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+			flashcache_sync_for_remove(dmc);
+			flashcache_writeback_md_store(dmc);
+		}
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return NOTIFY_DONE;
+}
+
+/*
+ * The notifiers are registered in descending order of priority and
+ * executed in descending order or priority. We should be run before
+ * any notifiers of ssd's or other block devices. Typically, devices
+ * use a priority of 0.
+ * XXX - If in the future we happen to use a md device as the cache
+ * block device, we have a problem because md uses a priority of 
+ * INT_MAX as well. But we want to run before the md's reboot notifier !
+ */
+static struct notifier_block flashcache_notifier = {
+	.notifier_call	= flashcache_notify_reboot,
+	.next		= NULL,
+	.priority	= INT_MAX, /* should be > ssd pri's and disk dev pri's */
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+struct dm_kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#else
+struct kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+/*
+ * Initiate a cache target.
+ */
+int __init 
+flashcache_init(void)
+{
+	int r;
+
+	r = flashcache_jobs_init();
+	if (r)
+		return r;
+	atomic_set(&nr_cache_jobs, 0);
+	atomic_set(&nr_pending_jobs, 0);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	r = dm_io_get(FLASHCACHE_ASYNC_SIZE);
+	if (r) {
+		DMERR("flashcache_init: Could not size dm io pool");
+		return r;
+	}
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+	if (r) {
+		DMERR("flashcache_init: Failed to initialize kcopyd client");
+		dm_io_put(FLASHCACHE_ASYNC_SIZE);
+		return r;
+	}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22) */
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)) || (defined(RHEL_RELEASE_CODE) && (RHEL_RELEASE_CODE >= 1538))
+	flashcache_io_client = dm_io_client_create();
+#else
+	flashcache_io_client = dm_io_client_create(FLASHCACHE_COPY_PAGES);
+#endif
+	if (IS_ERR(flashcache_io_client)) {
+		DMERR("flashcache_init: Failed to initialize DM IO client");
+		return r;
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+#elif (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)) || (defined(RHEL_RELEASE_CODE) && (RHEL_RELEASE_CODE >= 1538))
+	flashcache_kcp_client = dm_kcopyd_client_create();
+	if ((r = IS_ERR(flashcache_kcp_client))) {
+		r = PTR_ERR(flashcache_kcp_client);
+	}
+#else /* .26 <= VERSION < 3.0.0 */
+	r = dm_kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+#endif /* .26 <= VERSION < 3.0.0 */
+
+	if (r) {
+		dm_io_client_destroy(flashcache_io_client);
+		DMERR("flashcache_init: Failed to initialize kcopyd client");
+		return r;
+	}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22) */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&_kcached_wq, do_work, NULL);
+#else
+	INIT_WORK(&_kcached_wq, do_work);
+#endif
+	for (r = 0 ; r < 33 ; r++)
+		size_hist[r] = 0;
+	r = dm_register_target(&flashcache_target);
+	if (r < 0) {
+		DMERR("cache: register failed %d", r);
+	}
+
+        printk("flashcache: %s initialized\n", flashcache_sw_version);
+
+	flashcache_module_procfs_init();
+	flashcache_control = (struct flashcache_control_s *)
+		kmalloc(sizeof(struct flashcache_control_s), GFP_KERNEL);
+	flashcache_control->synch_flags = 0;
+	register_reboot_notifier(&flashcache_notifier);
+	return r;
+}
+
+/*
+ * Destroy a cache target.
+ */
+void 
+flashcache_exit(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	int r = dm_unregister_target(&flashcache_target);
+
+	if (r < 0)
+		DMERR("cache: unregister failed %d", r);
+#else
+	dm_unregister_target(&flashcache_target);
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	kcopyd_client_destroy(flashcache_kcp_client);
+#else
+	dm_kcopyd_client_destroy(flashcache_kcp_client);
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	dm_io_client_destroy(flashcache_io_client);
+#else
+	dm_io_put(FLASHCACHE_ASYNC_SIZE);
+#endif
+	unregister_reboot_notifier(&flashcache_notifier);
+	flashcache_jobs_exit();
+	flashcache_module_procfs_releae();
+	kfree(flashcache_control);
+}
+
+module_init(flashcache_init);
+module_exit(flashcache_exit);
+
+EXPORT_SYMBOL(flashcache_writeback_load);
+EXPORT_SYMBOL(flashcache_writeback_create);
+EXPORT_SYMBOL(flashcache_writeback_md_store);
+
+MODULE_DESCRIPTION(DM_NAME " Facebook flash cache target");
+MODULE_AUTHOR("Mohan - based on code by Ming");
+MODULE_LICENSE("GPL");
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache.h linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache.h
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache.h	2016-12-13 21:55:12.008000000 +0800
@@ -0,0 +1,605 @@
+/****************************************************************************
+ *  flashcache.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_H
+#define FLASHCACHE_H
+
+#define FLASHCACHE_VERSION		2
+
+#define DEV_PATHLEN	128
+
+#ifdef __KERNEL__
+
+/* Like ASSERT() but always compiled in */
+
+#define VERIFY(x) do { \
+	if (unlikely(!(x))) { \
+		dump_stack(); \
+		panic("VERIFY: assertion (%s) failed at %s (%d)\n", \
+		      #x,  __FILE__ , __LINE__);		    \
+	} \
+} while(0)
+
+#define DMC_DEBUG 0
+#define DMC_DEBUG_LITE 0
+
+#define DM_MSG_PREFIX "flashcache"
+#define DMC_PREFIX "flashcache: "
+
+#if DMC_DEBUG
+#define DPRINTK( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK( s, arg... )
+#endif
+
+/*
+ * Block checksums :
+ * Block checksums seem a good idea (especially for debugging, I found a couple 
+ * of bugs with this), but in practice there are a number of issues with this
+ * in production.
+ * 1) If a flash write fails, there is no guarantee that the failure was atomic.
+ * Some sectors may have been written to flash. If so, the checksum we have
+ * is wrong. We could re-read the flash block and recompute the checksum, but
+ * the read could fail too. 
+ * 2) On a node crash, we could have crashed between the flash data write and the
+ * flash metadata update (which updates the new checksum to flash metadata). When
+ * we reboot, the checksum we read from metadata is wrong. This is worked around
+ * by having the cache load recompute checksums after an unclean shutdown.
+ * 3) Checksums require 4 or 8 more bytes per block in terms of metadata overhead.
+ * Especially because the metadata is wired into memory.
+ * 4) Checksums force us to do a flash metadata IO on a block re-dirty. If we 
+ * didn't maintain checksums, we could avoid the metadata IO on a re-dirty.
+ * Therefore in production we disable block checksums.
+ */
+#if 0
+#define FLASHCACHE_DO_CHECKSUMS
+#endif
+
+#if DMC_DEBUG_LITE
+#define DPRINTK_LITE( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK_LITE( s, arg... )
+#endif
+
+/* Number of pages for I/O */
+#define FLASHCACHE_COPY_PAGES (1024)
+
+/* Default cache parameters */
+#define DEFAULT_CACHE_SIZE	65536
+#define DEFAULT_CACHE_ASSOC	512
+#define DEFAULT_BLOCK_SIZE	8	/* 4 KB */
+#define DEFAULT_MD_BLOCK_SIZE	8	/* 4 KB */
+#define FLASHCACHE_MAX_MD_BLOCK_SIZE	128	/* 64 KB */
+
+#define FLASHCACHE_FIFO		0
+#define FLASHCACHE_LRU		1
+
+/*
+ * The LRU pointers are maintained as set-relative offsets, instead of 
+ * pointers. This enables us to store the LRU pointers per cacheblock
+ * using 4 bytes instead of 16 bytes. The upshot of this is that we 
+ * are required to clamp the associativity at an 8K max.
+ */
+#define FLASHCACHE_MIN_ASSOC	 256
+#define FLASHCACHE_MAX_ASSOC	8192
+#define FLASHCACHE_LRU_NULL	0xFFFF
+
+struct cacheblock;
+
+struct cache_set {
+	u_int32_t		set_fifo_next;
+	u_int32_t		set_clean_next;
+	u_int16_t		clean_inprog;
+	u_int16_t		nr_dirty;
+	u_int16_t		lru_head, lru_tail;
+	u_int16_t		dirty_fallow;
+	unsigned long 		fallow_tstamp;
+	unsigned long 		fallow_next_cleaning;
+};
+
+struct flashcache_errors {
+	int	disk_read_errors;
+	int	disk_write_errors;
+	int	ssd_read_errors;
+	int	ssd_write_errors;
+	int	memory_alloc_errors;
+};
+
+struct flashcache_stats {
+	unsigned long reads;		/* Number of reads */
+	unsigned long writes;		/* Number of writes */
+	unsigned long read_hits;	/* Number of cache hits */
+	unsigned long write_hits;	/* Number of write hits (includes dirty write hits) */
+	unsigned long dirty_write_hits;	/* Number of "dirty" write hits */
+	unsigned long replace;		/* Number of cache replacements */
+	unsigned long wr_replace;
+	unsigned long wr_invalidates;	/* Number of write invalidations */
+	unsigned long rd_invalidates;	/* Number of read invalidations */
+	unsigned long pending_inval;	/* Invalidations due to concurrent ios on same block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	unsigned long checksum_store;
+	unsigned long checksum_valid;
+	unsigned long checksum_invalid;
+#endif
+	unsigned long enqueues;		/* enqueues on pending queue */
+	unsigned long cleanings;
+	unsigned long fallow_cleanings;
+	unsigned long noroom;		/* No room in set */
+	unsigned long md_write_dirty;	/* Metadata sector writes dirtying block */
+	unsigned long md_write_clean;	/* Metadata sector writes cleaning block */
+	unsigned long md_write_batch;	/* How many md updates did we batch ? */
+	unsigned long md_ssd_writes;	/* How many md ssd writes did we do ? */
+	unsigned long pid_drops;
+	unsigned long pid_adds;
+	unsigned long pid_dels;
+	unsigned long expiry;
+	unsigned long front_merge, back_merge;	/* Write Merging */
+	unsigned long uncached_reads, uncached_writes;
+	unsigned long uncached_sequential_reads, uncached_sequential_writes;
+	unsigned long disk_reads, disk_writes;
+	unsigned long ssd_reads, ssd_writes;
+	unsigned long uncached_io_requeue;
+	unsigned long skipclean;
+	unsigned long trim_blocks;
+	unsigned long clean_set_ios;
+};
+
+/* 
+ * Sequential block history structure - each one
+ * records a 'flow' of i/o.
+ */
+struct sequential_io {
+ 	sector_t 		most_recent_sector;
+	unsigned long		sequential_count;
+	/* We use LRU replacement when we need to record a new i/o 'flow' */
+	struct sequential_io 	*prev, *next;
+};
+#define SKIP_SEQUENTIAL_THRESHOLD 0			/* 0 = cache all, >0 = dont cache sequential i/o more than this (kb) */
+#define SEQUENTIAL_TRACKER_QUEUE_DEPTH	32		/* How many io 'flows' to track (random i/o will hog many).
+							 * This should be large enough so that we don't quickly 
+							 * evict sequential i/o when we see some random,
+							 * but small enough that searching through it isn't slow
+							 * (currently we do linear search, we could consider hashed */
+								
+	
+/*
+ * Cache context
+ */
+struct cache_c {
+	struct dm_target	*tgt;
+	
+	struct dm_dev 		*disk_dev;   /* Source device */
+	struct dm_dev 		*cache_dev; /* Cache device */
+
+	int 			on_ssd_version;
+	
+	spinlock_t		cache_spin_lock;
+
+	struct cacheblock	*cache;	/* Hash table for cache blocks */
+	struct cache_set	*cache_sets;
+	struct cache_md_block_head *md_blocks_buf;
+
+	unsigned int md_block_size;	/* Metadata block size in sectors */
+	
+	sector_t size;			/* Cache size */
+	unsigned int assoc;		/* Cache associativity */
+	unsigned int block_size;	/* Cache block size */
+	unsigned int block_shift;	/* Cache block size in bits */
+	unsigned int block_mask;	/* Cache block mask */
+	unsigned int assoc_shift;	/* Consecutive blocks size in bits */
+	unsigned int num_sets;		/* Number of cache sets */
+	
+	int	cache_mode;
+
+	wait_queue_head_t destroyq;	/* Wait queue for I/O completion */
+	/* XXX - Updates of nr_jobs should happen inside the lock. But doing it outside
+	   is OK since the filesystem is unmounted at this point */
+	atomic_t nr_jobs;		/* Number of I/O jobs */
+
+#define SLOW_REMOVE    1                                                                                    
+#define FAST_REMOVE    2
+	atomic_t remove_in_prog;
+
+	int	dirty_thresh_set;	/* Per set dirty threshold to start cleaning */
+	int	max_clean_ios_set;	/* Max cleaning IOs per set */
+	int	max_clean_ios_total;	/* Total max cleaning IOs */
+	int	clean_inprog;
+	int	sync_index;
+	int	nr_dirty;
+	unsigned long cached_blocks;	/* Number of cached blocks */
+	unsigned long pending_jobs_count;
+	int	md_blocks;		/* Numbers of metadata blocks, including header */
+
+	/* Stats */
+	struct flashcache_stats flashcache_stats;
+
+	/* Errors */
+	struct flashcache_errors flashcache_errors;
+
+#define IO_LATENCY_GRAN_USECS	250
+#define IO_LATENCY_MAX_US_TRACK	10000	/* 10 ms */
+#define IO_LATENCY_BUCKETS	(IO_LATENCY_MAX_US_TRACK / IO_LATENCY_GRAN_USECS)
+	unsigned long	latency_hist[IO_LATENCY_BUCKETS];
+	unsigned long	latency_hist_10ms;
+	
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	struct work_struct delayed_clean;
+#else
+	struct delayed_work delayed_clean;
+#endif
+
+	unsigned long pid_expire_check;
+
+	struct flashcache_cachectl_pid *blacklist_head, *blacklist_tail;
+	struct flashcache_cachectl_pid *whitelist_head, *whitelist_tail;
+	int num_blacklist_pids, num_whitelist_pids;
+	unsigned long blacklist_expire_check, whitelist_expire_check;
+
+#define PENDING_JOB_HASH_SIZE		32
+	struct pending_job *pending_job_hashbuckets[PENDING_JOB_HASH_SIZE];
+	
+	struct cache_c	*next_cache;
+
+	void *sysctl_handle;
+
+	// DM virtual device name, stored in superblock and restored on load
+	char dm_vdevname[DEV_PATHLEN];
+	// real device names are now stored as UUIDs
+	char cache_devname[DEV_PATHLEN];
+	char disk_devname[DEV_PATHLEN];
+
+	/* Per device sysctls */
+	int sysctl_io_latency_hist;
+	int sysctl_do_sync;
+	int sysctl_stop_sync;
+	int sysctl_dirty_thresh;
+	int sysctl_pid_do_expiry;
+	int sysctl_max_pids;
+	int sysctl_pid_expiry_secs;
+	int sysctl_reclaim_policy;
+	int sysctl_zerostats;
+	int sysctl_error_inject;
+	int sysctl_fast_remove;
+	int sysctl_cache_all;
+	int sysctl_fallow_clean_speed;
+	int sysctl_fallow_delay;
+	int sysctl_skip_seq_thresh_kb;
+
+	/* Sequential I/O spotter */
+	struct sequential_io	seq_recent_ios[SEQUENTIAL_TRACKER_QUEUE_DEPTH];
+	struct sequential_io	*seq_io_head;
+	struct sequential_io 	*seq_io_tail;
+};
+
+/* kcached/pending job states */
+#define READCACHE	1
+#define WRITECACHE	2
+#define READDISK	3
+#define WRITEDISK	4
+#define READFILL	5	/* Read Cache Miss Fill */
+#define INVALIDATE	6
+#define WRITEDISK_SYNC	7
+
+struct kcached_job {
+	struct list_head list;
+	struct cache_c *dmc;
+	struct bio *bio;	/* Original bio */
+	struct job_io_regions {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		struct io_region disk;
+		struct io_region cache;
+#else
+		struct dm_io_region disk;
+		struct dm_io_region cache;
+#endif
+	} job_io_regions;
+	int    index;
+	int    action;
+	int 	error;
+	struct flash_cacheblock *md_block;
+	struct bio_vec md_io_bvec;
+	struct timeval io_start_time;
+	struct kcached_job *next;
+};
+
+struct pending_job {
+	struct bio *bio;
+	int	action;	
+	int	index;
+	struct pending_job *prev, *next;
+};
+#endif /* __KERNEL__ */
+
+/* Cache Modes */
+enum {
+	FLASHCACHE_WRITE_BACK=1,
+	FLASHCACHE_WRITE_THROUGH=2,
+	FLASHCACHE_WRITE_AROUND=3,
+};
+
+/* States of a cache block */
+#define INVALID			0x0001
+#define VALID			0x0002	/* Valid */
+#define DISKREADINPROG		0x0004	/* Read from disk in progress */
+#define DISKWRITEINPROG		0x0008	/* Write to disk in progress */
+#define CACHEREADINPROG		0x0010	/* Read from cache in progress */
+#define CACHEWRITEINPROG	0x0020	/* Write to cache in progress */
+#define DIRTY			0x0040	/* Dirty, needs writeback to disk */
+/*
+ * Old and Dirty blocks are cleaned with a Clock like algorithm. The leading hand
+ * marks DIRTY_FALLOW_1. 900 seconds (default) later, the trailing hand comes along and
+ * marks DIRTY_FALLOW_2 if DIRTY_FALLOW_1 is already set. If the block was used in the 
+ * interim, (DIRTY_FALLOW_1|DIRTY_FALLOW_2) is cleared. Any block that has both 
+ * DIRTY_FALLOW_1 and DIRTY_FALLOW_2 marked is considered old and is eligible 
+ * for cleaning.
+ */
+#define DIRTY_FALLOW_1		0x0080	
+#define DIRTY_FALLOW_2		0x0100
+
+#define FALLOW_DOCLEAN		(DIRTY_FALLOW_1 | DIRTY_FALLOW_2)
+#define BLOCK_IO_INPROG	(DISKREADINPROG | DISKWRITEINPROG | CACHEREADINPROG | CACHEWRITEINPROG)
+
+/* Cache metadata is read by Flashcache utilities */
+#ifndef __KERNEL__
+typedef u_int64_t sector_t;
+#endif
+
+/* On Flash (cache metadata) Structures */
+#define CACHE_MD_STATE_DIRTY		0xdeadbeef
+#define CACHE_MD_STATE_CLEAN		0xfacecafe
+#define CACHE_MD_STATE_FASTCLEAN	0xcafefeed
+#define CACHE_MD_STATE_UNSTABLE		0xc8249756
+
+/* Cache block metadata structure */
+struct cacheblock {
+	u_int16_t	cache_state;
+	int16_t 	nr_queued;	/* jobs in pending queue */
+	u_int16_t	lru_prev, lru_next;
+	sector_t 	dbn;	/* Sector number of the cached block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	u_int64_t 	checksum;
+#endif
+};
+
+struct flash_superblock {
+	sector_t size;		/* Cache size */
+	u_int32_t block_size;	/* Cache block size */
+	u_int32_t assoc;	/* Cache associativity */
+	u_int32_t cache_sb_state;	/* Clean shutdown ? */
+	char cache_devname[DEV_PATHLEN]; /* Contains dm_vdev name as of v2 modifications */
+	sector_t cache_devsize;
+	char disk_devname[DEV_PATHLEN]; /* underlying block device name (use UUID paths!) */
+	sector_t disk_devsize;
+	u_int32_t cache_version;
+	u_int32_t md_block_size;
+};
+
+/* 
+ * We do metadata updates only when a block trasitions from DIRTY -> CLEAN
+ * or from CLEAN -> DIRTY. Consequently, on an unclean shutdown, we only
+ * pick up blocks that are marked (DIRTY | CLEAN), we clean these and stick
+ * them in the cache.
+ * On a clean shutdown, we will sync the state for every block, and we will
+ * load every block back into cache on a restart.
+ * 
+ * Note: When using larger flashcache metadata blocks, it is important to make 
+ * sure that a flash_cacheblock does not straddle 2 sectors. This avoids
+ * partial writes of a metadata slot on a powerfail/node crash. Aligning this
+ * a 16b or 32b struct avoids that issue.
+ * 
+ * Note: If a on-ssd flash_cacheblock does not fit exactly within a 512b sector,
+ * (ie. if there are any remainder runt bytes), logic in flashcache_conf.c which
+ * reads and writes flashcache metadata on create/load/remove will break.
+ * 
+ * If changing these, make sure they remain a ^2 size !
+ */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int64_t 	checksum;
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(32)));
+#else
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(16)));
+#endif
+
+#define MD_BLOCK_BYTES(DMC)		((DMC)->md_block_size * 512)
+#define MD_SECTORS_PER_BLOCK(DMC)	((DMC)->md_block_size)
+#define MD_SLOTS_PER_BLOCK(DMC)		(MD_BLOCK_BYTES(DMC) / (sizeof(struct flash_cacheblock)))
+#define INDEX_TO_MD_BLOCK(DMC, INDEX)	((INDEX) / MD_SLOTS_PER_BLOCK(DMC))
+#define INDEX_TO_MD_BLOCK_OFFSET(DMC, INDEX)	((INDEX) % MD_SLOTS_PER_BLOCK(DMC))
+
+#define METADATA_IO_BLOCKSIZE		(256*1024)
+#define METADATA_IO_NUM_BLOCKS(dmc)	(METADATA_IO_BLOCKSIZE / MD_BLOCK_BYTES(dmc))
+
+#define INDEX_TO_CACHE_ADDR(DMC, INDEX)	\
+	(((sector_t)(INDEX) << (DMC)->block_shift) + (DMC)->md_blocks * MD_SECTORS_PER_BLOCK((DMC)))
+
+#ifdef __KERNEL__
+
+/* Cache persistence */
+#define CACHE_RELOAD		1
+#define CACHE_CREATE		2
+#define CACHE_FORCECREATE	3
+
+/* 
+ * We have one of these for *every* cache metadata sector, to keep track
+ * of metadata ios in progress for blocks covered in this sector. Only
+ * one metadata IO per sector can be in progress at any given point in 
+ * time
+ */
+struct cache_md_block_head {
+	u_int32_t		nr_in_prog;
+	struct kcached_job	*queued_updates, *md_io_inprog;
+};
+
+#define MIN_JOBS 1024
+
+/* Default values for sysctls */
+#define DIRTY_THRESH_MIN	10
+#define DIRTY_THRESH_MAX	90
+#define DIRTY_THRESH_DEF	20
+
+#define MAX_CLEAN_IOS_SET	2
+#define MAX_CLEAN_IOS_TOTAL	4
+#define MAX_PIDS		100
+#define PID_EXPIRY_SECS		60
+#define FALLOW_DELAY		(60*15) /* 15 Mins default */
+#define FALLOW_SPEED_MIN	1
+#define FALLOW_SPEED_MAX	100
+#define FALLOW_CLEAN_SPEED	2
+
+/* DM async IO mempool sizing */
+#define FLASHCACHE_ASYNC_SIZE 1024
+
+enum {
+	FLASHCACHE_WHITELIST=0,
+	FLASHCACHE_BLACKLIST=1,
+};
+
+struct flashcache_cachectl_pid {
+	pid_t					pid;
+	struct flashcache_cachectl_pid		*next, *prev;
+	unsigned long				expiry;
+};
+
+struct dbn_index_pair {
+	sector_t	dbn;
+	int		index;
+};
+
+/* Error injection flags */
+#define READDISK_ERROR				0x00000001
+#define READCACHE_ERROR				0x00000002
+#define READFILL_ERROR				0x00000004
+#define WRITECACHE_ERROR			0x00000008
+#define WRITECACHE_MD_ERROR			0x00000010
+#define WRITEDISK_MD_ERROR			0x00000020
+#define KCOPYD_CALLBACK_ERROR			0x00000040
+#define DIRTY_WRITEBACK_JOB_ALLOC_FAIL		0x00000080
+#define READ_MISS_JOB_ALLOC_FAIL		0x00000100
+#define READ_HIT_JOB_ALLOC_FAIL			0x00000200
+#define READ_HIT_PENDING_JOB_ALLOC_FAIL		0x00000400
+#define INVAL_PENDING_JOB_ALLOC_FAIL		0x00000800
+#define WRITE_HIT_JOB_ALLOC_FAIL		0x00001000
+#define WRITE_HIT_PENDING_JOB_ALLOC_FAIL	0x00002000
+#define WRITE_MISS_JOB_ALLOC_FAIL		0x00004000
+#define WRITES_LIST_ALLOC_FAIL			0x00008000
+#define MD_ALLOC_SECTOR_ERROR			0x00010000
+
+/* Inject a 5s delay between syncing blocks and metadata */
+#define FLASHCACHE_SYNC_REMOVE_DELAY		5000
+
+int flashcache_map(struct dm_target *ti, struct bio *bio,
+		   union map_info *map_context);
+int flashcache_ctr(struct dm_target *ti, unsigned int argc,
+		   char **argv);
+void flashcache_dtr(struct dm_target *ti);
+
+int flashcache_status(struct dm_target *ti, status_type_t type,
+		      char *result, unsigned int maxlen);
+
+struct kcached_job *flashcache_alloc_cache_job(void);
+void flashcache_free_cache_job(struct kcached_job *job);
+struct pending_job *flashcache_alloc_pending_job(struct cache_c *dmc);
+void flashcache_free_pending_job(struct pending_job *job);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+u_int64_t flashcache_compute_checksum(struct bio *bio);
+void flashcache_store_checksum(struct kcached_job *job);
+int flashcache_validate_checksum(struct kcached_job *job);
+int flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block);
+#endif
+struct kcached_job *pop(struct list_head *jobs);
+void push(struct list_head *jobs, struct kcached_job *job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+void do_work(void *unused);
+#else
+void do_work(struct work_struct *unused);
+#endif
+struct kcached_job *new_kcached_job(struct cache_c *dmc, struct bio* bio,
+				    int index);
+void push_pending(struct kcached_job *job);
+void push_io(struct kcached_job *job);
+void push_md_io(struct kcached_job *job);
+void push_md_complete(struct kcached_job *job);
+void push_uncached_io_complete(struct kcached_job *job);
+int flashcache_pending_empty(void);
+int flashcache_io_empty(void);
+int flashcache_md_io_empty(void);
+int flashcache_md_complete_empty(void);
+void flashcache_md_write_done(struct kcached_job *job);
+void flashcache_do_pending(struct kcached_job *job);
+void flashcache_md_write(struct kcached_job *job);
+void flashcache_md_write_kickoff(struct kcached_job *job);
+void flashcache_do_io(struct kcached_job *job);
+void flashcache_uncached_io_complete(struct kcached_job *job);
+void flashcache_clean_set(struct cache_c *dmc, int set);
+void flashcache_sync_all(struct cache_c *dmc);
+void flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index);
+void flashcache_merge_writes(struct cache_c *dmc, 
+			     struct dbn_index_pair *writes_list, 
+			     int *nr_writes, int set);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct io_region *where, 
+			     int rw, void *data);
+#else
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where, 
+			     int rw, void *data);
+#endif
+void flashcache_update_sync_progress(struct cache_c *dmc);
+void flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+			    int index, int action, struct pending_job *job);
+struct pending_job *flashcache_deq_pending(struct cache_c *dmc, int index);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions, 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where, 
+#else
+			    struct io_region *where, 
+#endif
+			    int rw, 
+			    struct bio_vec *bvec, io_notify_fn fn, 
+			    void *context);
+#endif
+
+void flashcache_detect_fallow(struct cache_c *dmc, int index);
+void flashcache_clear_fallow(struct cache_c *dmc, int index);
+
+void flashcache_bio_endio(struct bio *bio, int error, 
+			  struct cache_c *dmc, struct timeval *io_start_time);
+
+/* procfs */
+void flashcache_module_procfs_init(void);
+void flashcache_module_procfs_releae(void);
+void flashcache_ctr_procfs(struct cache_c *dmc);
+void flashcache_dtr_procfs(struct cache_c *dmc);
+
+#endif /* __KERNEL__ */
+
+#endif
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_ioctl.c linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_ioctl.c
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_ioctl.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_ioctl.c	2016-12-13 21:55:12.009000000 +0800
@@ -0,0 +1,565 @@
+/****************************************************************************
+ *  flashcache_ioctl.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+static int flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid, 
+				      int which_list);
+static void flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid, 
+				      int which_list);
+
+static int
+flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid, 
+			   int which_list)
+{
+	struct flashcache_cachectl_pid *pid_list;
+	
+	pid_list = ((which_list == FLASHCACHE_WHITELIST) ? 
+		    dmc->whitelist_head : dmc->blacklist_head);
+	for ( ; pid_list != NULL ; pid_list = pid_list->next) {
+		if (pid_list->pid == pid)
+			return 1;
+	}
+	return 0;	
+}
+
+static void
+flashcache_drop_pids(struct cache_c *dmc, int which_list)
+{
+	if (which_list == FLASHCACHE_WHITELIST) {
+		while (dmc->num_whitelist_pids >= dmc->sysctl_max_pids) {
+			VERIFY(dmc->whitelist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->whitelist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}
+	} else {
+		while (dmc->num_blacklist_pids >= dmc->sysctl_max_pids) {
+			VERIFY(dmc->blacklist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->blacklist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}		
+	}
+}
+
+static void
+flashcache_add_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *new;
+ 	unsigned long flags;
+
+	new = kmalloc(sizeof(struct flashcache_cachectl_pid), GFP_KERNEL);
+	new->pid = pid;
+	new->next = NULL;
+	new->expiry = jiffies + dmc->sysctl_pid_expiry_secs * HZ;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (which_list == FLASHCACHE_WHITELIST) {
+		if (dmc->num_whitelist_pids > dmc->sysctl_max_pids)
+			flashcache_drop_pids(dmc, which_list);
+	} else {
+		if (dmc->num_blacklist_pids > dmc->sysctl_max_pids)
+			flashcache_drop_pids(dmc, which_list);		
+	}
+	if (flashcache_find_pid_locked(dmc, pid, which_list) == 0) {
+		struct flashcache_cachectl_pid **head, **tail;
+		
+		if (which_list == FLASHCACHE_WHITELIST) {
+			head = &dmc->whitelist_head;
+			tail = &dmc->whitelist_tail;
+		} else {
+			head = &dmc->blacklist_head;
+			tail = &dmc->blacklist_tail;
+		}
+		/* Add the new pid to the tail */
+		new->prev = *tail;
+		if (*head == NULL) {
+			VERIFY(*tail == NULL);
+			*head = new;
+		} else {
+			VERIFY(*tail != NULL);
+			(*tail)->next = new;
+		}
+		*tail = new;
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids++;
+		else
+			dmc->num_blacklist_pids++;
+		dmc->flashcache_stats.pid_adds++;
+		/* When adding the first entry to list, set expiry check timeout */
+		if (*head == new)
+			dmc->pid_expire_check = 
+				jiffies + ((dmc->sysctl_pid_expiry_secs + 1) * HZ);
+	} else
+		kfree(new);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	return;
+}
+
+static void
+flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *node;
+	struct flashcache_cachectl_pid **head, **tail;
+	
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *tail ; node != NULL ; node = node->prev) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (node->pid == pid) {
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				*tail = node->prev;
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			kfree(node);
+			dmc->flashcache_stats.pid_dels++;
+			if (which_list == FLASHCACHE_WHITELIST)
+				dmc->num_whitelist_pids--;
+			else
+				dmc->num_blacklist_pids--;
+			return;
+		}
+	}
+}
+
+static void
+flashcache_del_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	flashcache_del_pid_locked(dmc, pid, which_list);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+}
+
+/*
+ * This removes all "dead" pids. Pids that may have not cleaned up.
+ */
+void
+flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force)
+{
+	struct flashcache_cachectl_pid *node, **tail;
+	unsigned long flags;
+	
+	if (which_list == FLASHCACHE_WHITELIST)
+		tail = &dmc->whitelist_tail;
+	else
+		tail = &dmc->blacklist_tail;
+	rcu_read_lock();
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	node = *tail;
+	while (node != NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+		if (force == 0) {
+			struct task_struct *task;
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
+			task = find_task_by_pid_type(PIDTYPE_PID, node->pid);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+			task = find_task_by_vpid(node->pid);
+#else
+			ask = pid_task(find_vpid(node->pid), PIDTYPE_PID);
+#endif
+			/*
+			 * If that task was found, don't remove it !
+			 * This prevents a rogue "delete all" from removing
+			 * every thread from the list.
+			 */
+			if (task) {
+				node = node->prev;
+				continue;
+			}
+		}
+#endif
+		flashcache_del_pid_locked(dmc, node->pid, which_list);
+		node = *tail;
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	rcu_read_unlock();
+}
+
+static void
+flashcache_pid_expiry_list_locked(struct cache_c *dmc, int which_list)
+{
+	struct flashcache_cachectl_pid **head, **tail, *node;
+	
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *head ; node != NULL ; node = node->next) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (time_after(node->expiry, jiffies))
+			continue;
+		if (node->prev == NULL) {
+			*head = node->next;
+			if (node->next)
+				node->next->prev = NULL;
+		} else
+			node->prev->next = node->next;
+		if (node->next == NULL) {
+			*tail = node->prev;
+			if (node->prev)
+				node->prev->next = NULL;
+		} else
+			node->next->prev = node->prev;
+		kfree(node);
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids--;
+		else
+			dmc->num_blacklist_pids--;
+		dmc->flashcache_stats.expiry++;
+	}
+}
+
+void
+flashcache_pid_expiry_all_locked(struct cache_c *dmc)
+{
+	if (likely(time_before(jiffies, dmc->pid_expire_check)))
+		return;
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_WHITELIST);
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_BLACKLIST);
+	dmc->pid_expire_check = jiffies + (dmc->sysctl_pid_expiry_secs + 1) * HZ;
+}
+
+/*
+ * Is the IO cacheable, depending on global cacheability and the white/black
+ * lists ? This function is a bit confusing because we want to support inheritance
+ * of cacheability across pthreads (so we use the tgid). But when an entire thread
+ * group is added to the white/black list, we want to provide for exceptions for 
+ * individual threads as well.
+ * The Rules (in decreasing order of priority) :
+ * 1) Check the pid (thread id) against the list. 
+ * 2) Check the tgid against the list, then check for exceptions within the tgid.
+ * 3) Possibly don't cache sequential i/o.
+ */
+int
+flashcache_uncacheable(struct cache_c *dmc, struct bio *bio)
+{
+	int dontcache;
+	
+	if (dmc->sysctl_cache_all) {
+		/* If the tid has been blacklisted, we don't cache at all.
+		   This overrides everything else */
+		dontcache = flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_BLACKLIST);
+		if (dontcache)
+			goto out;
+		/* Is the tgid in the blacklist ? */
+		dontcache = flashcache_find_pid_locked(dmc, current->tgid, 
+						       FLASHCACHE_BLACKLIST);
+		/* 
+		 * If we found the tgid in the blacklist, is there a whitelist
+		 * exception entered for this thread ?
+		 */
+		if (dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_WHITELIST)) {
+				dontcache = 0;
+				goto out;
+			}
+		}
+
+		/* Finally, if we are neither in a whitelist or a blacklist,
+		 * do a final check to see if this is sequential i/o.  If
+		 * the relevant sysctl is set, we will skip it.
+		 */
+		dontcache = skip_sequential_io(dmc, bio);
+			
+	} else { /* cache nothing */
+		/* If the tid has been whitelisted, we cache 
+		   This overrides everything else */
+		dontcache = !flashcache_find_pid_locked(dmc, current->pid, 
+							FLASHCACHE_WHITELIST);
+		if (!dontcache)
+			goto out;
+		/* Is the tgid in the whitelist ? */
+		dontcache = !flashcache_find_pid_locked(dmc, current->tgid, 
+							FLASHCACHE_WHITELIST);
+		/* 
+		 * If we found the tgid in the whitelist, is there a black list 
+		 * exception entered for this thread ?
+		 */
+		if (!dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_BLACKLIST))
+				dontcache = 1;
+		}
+		/* No sequential handling here.  If we add to the whitelist,
+		 * everything is cached, sequential or not.
+  		 */
+	}
+out:
+	return dontcache;
+}
+
+/* Below 2 functions manage the LRU cache of recent IO 'flows'.  
+ * A sequential IO will only take up one slot (we keep updating the 
+ * last sector seen) but random IO will quickly fill multiple slots.  
+ * We allocate the LRU cache from a small fixed sized buffer at startup. 
+ */
+void
+seq_io_remove_from_lru(struct cache_c *dmc, struct sequential_io *seqio)
+{
+	if (seqio->prev != NULL) 
+		seqio->prev->next = seqio->next;
+	else {
+		VERIFY(dmc->seq_io_head == seqio);
+		dmc->seq_io_head = seqio->next;
+	}
+	if (seqio->next != NULL)
+		seqio->next->prev = seqio->prev;
+	else {
+		VERIFY(dmc->seq_io_tail == seqio);
+		dmc->seq_io_tail = seqio->prev;
+	}
+}
+
+void
+seq_io_move_to_lruhead(struct cache_c *dmc, struct sequential_io *seqio)
+{
+	if (likely(seqio->prev != NULL || seqio->next != NULL))
+		seq_io_remove_from_lru(dmc, seqio);
+	/* Add it to LRU head */
+	if (dmc->seq_io_head != NULL)
+		dmc->seq_io_head->prev = seqio;
+	seqio->next = dmc->seq_io_head;
+	seqio->prev = NULL;
+	dmc->seq_io_head = seqio;
+}
+       
+
+/* Look for and maybe skip sequential i/o.  
+ *
+ * Since          performance(SSD) >> performance(HDD) for random i/o,
+ * but            performance(SSD) ~= performance(HDD) for sequential i/o,
+ * it may be optimal to save (presumably expensive) SSD cache space for random i/o only.
+ *
+ * We don't know whether a single request is part of a big sequential read/write.
+ * So all we can do is monitor a few requests, and try to spot if they are
+ * continuations of a recent 'flow' of i/o.  After several contiguous blocks we consider
+ * it sequential.
+ *
+ * You can tune the threshold with the sysctl skip_seq_thresh_kb (e.g. 64 = 64kb),
+ * or cache all i/o (without checking whether random or sequential) with skip_seq_thresh_kb = 0.
+ */
+int 
+skip_sequential_io(struct cache_c *dmc, struct bio *bio)
+{
+	struct sequential_io *seqio;
+	int sequential = 0;	/* Saw > 1 in a row? */
+	int skip       = 0;	/* Enough sequential to hit the threshold */
+
+	/* sysctl skip sequential threshold = 0 : disable, cache all sequential and random i/o.
+	 * This is the default. */	 
+	if (dmc->sysctl_skip_seq_thresh_kb == 0)  {
+		skip = 0;	/* Redundant, for emphasis */
+		goto out;
+	}
+
+	/* locking : We are already within cache_spin_lock so we don't
+	 * need to explicitly lock our data structures.
+ 	 */
+
+	/* Is it a continuation of recent i/o?  Try to find a match.  */
+	DPRINTK("skip_sequential_io: searching for %ld", bio->bi_sector);
+	/* search the list in LRU order so single sequential flow hits first slot */
+	for (seqio = dmc->seq_io_head; seqio != NULL && sequential == 0; seqio = seqio->next) { 
+
+		if (bio->bi_sector == seqio->most_recent_sector) {
+			/* Reread or write same sector again.  Ignore but move to head */
+			DPRINTK("skip_sequential_io: repeat");
+			sequential = 1;
+			if (dmc->seq_io_head != seqio)
+				seq_io_move_to_lruhead(dmc, seqio);
+		}
+		/* i/o to one block more than the previous i/o = sequential */	
+		else if (bio->bi_sector == seqio->most_recent_sector + dmc->block_size) {
+			DPRINTK("skip_sequential_io: sequential found");
+			/* Update stats.  */
+			seqio->most_recent_sector = bio->bi_sector;
+			seqio->sequential_count++;
+			sequential = 1;
+
+			/* And move to head, if not head already */
+			if (dmc->seq_io_head != seqio)
+				seq_io_move_to_lruhead(dmc, seqio);
+
+			/* Is it now sequential enough to be sure? (threshold expressed in kb) */
+			if (to_bytes(seqio->sequential_count * dmc->block_size) > dmc->sysctl_skip_seq_thresh_kb * 1024) {
+				DPRINTK("skip_sequential_io: Sequential i/o detected, seq count now %lu", 
+					seqio->sequential_count);
+				/* Sufficiently sequential */
+				skip = 1;
+			}
+		}
+	}
+	if (!sequential) {
+		/* Record the start of some new i/o, maybe we'll spot it as 
+		 * sequential soon.  */
+		DPRINTK("skip_sequential_io: concluded that its random i/o");
+
+		seqio = dmc->seq_io_tail;
+		seq_io_move_to_lruhead(dmc, seqio);
+
+		DPRINTK("skip_sequential_io: fill in data");
+
+		/* Fill in data */
+		seqio->most_recent_sector = bio->bi_sector;
+		seqio->sequential_count	  = 1;
+	}
+	DPRINTK("skip_sequential_io: complete.");
+out:
+	if (skip) {
+		if (bio_data_dir(bio) == READ)
+	        	dmc->flashcache_stats.uncached_sequential_reads++;
+		else 
+	        	dmc->flashcache_stats.uncached_sequential_writes++;
+	}
+
+	return skip;
+}
+
+
+/*
+ * Add/del pids whose IOs should be non-cacheable.
+ * We limit this number to 100 (arbitrary and sysctl'able).
+ * We also add an expiry to each entry (defaluts at 60 sec,
+ * arbitrary and sysctlable).
+ * This is needed because Linux lacks an "at_exit()" hook
+ * that modules can supply to do any cleanup on process 
+ * exit, for cases where the process dies after marking itself
+ * non-cacheable.
+ */
+int 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		 struct file *filp, unsigned int cmd,
+		 unsigned long arg)
+#else
+flashcache_ioctl(struct dm_target *ti, unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct block_device *bdev = dmc->disk_dev->bdev;
+	struct file fake_file = {};
+	struct dentry fake_dentry = {};
+	pid_t pid;
+
+	switch(cmd) {
+	case FLASHCACHEADDBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELALLBLACKLIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 0);
+		return 0;
+	case FLASHCACHEADDWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELALLWHITELIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 0);
+		return 0;
+	default:
+		fake_file.f_mode = dmc->disk_dev->mode;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+		fake_file.f_dentry = &fake_dentry;
+#else
+		fake_file.f_path.dentry = &fake_dentry;
+#endif
+		fake_dentry.d_inode = bdev->bd_inode;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+		return blkdev_driver_ioctl(bdev->bd_inode, &fake_file, bdev->bd_disk, cmd, arg);
+#else
+		return __blkdev_driver_ioctl(dmc->disk_dev->bdev, dmc->disk_dev->mode, cmd, arg);
+#endif
+	}
+
+}
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_ioctl.h linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_ioctl.h
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_ioctl.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_ioctl.h	2016-12-13 21:55:12.009000000 +0800
@@ -0,0 +1,70 @@
+/****************************************************************************
+ *  flashcache_ioctl.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_IOCTL_H
+#define FLASHCACHE_IOCTL_H
+
+#include <linux/types.h>
+
+#define FLASHCACHE_IOCTL 0xfe
+
+enum {
+	FLASHCACHEADDNCPID_CMD=200,
+	FLASHCACHEDELNCPID_CMD,
+	FLASHCACHEDELNCALL_CMD,
+	FLASHCACHEADDWHITELIST_CMD,
+	FLASHCACHEDELWHITELIST_CMD,
+	FLASHCACHEDELWHITELISTALL_CMD,
+};
+
+#define FLASHCACHEADDNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCALL	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCALL_CMD, pid_t)
+
+#define FLASHCACHEADDBLACKLIST		FLASHCACHEADDNCPID
+#define FLASHCACHEDELBLACKLIST		FLASHCACHEDELNCPID
+#define FLASHCACHEDELALLBLACKLIST	FLASHCACHEDELNCALL
+
+#define FLASHCACHEADDWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELALLWHITELIST	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELISTALL_CMD, pid_t)
+
+#ifdef __KERNEL__
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+int flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		     struct file *filp, unsigned int cmd,
+		     unsigned long arg);
+#else
+int flashcache_ioctl(struct dm_target *ti, unsigned int cmd,
+ 		     unsigned long arg);
+#endif
+void flashcache_pid_expiry_all_locked(struct cache_c *dmc);
+int flashcache_uncacheable(struct cache_c *dmc, struct bio *bio);
+void seq_io_remove_from_lru(struct cache_c *dmc, struct sequential_io *seqio);
+void seq_io_move_to_lruhead(struct cache_c *dmc, struct sequential_io *seqio);
+int skip_sequential_io(struct cache_c *dmc, struct bio *bio);
+void flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force);
+#endif /* __KERNEL__ */
+
+#endif
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_main.c linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_main.c
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_main.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_main.c	2016-12-13 21:55:12.012000000 +0800
@@ -0,0 +1,2051 @@
+/****************************************************************************
+ *  flashcache_main.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,21)
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#endif
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+#ifndef DM_MAPIO_SUBMITTED
+#define DM_MAPIO_SUBMITTED	0
+#endif
+
+/*
+ * TODO List :
+ * 1) Management of non cache pids : Needs improvement. Remove registration
+ * on process exits (with  a pseudo filesstem'ish approach perhaps) ?
+ * 2) Breaking up the cache spinlock : Right now contention on the spinlock
+ * is not a problem. Might need change in future.
+ * 3) Use the standard linked list manipulation macros instead rolling our own.
+ * 4) Fix a security hole : A malicious process with 'ro' access to a file can 
+ * potentially corrupt file data. This can be fixed by copying the data on a
+ * cache read miss.
+ */
+
+#define FLASHCACHE_SW_VERSION "flashcache-1.0"
+char *flashcache_sw_version = FLASHCACHE_SW_VERSION;
+
+static void flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+				 int index);
+static void flashcache_write(struct cache_c *dmc, struct bio* bio);
+static int flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio);
+static void flashcache_dirty_writeback(struct cache_c *dmc, int index);
+void flashcache_sync_blocks(struct cache_c *dmc);
+static void flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio);
+
+extern struct work_struct _kcached_wq;
+extern u_int64_t size_hist[];
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+extern struct dm_kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#else
+extern struct kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+extern struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions, 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where, 
+#else
+			    struct io_region *where, 
+#endif
+			    int rw, 
+			    struct bio_vec *bvec, io_notify_fn fn, 
+			    void *context)
+{
+	struct dm_io_request iorq;
+
+	iorq.bi_rw = rw;
+	iorq.mem.type = DM_IO_BVEC;
+	iorq.mem.ptr.bvec = bvec;
+	iorq.notify.fn = fn;
+	iorq.notify.context = context;
+	iorq.client = flashcache_io_client;
+	return dm_io(&iorq, num_regions, where, NULL);
+}
+#endif
+
+/* 
+ * A simple 2-hand clock like algorithm is used to identify dirty blocks 
+ * that lie fallow in the cache and thus are candidates for cleaning. 
+ * Note that we could have such fallow blocks in sets where the dirty blocks 
+ * is under the configured threshold.
+ * The hands are spaced fallow_delay seconds apart (one sweep runs every 
+ * fallow_delay seconds).  The interval is configurable via a sysctl. 
+ * Blocks are moved to DIRTY_FALLOW_1, if they are found to be in DIRTY_FALLOW_1
+ * for fallow_delay seconds or more, they are moved to DIRTY_FALLOW_1 | DIRTY_FALLOW_2, 
+ * at which point they are eligible for cleaning. Of course any intervening use
+ * of the block within the interval turns off these 2 bits.
+ * 
+ * Cleaning of these blocks happens from the flashcache_clean_set() function.
+ */
+void
+flashcache_detect_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	if ((cacheblk->cache_state & DIRTY) &&
+	    ((cacheblk->cache_state & BLOCK_IO_INPROG) == 0)) {
+		if ((cacheblk->cache_state & DIRTY_FALLOW_1) == 0)
+			cacheblk->cache_state |= DIRTY_FALLOW_1;
+		else if ((cacheblk->cache_state & DIRTY_FALLOW_2) == 0) {
+			dmc->cache_sets[index / dmc->assoc].dirty_fallow++;
+			cacheblk->cache_state |= DIRTY_FALLOW_2;
+		}
+	}
+}
+
+void
+flashcache_clear_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int set = index / dmc->assoc;
+	
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	if (cacheblk->cache_state & FALLOW_DOCLEAN) {
+		if (cacheblk->cache_state & DIRTY_FALLOW_2) {
+			VERIFY(dmc->cache_sets[set].dirty_fallow > 0);
+			dmc->cache_sets[set].dirty_fallow--;
+		}
+		cacheblk->cache_state &= ~FALLOW_DOCLEAN;
+	}
+}
+
+void 
+flashcache_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+	struct cache_c *dmc = job->dmc;
+	struct bio *bio;
+	unsigned long flags;
+	int index = job->index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	VERIFY(index != -1);		
+	bio = job->bio;
+	VERIFY(bio != NULL);
+	if (unlikely(error)) {
+		error = -EIO;
+		DMERR("flashcache_io_callback: io error %ld block %lu action %d", 
+		      error, job->job_io_regions.disk.sector, job->action);
+	}
+	job->error = error;
+	switch (job->action) {
+	case READDISK:
+		DPRINTK("flashcache_io_callback: READDISK  %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READDISK_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READDISK_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (likely(error == 0)) {
+			/* Kick off the write to the cache */
+			job->action = READFILL;
+			push_io(job);
+			schedule_work(&_kcached_wq);
+			return;
+		} else
+			dmc->flashcache_errors.disk_read_errors++;			
+		break;
+	case READCACHE:
+		DPRINTK("flashcache_io_callback: READCACHE %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READCACHE_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READCACHE_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & CACHEREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_read_errors++;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		if (likely(error == 0)) {
+			if (flashcache_validate_checksum(job)) {
+				DMERR("flashcache_io_callback: Checksum mismatch at disk offset %lu", 
+				      job->job_io_regions.disk.sector);
+				error = -EIO;
+			}
+		}
+#endif
+		break;		       
+	case READFILL:
+		DPRINTK("flashcache_io_callback: READFILL %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READFILL_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READFILL_ERROR;
+		}
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_write_errors++;
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		break;
+	case WRITECACHE:
+		DPRINTK("flashcache_io_callback: WRITECACHE %d",
+			index);
+		if (unlikely(dmc->sysctl_error_inject & WRITECACHE_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~WRITECACHE_ERROR;
+		}
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		VERIFY(cacheblk->cache_state & CACHEWRITEINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (likely(error == 0)) {
+			if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				dmc->flashcache_stats.checksum_store++;
+				flashcache_store_checksum(job);
+				/* 
+				 * We need to update the metadata on a DIRTY->DIRTY as well 
+				 * since we save the checksums.
+				 */
+				flashcache_md_write(job);
+				return;
+#else
+				/* Only do cache metadata update on a non-DIRTY->DIRTY transition */
+				if ((cacheblk->cache_state & DIRTY) == 0) {
+					flashcache_md_write(job);
+					return;
+				}
+#endif
+			} else { /* cache_mode == WRITE_THROUGH */
+				/* Writs to both disk and cache completed */
+				VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				flashcache_store_checksum(job);
+				job->dmc->flashcache_stats.checksum_store++;
+#endif
+			}
+		} else {
+			dmc->flashcache_errors.ssd_write_errors++;
+			if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+				/* 
+				 * We don't know if the IO failed because of a ssd write
+				 * error or a disk write error. Bump up both.
+				 * XXX - TO DO. We could check the error bits and allow
+				 * the IO to succeed as long as the disk write suceeded.
+				 * and invalidate the cache block.
+				 */
+				dmc->flashcache_errors.disk_write_errors++;
+		}
+		break;
+	}
+	flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+	/* 
+	 * The INPROG flag is still set. We cannot turn that off until all the pending requests
+	 * processed. We need to loop the pending requests back to a workqueue. We have the job,
+	 * add it to the pending req queue.
+	 */
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (unlikely(error || cacheblk->nr_queued > 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		push_pending(job);
+		schedule_work(&_kcached_wq);
+	} else {
+		cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_free_cache_job(job);
+		if (atomic_dec_and_test(&dmc->nr_jobs))
+			wake_up(&dmc->destroyq);
+	}
+}
+
+static void
+flashcache_free_pending_jobs(struct cache_c *dmc, struct cacheblock *cacheblk, 
+			     int error)
+{
+	struct pending_job *pending_job, *freelist = NULL;
+
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	freelist = flashcache_deq_pending(dmc, cacheblk - &dmc->cache[0]);
+	while (freelist != NULL) {
+		pending_job = freelist;
+		freelist = pending_job->next;
+		VERIFY(cacheblk->nr_queued > 0);
+		cacheblk->nr_queued--;
+		flashcache_bio_endio(pending_job->bio, error, dmc, NULL);
+		flashcache_free_pending_job(pending_job);
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+}
+
+/* 
+ * Common error handling for everything.
+ * 1) If the block isn't dirty, invalidate it.
+ * 2) Error all pending IOs that totally or partly overlap this block.
+ * 3) Free the job.
+ */
+static void
+flashcache_do_pending_error(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[job->index];
+
+	DMERR("flashcache_do_pending_error: error %d block %lu action %d", 
+	      job->error, job->job_io_regions.disk.sector, job->action);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(cacheblk->cache_state & VALID);
+	/* Invalidate block if possible */
+	if ((cacheblk->cache_state & DIRTY) == 0) {
+		dmc->cached_blocks--;
+		dmc->flashcache_stats.pending_inval++;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+	}
+	flashcache_free_pending_jobs(dmc, cacheblk, job->error);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void
+flashcache_do_pending_noerror(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+	struct pending_job *pending_job, *freelist;
+	int queued;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (cacheblk->cache_state & DIRTY) {
+		VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_BACK);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, index);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_dirty_writeback(dmc, index);
+		goto out;
+	}
+	DPRINTK("flashcache_do_pending: Index %d %lx",
+		index, cacheblk->cache_state);
+	VERIFY(cacheblk->cache_state & VALID);
+	dmc->cached_blocks--;
+	dmc->flashcache_stats.pending_inval++;
+	cacheblk->cache_state &= ~VALID;
+	cacheblk->cache_state |= INVALID;
+	while ((freelist = flashcache_deq_pending(dmc, index)) != NULL) {
+		while (freelist != NULL) {
+			VERIFY(!(cacheblk->cache_state & DIRTY));
+			pending_job = freelist;
+			freelist = pending_job->next;
+			VERIFY(cacheblk->nr_queued > 0);
+			cacheblk->nr_queued--;
+			if (pending_job->action == INVALIDATE) {
+				DPRINTK("flashcache_do_pending: INVALIDATE  %llu",
+					pending_job->bio->bi_sector);
+				VERIFY(pending_job->bio != NULL);
+				queued = flashcache_inval_blocks(dmc, pending_job->bio);
+				if (queued) {
+					if (unlikely(queued < 0)) {
+						/*
+						 * Memory allocation failure inside inval_blocks.
+						 * Fail this io.
+						 */
+						flashcache_bio_endio(pending_job->bio, -EIO, dmc, NULL);
+					}
+					flashcache_free_pending_job(pending_job);
+					continue;
+				}
+			}
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			DPRINTK("flashcache_do_pending: Sending down IO %llu",
+				pending_job->bio->bi_sector);
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, pending_job->bio);
+			flashcache_free_pending_job(pending_job);
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+out:
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+void
+flashcache_do_pending(struct kcached_job *job)
+{
+	if (job->error)
+		flashcache_do_pending_error(job);
+	else
+		flashcache_do_pending_noerror(job);
+}
+
+void
+flashcache_do_io(struct kcached_job *job)
+{
+	struct bio *bio = job->bio;
+	int r = 0;
+	
+	VERIFY(job->action == READFILL);
+	VERIFY(job->action == READFILL);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	flashcache_store_checksum(job);
+	job->dmc->flashcache_stats.checksum_store++;
+#endif
+	/* Write to cache device */
+	job->dmc->flashcache_stats.ssd_writes++;
+	r = dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, bio->bi_io_vec + bio->bi_idx,
+			     flashcache_io_callback, job);
+	VERIFY(r == 0);
+	/* In our case, dm_io_async_bvec() must always return 0 */
+}
+
+/*
+ * Map a block from the source device to a block in the cache device.
+ */
+static unsigned long 
+hash_block(struct cache_c *dmc, sector_t dbn)
+{
+	unsigned long set_number, value;
+
+	value = (unsigned long)
+		(dbn >> (dmc->block_shift + dmc->assoc_shift));
+	set_number = value % dmc->num_sets;
+	DPRINTK("Hash: %llu(%lu)->%lu", dbn, value, set_number);
+	return set_number;
+}
+
+static void
+find_valid_dbn(struct cache_c *dmc, sector_t dbn, 
+	       int start_index, int *valid, int *invalid)
+{
+	int i;
+	int end_index = start_index + dmc->assoc;
+
+	*valid = *invalid = -1;
+	for (i = start_index ; i < end_index ; i++) {
+		if (dbn == dmc->cache[i].dbn &&
+		    (dmc->cache[i].cache_state & VALID)) {
+			*valid = i;
+			if (dmc->sysctl_reclaim_policy == FLASHCACHE_LRU &&
+			    ((dmc->cache[i].cache_state & BLOCK_IO_INPROG) == 0))
+				flashcache_reclaim_lru_movetail(dmc, i);
+			/* 
+			 * If the block was DIRTY and earmarked for cleaning because it was old, make 
+			 * the block young again.
+			 */
+			flashcache_clear_fallow(dmc, i);
+			return;
+		}
+		if (*invalid == -1 && dmc->cache[i].cache_state == INVALID) {
+			VERIFY((dmc->cache[i].cache_state & FALLOW_DOCLEAN) == 0);
+			*invalid = i;
+		}
+	}
+	if (*valid == -1 && *invalid != -1)
+		if (dmc->sysctl_reclaim_policy == FLASHCACHE_LRU)
+			flashcache_reclaim_lru_movetail(dmc, *invalid);
+}
+
+/* Search for a slot that we can reclaim */
+static void
+find_reclaim_dbn(struct cache_c *dmc, int start_index, int *index)
+{
+	int set = start_index / dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+	
+	if (dmc->sysctl_reclaim_policy == FLASHCACHE_FIFO) {
+		int end_index = start_index + dmc->assoc;
+		int slots_searched = 0;
+		int i;
+
+		i = cache_set->set_fifo_next;
+		while (slots_searched < dmc->assoc) {
+			VERIFY(i >= start_index);
+			VERIFY(i < end_index);
+			if (dmc->cache[i].cache_state == VALID) {
+				*index = i;
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				break;
+			}
+			slots_searched++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		i++;
+		if (i == end_index)
+			i = start_index;
+		cache_set->set_fifo_next = i;
+	} else { /* reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if (cacheblk->cache_state == VALID) {
+				VERIFY((cacheblk - &dmc->cache[0]) == 
+				       (lru_rel_index + start_index));
+				*index = cacheblk - &dmc->cache[0];
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				flashcache_reclaim_lru_movetail(dmc, *index);
+				break;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+}
+
+/* 
+ * dbn is the starting sector, io_size is the number of sectors.
+ */
+static int 
+flashcache_lookup(struct cache_c *dmc, struct bio *bio, int *index)
+{
+	sector_t dbn = bio->bi_sector;
+#if DMC_DEBUG
+	int io_size = to_sector(bio->bi_size);
+#endif
+	unsigned long set_number = hash_block(dmc, dbn);
+	int invalid, oldest_clean = -1;
+	int start_index;
+
+	start_index = dmc->assoc * set_number;
+	DPRINTK("Cache lookup : dbn %llu(%lu), set = %d",
+		dbn, io_size, set_number);
+	find_valid_dbn(dmc, dbn, start_index, index, &invalid);
+	if (*index >= 0) {
+		DPRINTK("Cache lookup HIT: Block %llu(%lu): VALID index %d",
+			     dbn, io_size, *index);
+		/* We found the exact range of blocks we are looking for */
+		return VALID;
+	}
+	if (invalid == -1) {
+		/* We didn't find an invalid entry, search for oldest valid entry */
+		find_reclaim_dbn(dmc, start_index, &oldest_clean);
+	}
+	/* 
+	 * Cache miss :
+	 * We can't choose an entry marked INPROG, but choose the oldest
+	 * INVALID or the oldest VALID entry.
+	 */
+	*index = start_index + dmc->assoc;
+	if (invalid != -1) {
+		DPRINTK("Cache lookup MISS (INVALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, invalid, start_index);
+		*index = invalid;
+	} else if (oldest_clean != -1) {
+		DPRINTK("Cache lookup MISS (VALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, oldest_clean, start_index);
+		*index = oldest_clean;
+	} else {
+		DPRINTK_LITE("Cache read lookup MISS (NOROOM): dbn %llu(%lu), set = %d",
+			dbn, io_size, set_number);
+	}
+	if (*index < (start_index + dmc->assoc))
+		return INVALID;
+	else {
+		dmc->flashcache_stats.noroom++;
+		return -1;
+	}
+}
+
+/*
+ * Cache Metadata Update functions 
+ */
+void 
+flashcache_md_write_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_md_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static int
+flashcache_alloc_md_sector(struct kcached_job *job)
+{
+	struct page *page = NULL;
+	struct cache_c *dmc = job->dmc;	
+	
+	if (likely((dmc->sysctl_error_inject & MD_ALLOC_SECTOR_ERROR) == 0)) {
+		unsigned long addr;
+
+		/* Get physically consecutive pages */
+		addr = __get_free_pages(GFP_NOIO, get_order(MD_BLOCK_BYTES(job->dmc)));
+		if (addr)
+			page = virt_to_page(addr);
+	} else
+		dmc->sysctl_error_inject &= ~MD_ALLOC_SECTOR_ERROR;
+	job->md_io_bvec.bv_page = page;
+	if (unlikely(page == NULL)) {
+		job->dmc->flashcache_errors.memory_alloc_errors++;
+		return -ENOMEM;
+	}
+	job->md_io_bvec.bv_len = MD_BLOCK_BYTES(job->dmc);
+	job->md_io_bvec.bv_offset = 0;
+	job->md_block = (struct flash_cacheblock *)page_address(page);
+	return 0;
+}
+
+static void
+flashcache_free_md_sector(struct kcached_job *job)
+{
+	if (job->md_io_bvec.bv_page != NULL)
+		__free_pages(job->md_io_bvec.bv_page, get_order(MD_BLOCK_BYTES(job->dmc)));
+}
+
+void
+flashcache_md_write_kickoff(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;	
+	struct flash_cacheblock *md_block;
+	int md_block_ix;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i;
+	struct cache_md_block_head *md_block_head;
+	struct kcached_job *orig_job = job;
+	unsigned long flags;
+
+	if (flashcache_alloc_md_sector(job)) {
+		DMERR("flashcache: %d: Cache metadata write failed, cannot alloc page ! block %lu", 
+		      job->action, job->job_io_regions.disk.sector);
+		flashcache_md_write_callback(-EIO, job);
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/*
+	 * Transfer whatever is on the pending queue to the md_io_inprog queue.
+	 */
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	md_block_head->md_io_inprog = md_block_head->queued_updates;
+	md_block_head->queued_updates = NULL;
+	md_block = job->md_block;
+	md_block_ix = INDEX_TO_MD_BLOCK(dmc, job->index) * MD_SLOTS_PER_BLOCK(dmc);
+	/* First copy out the entire md block */
+	for (i = 0 ; 
+	     i < MD_SLOTS_PER_BLOCK(dmc) && md_block_ix < dmc->size ; 
+	     i++, md_block_ix++) {
+		md_block[i].dbn = dmc->cache[md_block_ix].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		md_block[i].checksum = dmc->cache[md_block_ix].checksum;
+#endif
+		md_block[i].cache_state = 
+			dmc->cache[md_block_ix].cache_state & (VALID | INVALID | DIRTY);
+	}
+	/* Then set/clear the DIRTY bit for the "current" index */
+	if (job->action == WRITECACHE) {
+		/* DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = 
+			(VALID | DIRTY);
+	} else { /* job->action == WRITEDISK* */
+		/* un-DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+	}
+
+	for (job = md_block_head->md_io_inprog ; 
+	     job != NULL ;
+	     job = job->next) {
+		dmc->flashcache_stats.md_write_batch++;
+		if (job->action == WRITECACHE) {
+			/* DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = 
+				(VALID | DIRTY);
+		} else { /* job->action == WRITEDISK* */
+			/* un-DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+		}
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	where.bdev = dmc->cache_dev->bdev;
+	where.count = MD_SECTORS_PER_BLOCK(dmc);
+	where.sector = (1 + INDEX_TO_MD_BLOCK(dmc, orig_job->index)) * MD_SECTORS_PER_BLOCK(dmc);
+	dmc->flashcache_stats.ssd_writes++;
+	dmc->flashcache_stats.md_ssd_writes++;
+	dm_io_async_bvec(1, &where, WRITE,
+			 &orig_job->md_io_bvec,
+			 flashcache_md_write_callback, orig_job);
+}
+
+void
+flashcache_md_write_done(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	int index;
+	unsigned long flags;
+	struct kcached_job *job_list;
+	int error = job->error;
+	struct kcached_job *next;
+	struct cacheblock *cacheblk;
+		
+	VERIFY(!in_interrupt());
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE || 
+	       job->action == WRITEDISK_SYNC);
+	flashcache_free_md_sector(job);
+	job->md_block = NULL;
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	job_list = job;
+	job->next = md_block_head->md_io_inprog;
+	md_block_head->md_io_inprog = NULL;
+	for (job = job_list ; job != NULL ; job = next) {
+		next = job->next;
+		job->error = error;
+		index = job->index;
+		cacheblk = &dmc->cache[index];
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (job->action == WRITECACHE) {
+			if (unlikely(dmc->sysctl_error_inject & WRITECACHE_MD_ERROR)) {
+				job->error = -EIO;
+				dmc->sysctl_error_inject &= ~WRITECACHE_MD_ERROR;
+			}
+			if (likely(job->error == 0)) {
+				if ((cacheblk->cache_state & DIRTY) == 0) {
+					dmc->cache_sets[index / dmc->assoc].nr_dirty++;
+					dmc->nr_dirty++;
+				}
+				dmc->flashcache_stats.md_write_dirty++;
+				cacheblk->cache_state |= DIRTY;
+			} else
+				dmc->flashcache_errors.ssd_write_errors++;
+			flashcache_bio_endio(job->bio, job->error, dmc, &job->io_start_time);
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: WRITE: Cache metadata write failed ! error %d block %lu", 
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+			}
+		} else {
+			int action = job->action;
+
+			if (unlikely(dmc->sysctl_error_inject & WRITEDISK_MD_ERROR)) {
+				job->error = -EIO;
+				dmc->sysctl_error_inject &= ~WRITEDISK_MD_ERROR;
+			}
+			/*
+			 * If we have an error on a WRITEDISK*, no choice but to preserve the 
+			 * dirty block in cache. Fail any IOs for this block that occurred while
+			 * the block was being cleaned.
+			 */
+			if (likely(job->error == 0)) {
+				dmc->flashcache_stats.md_write_clean++;
+				cacheblk->cache_state &= ~DIRTY;
+				VERIFY(dmc->cache_sets[index / dmc->assoc].nr_dirty > 0);
+				VERIFY(dmc->nr_dirty > 0);
+				dmc->cache_sets[index / dmc->assoc].nr_dirty--;
+				dmc->nr_dirty--;
+			} else 
+				dmc->flashcache_errors.ssd_write_errors++;
+			VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+			VERIFY(dmc->clean_inprog > 0);
+			dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+			dmc->clean_inprog--;
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: CLEAN: Cache metadata write failed ! error %d block %lu", 
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+			}
+			/* Kick off more cleanings */
+			if (action == WRITEDISK)
+				flashcache_clean_set(dmc, index / dmc->assoc);
+			else
+				flashcache_sync_blocks(dmc);
+			dmc->flashcache_stats.cleanings++;
+			if (action == WRITEDISK_SYNC)
+				flashcache_update_sync_progress(dmc);
+		}
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (md_block_head->queued_updates != NULL) {
+		/* peel off the first job from the pending queue and kick that off */
+		job = md_block_head->queued_updates;
+		md_block_head->queued_updates = job->next;
+		job->next = NULL;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		VERIFY(job->action == WRITEDISK || job->action == WRITECACHE ||
+		       job->action == WRITEDISK_SYNC);
+		flashcache_md_write_kickoff(job);
+	} else {
+		md_block_head->nr_in_prog = 0;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	}
+}
+
+/* 
+ * Kick off a cache metadata update (called from workqueue).
+ * Cache metadata update IOs to a given metadata sector are serialized using the 
+ * nr_in_prog bit in the md sector bufhead.
+ * If a metadata IO is already in progress, we queue up incoming metadata updates
+ * on the pending_jobs list of the md sector bufhead. When kicking off an IO, we
+ * cluster all these pending updates and do all of them as 1 flash write (that 
+ * logic is in md_write_kickoff), where it switches out the entire pending_jobs
+ * list and does all of those updates as 1 ssd write.
+ */
+void
+flashcache_md_write(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	unsigned long flags;
+	
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE || 
+	       job->action == WRITEDISK_SYNC);
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/* If a write is in progress for this metadata sector, queue this update up */
+	if (md_block_head->nr_in_prog != 0) {
+		struct kcached_job **nodepp;
+		
+		/* A MD update is already in progress, queue this one up for later */
+		nodepp = &md_block_head->queued_updates;
+		while (*nodepp != NULL)
+			nodepp = &((*nodepp)->next);
+		job->next = NULL;
+		*nodepp = job;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	} else {
+		md_block_head->nr_in_prog = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/*
+		 * Always push to a worker thread. If the driver has
+		 * a completion thread, we could end up deadlocking even
+		 * if the context would be safe enough to write from.
+		 * This could be executed from the context of an IO 
+		 * completion thread. Kicking off the write from that
+		 * context could result in the IO completion thread 
+		 * blocking (eg on memory allocation). That can easily
+		 * deadlock.
+		 */
+		push_md_io(job);
+		schedule_work(&_kcached_wq);
+	}
+}
+
+static void 
+flashcache_kcopyd_callback(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (unlikely(dmc->sysctl_error_inject & KCOPYD_CALLBACK_ERROR)) {
+		read_err = -EIO;
+		dmc->sysctl_error_inject &= ~KCOPYD_CALLBACK_ERROR;
+	}
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this block from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu", 
+		      -read_err, -write_err, job->job_io_regions.disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_clean_set(dmc, index / dmc->assoc); /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+	
+	DPRINTK("flashcache_dirty_writeback: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	if (unlikely(dmc->sysctl_error_inject & DIRTY_WRITEBACK_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~DIRTY_WRITEBACK_JOB_ALLOC_FAIL;
+	}
+	/*
+	 * If the device is being removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog))) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu", 
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for set cleaning) failed ! Can't allocate memory, block %lu", 
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback, 
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback, 
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+			       (dm_kcopyd_notify_fn) flashcache_kcopyd_callback, 
+			       (void *)job);
+#endif
+	}
+}
+
+/*
+ * This function encodes the background disk cleaning logic.
+ * Background disk cleaning is triggered for 2 reasons.
+ A) Dirty blocks are lying fallow in the set, making them good 
+    candidates for being cleaned.
+ B) This set has dirty blocks over the configured threshold 
+    for a set.
+ * (A) takes precedence over (B). Fallow dirty blocks are cleaned
+ * first.
+ * The cleaning of disk blocks is subject to the write limits per
+ * set and across the cache, which this function enforces.
+ *
+ * 1) Select the n blocks that we want to clean (choosing whatever policy), 
+ *    sort them.
+ * 2) Then sweep the entire set looking for other DIRTY blocks that can be 
+ *    tacked onto any of these blocks to form larger contigous writes. 
+ *    The idea here is that if you are going to do a write anyway, then we 
+ *    might as well opportunistically write out any contigous blocks for 
+ *    free.
+ */
+
+/* Are we under the limits for disk cleaning ? */
+static inline int
+flashcache_can_clean(struct cache_c *dmc, 
+		     struct cache_set *cache_set,
+		     int nr_writes)
+{
+	return ((cache_set->clean_inprog + nr_writes) < dmc->max_clean_ios_set &&
+		(nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total);
+}
+
+void
+flashcache_clean_set(struct cache_c *dmc, int set)
+{
+	unsigned long flags;
+	int threshold_clean = 0;
+	struct dbn_index_pair *writes_list;
+	int nr_writes = 0, i;
+	int start_index = set * dmc->assoc; 
+	int end_index = start_index + dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+	int do_delayed_clean = 0;
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	/* 
+	 * If a removal of this device is in progress, don't kick off 
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if (atomic_read(&dmc->remove_in_prog))
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (unlikely(dmc->sysctl_error_inject & WRITES_LIST_ALLOC_FAIL)) {
+		if (writes_list)
+			kfree(writes_list);
+		writes_list = NULL;
+		dmc->sysctl_error_inject &= ~WRITES_LIST_ALLOC_FAIL;
+	}
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/* 
+	 * Before we try to clean any blocks, check the last time the fallow block
+	 * detection was done. If it has been more than "fallow_delay" seconds, make 
+	 * a sweep through the set to detect (mark) fallow blocks.
+	 */
+	if (dmc->sysctl_fallow_delay && time_after(jiffies, cache_set->fallow_tstamp)) {
+		for (i = start_index ; i < end_index ; i++)
+			flashcache_detect_fallow(dmc, i);
+		cache_set->fallow_tstamp = jiffies + dmc->sysctl_fallow_delay * HZ;
+	}
+	/* If there are any dirty fallow blocks, clean them first */
+	for (i = start_index ; 
+	     (dmc->sysctl_fallow_delay > 0 &&
+	      cache_set->dirty_fallow > 0 &&
+	      time_after(jiffies, cache_set->fallow_next_cleaning) &&
+	      i < end_index) ; 
+	     i++) {
+		cacheblk = &dmc->cache[i];
+		if (!(cacheblk->cache_state & DIRTY_FALLOW_2))
+			continue;
+		if (!flashcache_can_clean(dmc, cache_set, nr_writes)) {
+			/*
+			 * There are fallow blocks that need cleaning, but we 
+			 * can't clean them this pass, schedule delayed cleaning 
+			 * later.
+			 */
+			do_delayed_clean = 1;
+			goto out;
+		}
+		VERIFY(cacheblk->cache_state & DIRTY);
+		VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == 0);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, i);
+		writes_list[nr_writes].dbn = cacheblk->dbn;
+		writes_list[nr_writes].index = i;
+		dmc->flashcache_stats.fallow_cleanings++;
+		nr_writes++;
+	}
+	if (nr_writes > 0)
+		cache_set->fallow_next_cleaning = jiffies + HZ / dmc->sysctl_fallow_clean_speed;
+	if (cache_set->nr_dirty < dmc->dirty_thresh_set ||
+	    !flashcache_can_clean(dmc, cache_set, nr_writes))
+		goto out;
+	/*
+	 * We picked up all the dirty fallow blocks we can. We can still clean more to 
+	 * remain under the dirty threshold. Clean some more blocks.
+	 */
+	threshold_clean = cache_set->nr_dirty - dmc->dirty_thresh_set;
+	if (dmc->sysctl_reclaim_policy == FLASHCACHE_FIFO) {
+		int scanned;
+		
+		scanned = 0;
+		i = cache_set->set_clean_next;
+		DPRINTK("flashcache_clean_set: Set %d", set);
+		while (scanned < dmc->assoc &&
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[i];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {	
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = i;
+				nr_writes++;
+			}
+			scanned++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		cache_set->set_clean_next = i;
+	} else { /* reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL && 
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = cacheblk - &dmc->cache[0];
+				nr_writes++;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+out:
+	if (nr_writes > 0) {
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		dmc->flashcache_stats.clean_set_ios += nr_writes;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback(dmc, writes_list[i].index);
+	} else {
+		if (cache_set->nr_dirty > dmc->dirty_thresh_set)
+			do_delayed_clean = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (do_delayed_clean)
+			schedule_delayed_work(&dmc->delayed_clean, 1*HZ);
+	}
+	kfree(writes_list);
+}
+
+static void
+flashcache_read_hit(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+
+	cacheblk = &dmc->cache[index];
+	/* If block is busy, queue IO pending completion of in-progress IO */
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		struct kcached_job *job;
+			
+		cacheblk->cache_state |= CACHEREADINPROG;
+		dmc->flashcache_stats.read_hits++;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+			bio->bi_sector, bio->bi_size, index, "CACHE HIT");
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(dmc->sysctl_error_inject & READ_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			dmc->sysctl_error_inject &= ~READ_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/* 
+			 * We have a read hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any 
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Read (hit) failed ! Can't allocate memory for cache IO, block %lu", 
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			job->action = READCACHE; /* Fetch data from cache */
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_reads++;
+			dm_io_async_bvec(1, &job->job_io_regions.cache, READ,
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(dmc->sysctl_error_inject & READ_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			dmc->sysctl_error_inject &= ~READ_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (pjob == NULL)
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, READCACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+		     int index)
+{
+	struct kcached_job *job;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(dmc->sysctl_error_inject & READ_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~READ_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/* 
+		 * We have a read miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any 
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Read (miss) failed ! Can't allocate memory for cache IO, block %lu", 
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		job->action = READDISK; /* Fetch data from the source device */
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.disk_reads++;
+		dm_io_async_bvec(1, &job->job_io_regions.disk, READ,
+				 bio->bi_io_vec + bio->bi_idx,
+				 flashcache_io_callback, job);
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_read(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+	
+	DPRINTK("Got a %s for %llu (%u bytes)",
+	        (bio_rw(bio) == READ ? "READ":"READA"), 
+		bio->bi_sector, bio->bi_size);
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	/* Cache Read Hit case */
+	if (res > 0) {
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & VALID) && 
+		    (cacheblk->dbn == bio->bi_sector)) {
+			flashcache_read_hit(dmc, bio, index);
+			return;
+		}
+	}
+	/*
+	 * In all cases except for a cache hit (and VALID), test for potential 
+	 * invalidations that we need to do.
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+
+	if (res == -1 || flashcache_uncacheable(dmc, bio)) {
+		/* No room , non-cacheable or sequential i/o means not wanted in cache */
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu):%s",
+			bio->bi_sector, bio->bi_size, "CACHE MISS & NO ROOM");
+		if (res == -1)
+			flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+		/* Start uncached IO */
+		flashcache_start_uncached_io(dmc, bio);
+		return;
+	}
+	/* 
+	 * (res == INVALID) Cache Miss 
+	 * And we found cache blocks to replace
+	 * Claim the cache blocks before giving up the spinlock
+	 */
+	if (dmc->cache[index].cache_state & VALID)
+		dmc->flashcache_stats.replace++;
+	else
+		dmc->cached_blocks++;
+	dmc->cache[index].cache_state = VALID | DISKREADINPROG;
+	dmc->cache[index].dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+
+	DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+		bio->bi_sector, bio->bi_size, index, "CACHE MISS & REPLACE");
+	flashcache_read_miss(dmc, bio, index);
+}
+
+/*
+ * Invalidate any colliding blocks if they are !BUSY and !DIRTY. If the colliding
+ * block is DIRTY, we need to kick off a write. In both cases, we need to wait 
+ * until the underlying IO is finished, and then proceed with the invalidation.
+ */
+static int
+flashcache_inval_block_set(struct cache_c *dmc, int set, struct bio *bio, int rw,
+			   struct pending_job *pjob)
+{
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_index, end_index, i;
+	struct cacheblock *cacheblk;
+	
+	start_index = dmc->assoc * set;
+	end_index = start_index + dmc->assoc;
+	for (i = start_index ; i < end_index ; i++) {
+		sector_t start_dbn = dmc->cache[i].dbn;
+		sector_t end_dbn = start_dbn + dmc->block_size;
+		
+		cacheblk = &dmc->cache[i];
+		if (cacheblk->cache_state & INVALID)
+			continue;
+		if ((io_start >= start_dbn && io_start < end_dbn) ||
+		    (io_end >= start_dbn && io_end < end_dbn)) {
+			/* We have a match */
+			if (rw == WRITE)
+				dmc->flashcache_stats.wr_invalidates++;
+			else
+				dmc->flashcache_stats.rd_invalidates++;
+			if (!(cacheblk->cache_state & (BLOCK_IO_INPROG | DIRTY)) &&
+			    (cacheblk->nr_queued == 0)) {
+				dmc->cached_blocks--;			
+				DPRINTK("Cache invalidate (!BUSY): Block %llu %lx",
+					start_dbn, cacheblk->cache_state);
+				cacheblk->cache_state = INVALID;
+				continue;
+			}
+			/*
+			 * The conflicting block has either IO in progress or is 
+			 * Dirty. In all cases, we need to add ourselves to the 
+			 * pending queue. Then if the block is dirty, we kick off
+			 * an IO to clean the block. 
+			 * Note that if the block is dirty and IO is in progress
+			 * on it, the do_pending handler will clean the block
+			 * and then process the pending queue.
+			 */
+			flashcache_enq_pending(dmc, bio, i, INVALIDATE, pjob);
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				/* 
+				 * Kick off block write.
+				 * We can't kick off the write under the spinlock.
+				 * Instead, we mark the slot DISKWRITEINPROG, drop 
+				 * the spinlock and kick off the write. A block marked
+				 * DISKWRITEINPROG cannot change underneath us. 
+				 * to enqueue ourselves onto it's pending queue.
+				 *
+				 * XXX - The dropping of the lock here can be avoided if
+				 * we punt the cleaning of the block to the worker thread,
+				 * at the cost of a context switch.
+				 */
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				spin_unlock_irq(&dmc->cache_spin_lock);
+				flashcache_dirty_writeback(dmc, i); /* Must inc nr_jobs */
+				spin_lock_irq(&dmc->cache_spin_lock);
+			}
+			return 1;
+		}
+	}
+	return 0;
+}
+
+/* 
+ * Since md will break up IO into blocksize pieces, we only really need to check 
+ * the start set and the end set for overlaps.
+ */
+static int
+flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio)
+{	
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_set, end_set;
+	int queued;
+	struct pending_job *pjob1, *pjob2;
+
+	pjob1 = flashcache_alloc_pending_job(dmc);
+	if (unlikely(dmc->sysctl_error_inject & INVAL_PENDING_JOB_ALLOC_FAIL)) {
+		if (pjob1) {
+			flashcache_free_pending_job(pjob1);
+			pjob1 = NULL;
+		}
+		dmc->sysctl_error_inject &= ~INVAL_PENDING_JOB_ALLOC_FAIL;
+	}
+	if (pjob1 == NULL) {
+		queued = -ENOMEM;
+		goto out;
+	}
+	pjob2 = flashcache_alloc_pending_job(dmc);
+	if (pjob2 == NULL) {
+		flashcache_free_pending_job(pjob1);
+		queued = -ENOMEM;
+		goto out;
+	}
+	start_set = hash_block(dmc, io_start);
+	end_set = hash_block(dmc, io_end);
+	queued = flashcache_inval_block_set(dmc, start_set, bio, 
+					    bio_data_dir(bio), pjob1);
+	if (queued) {
+		flashcache_free_pending_job(pjob2);
+		goto out;
+	} else
+		flashcache_free_pending_job(pjob1);		
+	if (start_set != end_set) {
+		queued = flashcache_inval_block_set(dmc, end_set, 
+						    bio, bio_data_dir(bio), pjob2);
+		if (!queued)
+			flashcache_free_pending_job(pjob2);
+	} else
+		flashcache_free_pending_job(pjob2);		
+out:
+	return queued;
+}
+
+static void
+flashcache_write_miss(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct kcached_job *job;
+	int queued;
+
+	cacheblk = &dmc->cache[index];
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+	if (cacheblk->cache_state & VALID)
+		dmc->flashcache_stats.wr_replace++;
+	else
+		dmc->cached_blocks++;
+	cacheblk->cache_state = VALID | CACHEWRITEINPROG;
+	cacheblk->dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(dmc->sysctl_error_inject & WRITE_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~WRITE_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/* 
+		 * We have a write miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any 
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Write (miss) failed ! Can't allocate memory for cache IO, block %lu", 
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_writes++;
+		job->action = WRITECACHE; 
+		if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+			/* Write data to the cache */		
+			dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, 
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		} else {
+			VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+			/* Write data to both disk and cache */
+			dm_io_async_bvec(2, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+					 (struct io_region *)&job->job_io_regions, 
+#else
+					 (struct dm_io_region *)&job->job_io_regions, 
+#endif
+					 WRITE, 
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		}
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_write_hit(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+	struct kcached_job *job;
+
+	cacheblk = &dmc->cache[index];
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		if (cacheblk->cache_state & DIRTY)
+			dmc->flashcache_stats.dirty_write_hits++;
+		dmc->flashcache_stats.write_hits++;
+		cacheblk->cache_state |= CACHEWRITEINPROG;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(dmc->sysctl_error_inject & WRITE_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			dmc->sysctl_error_inject &= ~WRITE_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/* 
+			 * We have a write hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any 
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Write (hit) failed ! Can't allocate memory for cache IO, block %lu", 
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			DPRINTK("Queue job for %llu", bio->bi_sector);
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_writes++;
+			job->action = WRITECACHE;
+			if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+				/* Write data to the cache */
+				dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, 
+						 bio->bi_io_vec + bio->bi_idx,
+						 flashcache_io_callback, job);
+				flashcache_clean_set(dmc, index / dmc->assoc);
+			} else {
+				VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+				/* Write data to both disk and cache */
+				dmc->flashcache_stats.disk_writes++;
+				dm_io_async_bvec(2, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+						 (struct io_region *)&job->job_io_regions, 
+#else
+						 (struct dm_io_region *)&job->job_io_regions, 
+#endif
+						 WRITE, 
+						 bio->bi_io_vec + bio->bi_idx,
+						 flashcache_io_callback, job);				
+			}
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(dmc->sysctl_error_inject & WRITE_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			dmc->sysctl_error_inject &= ~WRITE_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(pjob == NULL))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, WRITECACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_write(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+	
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	if (res != -1) {
+		/* Cache Hit */
+		cacheblk = &dmc->cache[index];		
+		if ((cacheblk->cache_state & VALID) && 
+		    (cacheblk->dbn == bio->bi_sector)) {
+			/* Cache Hit */
+			flashcache_write_hit(dmc, bio, index);
+		} else {
+			/* Cache Miss, found block to recycle */
+			flashcache_write_miss(dmc, bio, index);
+		}
+		return;
+	}
+	/*
+	 * No room in the set. We cannot write to the cache and have to 
+	 * send the request to disk. Before we do that, we must check 
+	 * for potential invalidations !
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	/* Start uncached IO */
+	flashcache_start_uncached_io(dmc, bio);
+	flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,32)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+#define bio_barrier(bio)        ((bio)->bi_rw & (1 << BIO_RW_BARRIER))
+#else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+#define bio_barrier(bio)        ((bio)->bi_rw & REQ_HARDBARRIER)
+#else
+#define bio_barrier(bio)        ((bio)->bi_rw & REQ_FLUSH)
+#endif
+#endif
+#endif
+
+/*
+ * Decide the mapping and perform necessary cache operations for a bio request.
+ */
+int 
+flashcache_map(struct dm_target *ti, struct bio *bio,
+	       union map_info *map_context)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	int sectors = to_sector(bio->bi_size);
+	int queued;
+	
+	if (sectors <= 32)
+		size_hist[sectors]++;
+
+	if (bio_barrier(bio))
+		return -EOPNOTSUPP;
+
+	VERIFY(to_sector(bio->bi_size) <= dmc->block_size);
+
+	if (bio_data_dir(bio) == READ)
+		dmc->flashcache_stats.reads++;
+	else
+		dmc->flashcache_stats.writes++;
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	if (unlikely(dmc->sysctl_pid_do_expiry && 
+		     (dmc->whitelist_head || dmc->blacklist_head)))
+		flashcache_pid_expiry_all_locked(dmc);
+	if ((to_sector(bio->bi_size) != dmc->block_size) ||
+	    (bio_data_dir(bio) == WRITE && 
+	     (dmc->cache_mode == FLASHCACHE_WRITE_AROUND || flashcache_uncacheable(dmc, bio)))) {
+		queued = flashcache_inval_blocks(dmc, bio);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		if (queued) {
+			if (unlikely(queued < 0))
+				flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		} else {
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, bio);
+		}
+	} else {
+		spin_unlock_irq(&dmc->cache_spin_lock);		
+		if (bio_data_dir(bio) == READ)
+			flashcache_read(dmc, bio);
+		else
+			flashcache_write(dmc, bio);
+	}
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* Block sync support functions */
+static void 
+flashcache_kcopyd_callback_sync(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback_sync: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this cache from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu", 
+		      -read_err, -write_err, job->job_io_regions.disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;			
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_sync_blocks(dmc);  /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback_sync(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+	
+	VERIFY((cacheblk->cache_state & FALLOW_DOCLEAN) == 0);
+	DPRINTK("flashcache_dirty_writeback_sync: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	/*
+	 * If the device is being (fast) removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog) == FAST_REMOVE)) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu", 
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for sync) failed ! Can't allocate memory, block %lu", 
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK_SYNC;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback_sync,
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback_sync, 
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+			       (dm_kcopyd_notify_fn)flashcache_kcopyd_callback_sync, 
+			       (void *)job);
+#endif
+	}
+}
+
+/* 
+ * Sync all dirty blocks. We pick off dirty blocks, sort them, merge them with 
+ * any contigous blocks we can within the set and fire off the writes.
+ */
+void
+flashcache_sync_blocks(struct cache_c *dmc)
+{
+	unsigned long flags;
+	int index;
+	struct dbn_index_pair *writes_list;
+	int nr_writes;
+	int i, set;
+	struct cacheblock *cacheblk;
+
+	/* 
+	 * If a (fast) removal of this device is in progress, don't kick off 
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback_sync() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if ((atomic_read(&dmc->remove_in_prog) == FAST_REMOVE) || 
+	    dmc->sysctl_stop_sync)
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	nr_writes = 0;
+	set = -1;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);	
+	index = dmc->sync_index;
+	while (index < dmc->size && 
+	       (nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total) {
+		VERIFY(nr_writes <= dmc->assoc);
+		if (((index % dmc->assoc) == 0) && (nr_writes > 0)) {
+			/*
+			 * Crossing a set, sort/merge all the IOs collected so
+			 * far and issue the writes.
+			 */
+			VERIFY(set != -1);
+			flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			for (i = 0 ; i < nr_writes ; i++)
+				flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+			nr_writes = 0;
+			set = -1;
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			cacheblk->cache_state |= DISKWRITEINPROG;
+			flashcache_clear_fallow(dmc, index);
+			writes_list[nr_writes].dbn = cacheblk->dbn;
+			writes_list[nr_writes].index = index;
+			set = index / dmc->assoc;
+			nr_writes++;
+		}
+		index++;
+	}
+	dmc->sync_index = index;
+	if (nr_writes > 0) {
+		VERIFY(set != -1);
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+	} else
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	kfree(writes_list);
+}
+
+void
+flashcache_sync_all(struct cache_c *dmc)
+{
+	unsigned long flags;
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	dmc->sysctl_stop_sync = 0;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	dmc->sync_index = 0;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);	
+	flashcache_sync_blocks(dmc);
+}
+
+/*
+ * We handle uncached IOs ourselves to deal with the problem of out of ordered
+ * IOs corrupting the cache. Consider the case where we get 2 concurent IOs
+ * for the same block Write-Read (or a Write-Write). Consider the case where
+ * the first Write is uncacheable and the second IO is cacheable. If the 
+ * 2 IOs are out-of-ordered below flashcache, then we will cache inconsistent
+ * data in flashcache (persistently).
+ * 
+ * We do invalidations before launching uncacheable IOs to disk. But in case
+ * of out of ordering the invalidations before launching the IOs does not help.
+ * We need to invalidate after the IO completes.
+ * 
+ * Doing invalidations after the completion of an uncacheable IO will cause 
+ * any overlapping dirty blocks in the cache to be written out and the IO 
+ * relaunched. If the overlapping blocks are busy, the IO is relaunched to 
+ * disk also (post invalidation). In these 2 cases, we will end up sending
+ * 2 disk IOs for the block. But this is a rare case.
+ * 
+ * When 2 IOs for the same block are sent down (by un co-operating processes)
+ * the storage stack is allowed to re-order the IOs at will. So the applications
+ * cannot expect any ordering at all.
+ * 
+ * What we try to avoid here is inconsistencies between disk and the ssd cache.
+ */
+void 
+flashcache_uncached_io_complete(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	int queued;
+	int error = job->error;
+
+	if (unlikely(error)) {
+		DMERR("flashcache uncached disk IO error: io error %d block %lu R/w %s", 
+		      error, job->job_io_regions.disk.sector, 
+		      (bio_data_dir(job->bio) == WRITE) ? "WRITE" : "READ");
+		if (bio_data_dir(job->bio) == WRITE)
+			dmc->flashcache_errors.disk_write_errors++;
+		else
+			dmc->flashcache_errors.disk_read_errors++;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	queued = flashcache_inval_blocks(dmc, job->bio);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(job->bio, -EIO, dmc, NULL);
+		/* 
+		 * The IO will be re-executed.
+		 * The do_pending logic will re-launch the 
+		 * disk IO post-invalidation calling start_uncached_io.
+		 * This should be a rare occurrence.
+		 */
+		dmc->flashcache_stats.uncached_io_requeue++;
+	} else {
+		flashcache_bio_endio(job->bio, error, dmc, &job->io_start_time);
+	}
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void 
+flashcache_uncached_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+
+	VERIFY(job->index == -1);
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_uncached_io_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static void
+flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio)
+{
+	int is_write = (bio_data_dir(bio) == WRITE);
+	struct kcached_job *job;
+	
+	if (is_write) {
+		dmc->flashcache_stats.uncached_writes++;
+		dmc->flashcache_stats.disk_writes++;
+	} else {
+		dmc->flashcache_stats.uncached_reads++;
+		dmc->flashcache_stats.disk_reads++;
+	}
+	job = new_kcached_job(dmc, bio, -1);
+	if (unlikely(job == NULL)) {
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	atomic_inc(&dmc->nr_jobs);
+	dm_io_async_bvec(1, &job->job_io_regions.disk,
+			 ((is_write) ? WRITE : READ), 
+			 bio->bi_io_vec + bio->bi_idx,
+			 flashcache_uncached_io_callback, job);
+}
+
+EXPORT_SYMBOL(flashcache_io_callback);
+EXPORT_SYMBOL(flashcache_do_pending_error);
+EXPORT_SYMBOL(flashcache_do_pending_noerror);
+EXPORT_SYMBOL(flashcache_do_pending);
+EXPORT_SYMBOL(flashcache_do_io);
+EXPORT_SYMBOL(flashcache_map);
+EXPORT_SYMBOL(flashcache_write);
+EXPORT_SYMBOL(flashcache_inval_blocks);
+EXPORT_SYMBOL(flashcache_inval_block_set);
+EXPORT_SYMBOL(flashcache_read);
+EXPORT_SYMBOL(flashcache_read_miss);
+EXPORT_SYMBOL(flashcache_clean_set);
+EXPORT_SYMBOL(flashcache_dirty_writeback);
+EXPORT_SYMBOL(flashcache_kcopyd_callback);
+EXPORT_SYMBOL(flashcache_lookup);
+EXPORT_SYMBOL(flashcache_alloc_md_sector);
+EXPORT_SYMBOL(flashcache_free_md_sector);
+EXPORT_SYMBOL(flashcache_md_write_callback);
+EXPORT_SYMBOL(flashcache_md_write_kickoff);
+EXPORT_SYMBOL(flashcache_md_write_done);
+EXPORT_SYMBOL(flashcache_md_write);
+
+
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_procfs.c linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_procfs.c
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_procfs.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_procfs.c	2016-12-13 21:55:12.013000000 +0800
@@ -0,0 +1,1100 @@
+/****************************************************************************
+ *  flashcache_conf.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+static int fallow_clean_speed_min = FALLOW_SPEED_MIN;
+static int fallow_clean_speed_max = FALLOW_SPEED_MAX;
+
+extern u_int64_t size_hist[];
+
+static char *flashcache_cons_procfs_cachename(struct cache_c *dmc, char *path_component);
+static char *flashcache_cons_sysctl_devname(struct cache_c *dmc);
+
+#define FLASHCACHE_PROC_ROOTDIR_NAME	"flashcache"
+
+static int
+flashcache_io_latency_init(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			   struct file *file,
+#endif
+			   void __user *buffer,
+			   size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_io_latency_hist) {
+			int i;
+				
+			for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+				dmc->latency_hist[i] = 0;
+			dmc->latency_hist_10ms = 0;
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_sync_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+		       struct file *file, 
+#endif
+		       void __user *buffer, 
+		       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_do_sync) {
+			dmc->sysctl_stop_sync = 0;
+			cancel_delayed_work(&dmc->delayed_clean);
+			flush_scheduled_work();
+			flashcache_sync_all(dmc);
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_zerostats_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			    struct file *file, 
+#endif
+			    void __user *buffer, 
+			    size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_zerostats) {
+			int i;
+
+			memset(&dmc->flashcache_stats, 0, sizeof(struct flashcache_stats));
+			for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+				dmc->latency_hist[i] = 0;
+			dmc->latency_hist_10ms = 0;
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_fallow_clean_speed_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+				     struct file *file, 
+#endif
+				     void __user *buffer, 
+				     size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_fallow_clean_speed < fallow_clean_speed_min)
+			dmc->sysctl_fallow_clean_speed = fallow_clean_speed_min;
+
+		if (dmc->sysctl_fallow_clean_speed > fallow_clean_speed_max)
+			dmc->sysctl_fallow_clean_speed = fallow_clean_speed_max;
+	}
+	return 0;
+}
+
+static int
+flashcache_dirty_thresh_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			       struct file *file, 
+#endif
+			       void __user *buffer, 
+			       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+        proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+        proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_dirty_thresh > DIRTY_THRESH_MAX)
+			dmc->sysctl_dirty_thresh = DIRTY_THRESH_MAX;
+
+		if (dmc->sysctl_dirty_thresh < DIRTY_THRESH_MIN)
+			dmc->sysctl_dirty_thresh = DIRTY_THRESH_MIN;
+
+		dmc->dirty_thresh_set = 
+			(dmc->assoc * dmc->sysctl_dirty_thresh) / 100;
+	}
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+#define CTL_UNNUMBERED			-2
+#endif
+
+/*
+ * Each ctl_table array needs to be 1 more than the actual number of
+ * entries - zero padded at the end ! Therefore the NUM_*_SYSCTLS
+ * is 1 more than then number of sysctls.
+ */
+#define FLASHCACHE_NUM_WRITEBACK_SYSCTLS	17
+
+static struct flashcache_writeback_sysctl_table {
+	struct ctl_table_header *sysctl_header;
+	ctl_table		vars[FLASHCACHE_NUM_WRITEBACK_SYSCTLS];
+	ctl_table		dev[2];
+	ctl_table		dir[2];
+	ctl_table		root[2];
+} flashcache_writeback_sysctl = {
+	.vars = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "io_latency_hist",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_io_latency_init,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_sync",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_sync_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "stop_sync",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "dirty_thresh_pct",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_dirty_thresh_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_clean_ios_total",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_clean_ios_set",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_pid_expiry",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_pids",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "pid_expiry_secs",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "reclaim_policy",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "zero_stats",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_zerostats_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+#ifdef notdef
+		/* 
+		 * Disable this for all except devel builds 
+		 * If you enable this, you must bump FLASHCACHE_NUM_WRITEBACK_SYSCTLS
+		 * by 1 !
+		 */
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "error_inject",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+#endif
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fast_remove",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "cache_all",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fallow_clean_speed",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_fallow_clean_speed_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fallow_delay",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "skip_seq_thresh_kb",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+	},
+	.dev = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "flashcache-dev",
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writeback_sysctl.vars,
+		},
+	},
+	.dir = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= FLASHCACHE_PROC_ROOTDIR_NAME,
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writeback_sysctl.dev,
+		},
+	},
+	.root = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_DEV,
+#endif
+			.procname	= "dev",
+			.maxlen		= 0,
+			.mode		= 0555,
+			.child		= flashcache_writeback_sysctl.dir,
+		},
+	},
+};
+
+/*
+ * Each ctl_table array needs to be 1 more than the actual number of
+ * entries - zero padded at the end ! Therefore the NUM_*_SYSCTLS
+ * is 1 more than then number of sysctls.
+ */
+#define FLASHCACHE_NUM_WRITETHROUGH_SYSCTLS	9
+
+static struct flashcache_writethrough_sysctl_table {
+	struct ctl_table_header *sysctl_header;
+	ctl_table		vars[FLASHCACHE_NUM_WRITETHROUGH_SYSCTLS];
+	ctl_table		dev[2];
+	ctl_table		dir[2];
+	ctl_table		root[2];
+} flashcache_writethrough_sysctl = {
+	.vars = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "io_latency_hist",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_io_latency_init,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_pid_expiry",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_pids",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "pid_expiry_secs",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "reclaim_policy",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "zero_stats",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_zerostats_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+#ifdef notdef
+		/* 
+		 * Disable this for all except devel builds 
+		 * If you enable this, you must bump FLASHCACHE_NUM_WRITEBACK_SYSCTLS
+		 * by 1 !
+		 */
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "error_inject",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+#endif
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "cache_all",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "skip_seq_thresh_kb",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+	},
+	.dev = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "flashcache-dev",
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writethrough_sysctl.vars,
+		},
+	},
+	.dir = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= FLASHCACHE_PROC_ROOTDIR_NAME,
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writethrough_sysctl.dev,
+		},
+	},
+	.root = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_DEV,
+#endif
+			.procname	= "dev",
+			.maxlen		= 0,
+			.mode		= 0555,
+			.child		= flashcache_writethrough_sysctl.dir,
+		},
+	},
+};
+
+int *
+flashcache_find_sysctl_data(struct cache_c *dmc, ctl_table *vars)
+{
+	if (strcmp(vars->procname, "io_latency_hist") == 0)
+		return &dmc->sysctl_io_latency_hist;
+	else if (strcmp(vars->procname, "do_sync") == 0) 
+		return &dmc->sysctl_do_sync;
+	else if (strcmp(vars->procname, "stop_sync") == 0) 
+		return &dmc->sysctl_stop_sync;
+	else if (strcmp(vars->procname, "dirty_thresh_pct") == 0) 
+		return &dmc->sysctl_dirty_thresh;
+	else if (strcmp(vars->procname, "max_clean_ios_total") == 0) 
+		return &dmc->max_clean_ios_total;
+	else if (strcmp(vars->procname, "max_clean_ios_set") == 0) 
+		return &dmc->max_clean_ios_set;
+	else if (strcmp(vars->procname, "do_pid_expiry") == 0) 
+		return &dmc->sysctl_pid_do_expiry;
+	else if (strcmp(vars->procname, "max_pids") == 0) 
+		return &dmc->sysctl_max_pids;
+	else if (strcmp(vars->procname, "pid_expiry_secs") == 0) 
+		return &dmc->sysctl_pid_expiry_secs;
+	else if (strcmp(vars->procname, "reclaim_policy") == 0) 
+		return &dmc->sysctl_reclaim_policy;
+	else if (strcmp(vars->procname, "zero_stats") == 0) 
+		return &dmc->sysctl_zerostats;
+	else if (strcmp(vars->procname, "error_inject") == 0) 
+		return &dmc->sysctl_error_inject;
+	else if (strcmp(vars->procname, "fast_remove") == 0) 
+		return &dmc->sysctl_fast_remove;
+	else if (strcmp(vars->procname, "cache_all") == 0) 
+		return &dmc->sysctl_cache_all;
+	else if (strcmp(vars->procname, "fallow_clean_speed") == 0) 
+		return &dmc->sysctl_fallow_clean_speed;
+	else if (strcmp(vars->procname, "fallow_delay") == 0) 
+		return &dmc->sysctl_fallow_delay;
+	else if (strcmp(vars->procname, "skip_seq_thresh_kb") == 0) 
+		return &dmc->sysctl_skip_seq_thresh_kb;
+	VERIFY(0);
+	return NULL;
+}
+
+static void
+flashcache_writeback_sysctl_register(struct cache_c *dmc)
+{
+	int i;
+	struct flashcache_writeback_sysctl_table *t;
+	
+	t = kmemdup(&flashcache_writeback_sysctl, sizeof(*t), GFP_KERNEL);
+	if (t == NULL)
+		return;
+	for (i = 0 ; i < ARRAY_SIZE(t->vars) - 1 ; i++) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+		t->vars[i].de = NULL;
+#endif
+		t->vars[i].data = flashcache_find_sysctl_data(dmc, &t->vars[i]);
+		t->vars[i].extra1 = dmc;
+	}
+	
+	t->dev[0].procname = flashcache_cons_sysctl_devname(dmc);
+	t->dev[0].child = t->vars;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dev[0].de = NULL;
+#endif
+	t->dir[0].child = t->dev;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dir[0].de = NULL;
+#endif
+	t->root[0].child = t->dir;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->root[0].de = NULL;
+#endif
+	
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->sysctl_header = register_sysctl_table(t->root, 0);
+#else
+	t->sysctl_header = register_sysctl_table(t->root);
+#endif
+	if (t->sysctl_header == NULL)
+		goto out;
+	
+	dmc->sysctl_handle = t;
+	return;
+
+out:
+	kfree(t->dev[0].procname);
+	kfree(t);
+}
+
+static void
+flashcache_writeback_sysctl_unregister(struct cache_c *dmc)
+{
+	struct flashcache_writeback_sysctl_table *t;
+
+	t = dmc->sysctl_handle;
+	if (t != NULL) {
+		dmc->sysctl_handle = NULL;
+		unregister_sysctl_table(t->sysctl_header);
+		kfree(t->dev[0].procname);
+		kfree(t);		
+	}
+}
+
+static void
+flashcache_writethrough_sysctl_register(struct cache_c *dmc)
+{
+	int i;
+	struct flashcache_writethrough_sysctl_table *t;
+	
+	t = kmemdup(&flashcache_writethrough_sysctl, sizeof(*t), GFP_KERNEL);
+	if (t == NULL)
+		return;
+	for (i = 0 ; i < ARRAY_SIZE(t->vars) - 1 ; i++) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+		t->vars[i].de = NULL;
+#endif
+		t->vars[i].data = flashcache_find_sysctl_data(dmc, &t->vars[i]);
+		t->vars[i].extra1 = dmc;
+	}
+	
+	t->dev[0].procname = flashcache_cons_sysctl_devname(dmc);
+	t->dev[0].child = t->vars;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dev[0].de = NULL;
+#endif
+	t->dir[0].child = t->dev;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dir[0].de = NULL;
+#endif
+	t->root[0].child = t->dir;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->root[0].de = NULL;
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->sysctl_header = register_sysctl_table(t->root, 0);
+#else
+	t->sysctl_header = register_sysctl_table(t->root);
+#endif
+	if (t->sysctl_header == NULL)
+		goto out;
+	
+	dmc->sysctl_handle = t;
+	return;
+
+out:
+	kfree(t->dev[0].procname);
+	kfree(t);
+}
+
+static void
+flashcache_writethrough_sysctl_unregister(struct cache_c *dmc)
+{
+	struct flashcache_writethrough_sysctl_table *t;
+
+	t = dmc->sysctl_handle;
+	if (t != NULL) {
+		dmc->sysctl_handle = NULL;
+		unregister_sysctl_table(t->sysctl_header);
+		kfree(t->dev[0].procname);
+		kfree(t);		
+	}
+}
+
+
+static int 
+flashcache_stats_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+	struct flashcache_stats *stats;
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+
+	stats = &dmc->flashcache_stats;
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	seq_printf(seq, "reads=%lu writes=%lu \n", 
+		   stats->reads, stats->writes);
+	seq_printf(seq, "read_hits=%lu read_hit_percent=%d ", 
+		   stats->read_hits, read_hit_pct);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK || dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		seq_printf(seq, "write_hits=%lu write_hit_percent=%d ", 
+		   	   stats->write_hits, write_hit_pct);
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		seq_printf(seq, "dirty_write_hits=%lu dirty_write_hit_percent=%d ",
+			   stats->dirty_write_hits, dirty_write_hit_pct);
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK || dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		seq_printf(seq, "replacement=%lu write_replacement=%lu ",
+			   stats->replace, stats->wr_replace);
+		seq_printf(seq,  "write_invalidates=%lu read_invalidates=%lu ",
+			   stats->wr_invalidates, stats->rd_invalidates);
+	} else {	/* WRITE_AROUND */
+		seq_printf(seq, "replacement=%lu ",
+			   stats->replace);
+		seq_printf(seq, "read_invalidates=%lu ",
+			   stats->rd_invalidates);
+	}
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	seq_printf(seq,  "checksum_store=%ld checksum_valid=%ld checksum_invalid=%ld ",
+		stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+	seq_printf(seq,  "pending_enqueues=%lu pending_inval=%lu ",
+		   stats->enqueues, stats->pending_inval);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) { 
+		seq_printf(seq, "metadata_dirties=%lu metadata_cleans=%lu ",
+			   stats->md_write_dirty, stats->md_write_clean);
+		seq_printf(seq, "metadata_batch=%lu metadata_ssd_writes=%lu ",
+			   stats->md_write_batch, stats->md_ssd_writes);
+		seq_printf(seq, "cleanings=%lu fallow_cleanings=%lu ",
+			   stats->cleanings, stats->fallow_cleanings);
+	}
+	seq_printf(seq, "no_room=%lu ",
+		   stats->noroom);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+ 		seq_printf(seq, "front_merge=%lu back_merge=%lu ",
+			   stats->front_merge, stats->back_merge);
+	}
+	seq_printf(seq,  "disk_reads=%lu disk_writes=%lu ssd_reads=%lu ssd_writes=%lu ",
+		   stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes);
+	seq_printf(seq,  "uncached_reads=%lu uncached_writes=%lu uncached_IO_requeue=%lu ",
+		   stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue);
+	seq_printf(seq,  "uncached_sequential_reads=%lu uncached_sequential_writes=%lu ",
+		   stats->uncached_sequential_reads, stats->uncached_sequential_writes);
+	seq_printf(seq, "pid_adds=%lu pid_dels=%lu pid_drops=%lu pid_expiry=%lu\n",
+		   stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	return 0;
+}
+
+static int 
+flashcache_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_stats_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_stats_operations = {
+	.open		= flashcache_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_errors_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+
+	seq_printf(seq, "disk_read_errors=%d disk_write_errors=%d ",
+		   dmc->flashcache_errors.disk_read_errors, 
+		   dmc->flashcache_errors.disk_write_errors);
+	seq_printf(seq, "ssd_read_errors=%d ssd_write_errors=%d ",
+		   dmc->flashcache_errors.ssd_read_errors, 
+		   dmc->flashcache_errors.ssd_write_errors);
+	seq_printf(seq, "memory_alloc_errors=%d\n", 
+		   dmc->flashcache_errors.memory_alloc_errors);
+	return 0;
+}
+
+static int 
+flashcache_errors_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_errors_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_errors_operations = {
+	.open		= flashcache_errors_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_iosize_hist_show(struct seq_file *seq, void *v)
+{
+	int i;
+	
+	for (i = 1 ; i <= 32 ; i++) {
+		seq_printf(seq, "%d:%llu ", i*512, size_hist[i]);
+	}
+	seq_printf(seq, "\n");
+	return 0;
+}
+
+static int 
+flashcache_iosize_hist_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_iosize_hist_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_iosize_hist_operations = {
+	.open		= flashcache_iosize_hist_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_pidlists_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+	struct flashcache_cachectl_pid *pid_list;
+ 	unsigned long flags;
+	
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	seq_printf(seq, "Blacklist: ");
+	pid_list = dmc->blacklist_head;
+	while (pid_list != NULL) {
+		seq_printf(seq, "%u ", pid_list->pid);
+		pid_list = pid_list->next;
+	}
+	seq_printf(seq, "\n");
+	seq_printf(seq, "Whitelist: ");
+	pid_list = dmc->whitelist_head;
+	while (pid_list != NULL) {
+		seq_printf(seq, "%u ", pid_list->pid);
+		pid_list = pid_list->next;
+	}
+	seq_printf(seq, "\n");
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	return 0;
+}
+
+static int 
+flashcache_pidlists_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_pidlists_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_pidlists_operations = {
+	.open		= flashcache_pidlists_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+extern char *flashcache_sw_version;
+
+static int 
+flashcache_version_show(struct seq_file *seq, void *v)
+{
+	seq_printf(seq, "Flashcache Version : %s\n", flashcache_sw_version);
+#ifdef COMMIT_REV
+	seq_printf(seq, "git commit: %s\n", COMMIT_REV);
+#endif
+	return 0;
+}
+
+static int 
+flashcache_version_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_version_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_version_operations = {
+	.open		= flashcache_version_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+void
+flashcache_module_procfs_init(void)
+{
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *entry;
+
+	if (proc_mkdir("flashcache", NULL)) {
+		entry = create_proc_entry("flashcache/flashcache_version", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_version_operations;
+	}
+#endif /* CONFIG_PROC_FS */
+}
+
+void
+flashcache_module_procfs_releae(void)
+{
+#ifdef CONFIG_PROC_FS
+	(void)remove_proc_entry("flashcache/flashcache_version", NULL);
+	(void)remove_proc_entry("flashcache", NULL);
+#endif /* CONFIG_PROC_FS */
+}
+
+static char *
+flashcache_cons_sysctl_devname(struct cache_c *dmc)
+{
+	char *pathname;
+	
+	pathname = kzalloc(strlen(dmc->cache_devname) + strlen(dmc->disk_devname) + 2,
+			   GFP_KERNEL);
+	strcpy(pathname, strrchr(dmc->cache_devname, '/') + 1);
+	strcat(pathname, "+");
+	strcat(pathname, strrchr(dmc->disk_devname, '/') + 1);
+	return pathname;
+}
+
+static char *
+flashcache_cons_procfs_cachename(struct cache_c *dmc, char *path_component)
+{
+	char *pathname;
+	char *s;
+	
+	pathname = kzalloc(strlen(dmc->cache_devname) + strlen(dmc->disk_devname) + 4 + 
+			   strlen(FLASHCACHE_PROC_ROOTDIR_NAME) + 
+			   strlen(path_component), 
+			   GFP_KERNEL);
+	strcpy(pathname, FLASHCACHE_PROC_ROOTDIR_NAME);
+	strcat(pathname, "/");
+	s = strrchr(dmc->cache_devname, '/');
+	if (s) 
+		s++;
+	else
+		s = dmc->cache_devname;
+	strcat(pathname, s);
+	strcat(pathname, "+");
+	s = strrchr(dmc->disk_devname, '/');
+	if (s) 
+		s++;
+	else
+		s = dmc->disk_devname;
+	strcat(pathname, s);
+	if (strcmp(path_component, "") != 0) {
+		strcat(pathname, "/");
+		strcat(pathname, path_component);
+	}
+	return pathname;
+}
+
+void 
+flashcache_ctr_procfs(struct cache_c *dmc)
+{
+	char *s;
+	struct proc_dir_entry *entry;
+
+	s =  flashcache_cons_procfs_cachename(dmc, "");
+	entry = proc_mkdir(s, NULL);
+	kfree(s);
+	if (entry == NULL)
+		return;
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_stats");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_stats_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_errors");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_errors_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_iosize_hist");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_iosize_hist_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_pidlists");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_pidlists_operations;
+		entry->data = dmc;			
+	}
+	kfree(s);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		flashcache_writeback_sysctl_register(dmc);
+	else
+		flashcache_writethrough_sysctl_register(dmc);
+}
+
+void 
+flashcache_dtr_procfs(struct cache_c *dmc)
+{
+	char *s;
+	
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_stats");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_errors");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_iosize_hist");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_pidlists");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		flashcache_writeback_sysctl_unregister(dmc);
+	else
+		flashcache_writethrough_sysctl_unregister(dmc);
+
+}
+
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_subr.c linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_subr.c
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/flashcache_subr.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/flashcache_subr.c	2016-12-13 21:55:12.014000000 +0800
@@ -0,0 +1,785 @@
+/****************************************************************************
+ *  flashcache_subr.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/sort.h>
+#include <linux/time.h>
+#include <asm/kmap_types.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+
+static DEFINE_SPINLOCK(_job_lock);
+
+extern mempool_t *_job_pool;
+extern mempool_t *_pending_job_pool;
+
+extern atomic_t nr_cache_jobs;
+extern atomic_t nr_pending_jobs;
+
+LIST_HEAD(_pending_jobs);
+LIST_HEAD(_io_jobs);
+LIST_HEAD(_md_io_jobs);
+LIST_HEAD(_md_complete_jobs);
+LIST_HEAD(_uncached_io_complete_jobs);
+
+int
+flashcache_pending_empty(void)
+{
+	return list_empty(&_pending_jobs);
+}
+
+int
+flashcache_io_empty(void)
+{
+	return list_empty(&_io_jobs);
+}
+
+int
+flashcache_md_io_empty(void)
+{
+	return list_empty(&_md_io_jobs);
+}
+
+int
+flashcache_md_complete_empty(void)
+{
+	return list_empty(&_md_complete_jobs);
+}
+
+int
+flashcache_uncached_io_complete_empty(void)
+{
+	return list_empty(&_uncached_io_complete_jobs);
+}
+
+struct kcached_job *
+flashcache_alloc_cache_job(void)
+{
+	struct kcached_job *job;
+
+	job = mempool_alloc(_job_pool, GFP_NOIO);
+	if (likely(job))
+		atomic_inc(&nr_cache_jobs);
+	return job;
+}
+
+void
+flashcache_free_cache_job(struct kcached_job *job)
+{
+	mempool_free(job, _job_pool);
+	atomic_dec(&nr_cache_jobs);
+}
+
+struct pending_job *
+flashcache_alloc_pending_job(struct cache_c *dmc)
+{
+	struct pending_job *job;
+
+	job = mempool_alloc(_pending_job_pool, GFP_ATOMIC);
+	if (likely(job))
+		atomic_inc(&nr_pending_jobs);
+	else
+		dmc->flashcache_errors.memory_alloc_errors++;
+	return job;
+}
+
+void
+flashcache_free_pending_job(struct pending_job *job)
+{
+	mempool_free(job, _pending_job_pool);
+	atomic_dec(&nr_pending_jobs);
+}
+
+#define FLASHCACHE_PENDING_JOB_HASH(INDEX)		((INDEX) % PENDING_JOB_HASH_SIZE)
+
+void 
+flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+		       int index, int action, struct pending_job *job)
+{
+	struct pending_job **head;
+	
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	DPRINTK("flashcache_enq_pending: Queue to pending Q Index %d %llu",
+		index, bio->bi_sector);
+	VERIFY(job != NULL);
+	job->action = action;
+	job->index = index;
+	job->bio = bio;
+	job->prev = NULL;
+	job->next = *head;
+	if (*head)
+		(*head)->prev = job;
+	*head = job;
+	dmc->cache[index].nr_queued++;
+	dmc->flashcache_stats.enqueues++;
+	dmc->pending_jobs_count++;
+}
+
+/*
+ * Deq and move all pending jobs that match the index for this slot to list returned
+ */
+struct pending_job *
+flashcache_deq_pending(struct cache_c *dmc, int index)
+{
+	struct pending_job *node, *next, *movelist = NULL;
+	int moved = 0;
+	struct pending_job **head;
+	
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	for (node = *head ; node != NULL ; node = next) {
+		next = node->next;
+		if (node->index == index) {
+			/* 
+			 * Remove pending job from the global list of 
+			 * jobs and move it to the private list for freeing 
+			 */
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			node->prev = NULL;
+			node->next = movelist;
+			movelist = node;
+			moved++;
+		}
+	}
+	VERIFY(dmc->pending_jobs_count >= moved);
+	dmc->pending_jobs_count -= moved;
+	return movelist;
+}
+
+#ifdef FLASHCACHE_DO_CHECKSUMS
+int
+flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int error;
+	u_int64_t sum = 0, *idx;
+	int cnt;
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+	where.count = dmc->block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, block);
+	if (error)
+		return error;
+	cnt = dmc->block_size * 512;
+	idx = (u_int64_t *)block;
+	while (cnt > 0) {
+		sum += *idx++;
+		cnt -= sizeof(u_int64_t);		
+	}
+	dmc->cache[index].checksum = sum;
+	return 0;
+}
+
+u_int64_t
+flashcache_compute_checksum(struct bio *bio)
+{
+	int i;	
+	u_int64_t sum = 0, *idx;
+	int cnt;
+	int kmap_type;
+	void *kvaddr;
+
+	if (in_interrupt())
+		kmap_type = KM_SOFTIRQ0;
+	else
+		kmap_type = KM_USER0;
+	for (i = bio->bi_idx ; i < bio->bi_vcnt ; i++) {
+		kvaddr = kmap_atomic(bio->bi_io_vec[i].bv_page, kmap_type);
+		idx = (u_int64_t *)
+			((char *)kvaddr + bio->bi_io_vec[i].bv_offset);
+		cnt = bio->bi_io_vec[i].bv_len;
+		while (cnt > 0) {
+			sum += *idx++;
+			cnt -= sizeof(u_int64_t);
+		}
+		kunmap_atomic(kvaddr, kmap_type);
+	}
+	return sum;
+}
+
+void
+flashcache_store_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	unsigned long flags;
+	
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	job->dmc->cache[job->index].checksum = sum;
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+}
+
+int
+flashcache_validate_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	int retval;
+	unsigned long flags;
+	
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	if (likely(job->dmc->cache[job->index].checksum == sum)) {
+		job->dmc->flashcache_stats.checksum_valid++;		
+		retval = 0;
+	} else {
+		job->dmc->flashcache_stats.checksum_invalid++;
+		retval = 1;
+	}
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+	return retval;
+}
+#endif
+
+/*
+ * Functions to push and pop a job onto the head of a given job list.
+ */
+struct kcached_job *
+pop(struct list_head *jobs)
+{
+	struct kcached_job *job = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	if (!list_empty(jobs)) {
+		job = list_entry(jobs->next, struct kcached_job, list);
+		list_del(&job->list);
+	}
+	spin_unlock_irqrestore(&_job_lock, flags);
+	return job;
+}
+
+void 
+push(struct list_head *jobs, struct kcached_job *job)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	list_add_tail(&job->list, jobs);
+	spin_unlock_irqrestore(&_job_lock, flags);
+}
+
+void
+push_pending(struct kcached_job *job)
+{
+	push(&_pending_jobs, job);	
+}
+
+void
+push_io(struct kcached_job *job)
+{
+	push(&_io_jobs, job);	
+}
+
+void
+push_uncached_io_complete(struct kcached_job *job)
+{
+	push(&_uncached_io_complete_jobs, job);	
+}
+
+void
+push_md_io(struct kcached_job *job)
+{
+	push(&_md_io_jobs, job);	
+}
+
+void
+push_md_complete(struct kcached_job *job)
+{
+	push(&_md_complete_jobs, job);	
+}
+
+static void
+process_jobs(struct list_head *jobs,
+	     void (*fn) (struct kcached_job *))
+{
+	struct kcached_job *job;
+
+	while ((job = pop(jobs)))
+		(void)fn(job);
+}
+
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+do_work(void *unused)
+#else
+do_work(struct work_struct *unused)
+#endif
+{
+	process_jobs(&_md_complete_jobs, flashcache_md_write_done);
+	process_jobs(&_pending_jobs, flashcache_do_pending);
+	process_jobs(&_md_io_jobs, flashcache_md_write_kickoff);
+	process_jobs(&_io_jobs, flashcache_do_io);
+	process_jobs(&_uncached_io_complete_jobs, flashcache_uncached_io_complete);
+}
+
+struct kcached_job *
+new_kcached_job(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct kcached_job *job;
+
+	job = flashcache_alloc_cache_job();
+	if (unlikely(job == NULL)) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return NULL;
+	}
+	job->dmc = dmc;
+	job->index = index;
+	job->job_io_regions.cache.bdev = dmc->cache_dev->bdev;
+	if (index != -1) {
+		job->job_io_regions.cache.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+		job->job_io_regions.cache.count = dmc->block_size;	
+	}
+	job->error = 0;	
+	job->bio = bio;
+	job->job_io_regions.disk.bdev = dmc->disk_dev->bdev;
+	if (index != -1) {
+		job->job_io_regions.disk.sector = dmc->cache[index].dbn;
+		job->job_io_regions.disk.count = dmc->block_size;
+	} else {
+		job->job_io_regions.disk.sector = bio->bi_sector;
+		job->job_io_regions.disk.count = to_sector(bio->bi_size);
+	}
+	job->next = NULL;
+	job->md_block = NULL;
+	if (dmc->sysctl_io_latency_hist)
+		do_gettimeofday(&job->io_start_time);
+	else {
+		job->io_start_time.tv_sec = 0;
+		job->io_start_time.tv_usec = 0;
+	}
+	return job;
+}
+
+static void
+flashcache_record_latency(struct cache_c *dmc, struct timeval *start_tv)
+{
+	struct timeval latency;
+	int64_t us;
+	
+	do_gettimeofday(&latency);
+	latency.tv_sec -= start_tv->tv_sec;
+	latency.tv_usec -= start_tv->tv_usec;	
+	us = latency.tv_sec * USEC_PER_SEC + latency.tv_usec;
+	us /= IO_LATENCY_GRAN_USECS;	/* histogram 250us gran, scale 10ms total */
+	if (us < IO_LATENCY_BUCKETS)
+		/* < 10ms latency, track it */
+		dmc->latency_hist[us]++;
+	else
+		/* else count it in 10ms+ bucket */
+		dmc->latency_hist_10ms++;
+}
+
+void
+flashcache_bio_endio(struct bio *bio, int error, 
+		     struct cache_c *dmc, struct timeval *start_time)
+{
+	if (unlikely(dmc->sysctl_io_latency_hist && 
+		     start_time != NULL && 
+		     start_time->tv_sec != 0))
+		flashcache_record_latency(dmc, start_time);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24)
+	bio_endio(bio, bio->bi_size, error);
+#else
+	bio_endio(bio, error);
+#endif	
+}
+
+void
+flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index)
+{
+	int set = index / dmc->assoc;
+	int start_index = set * dmc->assoc;
+	int my_index = index - start_index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	/* Remove from LRU */
+	if (likely((cacheblk->lru_prev != FLASHCACHE_LRU_NULL) ||
+		   (cacheblk->lru_next != FLASHCACHE_LRU_NULL))) {
+		if (cacheblk->lru_prev != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_prev + start_index].lru_next = 
+				cacheblk->lru_next;
+		else
+			dmc->cache_sets[set].lru_head = cacheblk->lru_next;
+		if (cacheblk->lru_next != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_next + start_index].lru_prev = 
+				cacheblk->lru_prev;
+		else
+			dmc->cache_sets[set].lru_tail = cacheblk->lru_prev;
+	}
+	/* And add it to LRU Tail */
+	cacheblk->lru_next = FLASHCACHE_LRU_NULL;
+	cacheblk->lru_prev = dmc->cache_sets[set].lru_tail;
+	if (dmc->cache_sets[set].lru_tail == FLASHCACHE_LRU_NULL)
+		dmc->cache_sets[set].lru_head = my_index;
+	else
+		dmc->cache[dmc->cache_sets[set].lru_tail + start_index].lru_next = 
+			my_index;
+	dmc->cache_sets[set].lru_tail = my_index;
+}
+
+static int 
+cmp_dbn(const void *a, const void *b)
+{
+	if (((struct dbn_index_pair *)a)->dbn < ((struct dbn_index_pair *)b)->dbn)
+		return -1;
+	else
+		return 1;
+}
+
+static void
+swap_dbn_index_pair(void *a, void *b, int size)
+{
+	struct dbn_index_pair temp;
+	
+	temp = *(struct dbn_index_pair *)a;
+	*(struct dbn_index_pair *)a = *(struct dbn_index_pair *)b;
+	*(struct dbn_index_pair *)b = temp;
+}
+
+/* 
+ * We have a list of blocks to write out to disk.
+ * 1) Sort the blocks by dbn.
+ * 2) (sysctl'able) See if there are any other blocks in the same set
+ * that are contig to any of the blocks in step 1. If so, include them
+ * in our "to write" set, maintaining sorted order.
+ * Has to be called under the cache spinlock !
+ */
+void
+flashcache_merge_writes(struct cache_c *dmc, struct dbn_index_pair *writes_list, 
+			int *nr_writes, int set)
+{	
+	int start_index = set * dmc->assoc;
+	int end_index = start_index + dmc->assoc;
+	int old_writes = *nr_writes;
+	int new_inserts = 0;
+	struct dbn_index_pair *set_dirty_list = NULL;
+	int ix, nr_set_dirty;
+	struct cacheblock *cacheblk;
+			
+	if (unlikely(*nr_writes == 0))
+		return;
+	sort(writes_list, *nr_writes, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+
+	set_dirty_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_ATOMIC);
+	if (set_dirty_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		goto out;
+	}
+	nr_set_dirty = 0;
+	for (ix = start_index ; ix < end_index ; ix++) {
+		cacheblk = &dmc->cache[ix];
+		/*
+		 * Any DIRTY block in "writes_list" will be marked as 
+		 * DISKWRITEINPROG already, so we'll skip over those here.
+		 */
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			set_dirty_list[nr_set_dirty].dbn = cacheblk->dbn;
+			set_dirty_list[nr_set_dirty].index = ix;
+			nr_set_dirty++;
+		}
+	}
+	if (nr_set_dirty == 0)
+		goto out;
+	sort(set_dirty_list, nr_set_dirty, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+	for (ix = 0 ; ix < nr_set_dirty ; ix++) {
+		int back_merge, k;
+		int i;
+
+		cacheblk = &dmc->cache[set_dirty_list[ix].index];
+		back_merge = -1;
+		VERIFY((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY);
+		for (i = 0 ; i < *nr_writes ; i++) {
+			int insert;
+			int j = 0;
+				
+			insert = 0;
+			if (cacheblk->dbn + dmc->block_size == writes_list[i].dbn) {
+				/* cacheblk to be inserted above i */
+				insert = 1;
+				j = i;
+				back_merge = j;
+			}
+			if (cacheblk->dbn - dmc->block_size == writes_list[i].dbn ) {
+				/* cacheblk to be inserted after i */
+				insert = 1;
+				j = i + 1;
+			}
+			VERIFY(j < dmc->assoc);
+			if (insert) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[ix].index);
+				/* 
+				 * Shift down everthing from j to ((*nr_writes) - 1) to
+				 * make room for the new entry. And add the new entry.
+				 */
+				for (k = (*nr_writes) - 1 ; k >= j ; k--)
+					writes_list[k + 1] = writes_list[k];
+				writes_list[j].dbn = cacheblk->dbn;
+				writes_list[j].index = cacheblk - &dmc->cache[0];
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				if (back_merge == -1)
+					dmc->flashcache_stats.front_merge++;
+				else
+					dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				break;
+			}
+		}
+		/*
+		 * If we did a back merge, we need to walk back in the set's dirty list
+		 * to see if we can pick off any more contig blocks. Forward merges don't
+		 * need this special treatment since we are walking the 2 lists in that 
+		 * direction. It would be nice to roll this logic into the above.
+		 */
+		if (back_merge != -1) {
+			for (k = ix - 1 ; k >= 0 ; k--) {
+				int n;
+
+				if (set_dirty_list[k].dbn + dmc->block_size != 
+				    writes_list[back_merge].dbn)
+					break;
+				dmc->cache[set_dirty_list[k].index].cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[k].index);
+				for (n = (*nr_writes) - 1 ; n >= back_merge ; n--)
+					writes_list[n + 1] = writes_list[n];
+				writes_list[back_merge].dbn = set_dirty_list[k].dbn;
+				writes_list[back_merge].index = set_dirty_list[k].index;
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);				
+			}
+		}
+	}
+	VERIFY((*nr_writes) == (old_writes + new_inserts));
+out:
+	if (set_dirty_list)
+		kfree(set_dirty_list);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+extern struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int 
+flashcache_dm_io_async_vm(struct cache_c *dmc, unsigned int num_regions, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			  struct io_region *where, 
+#else
+			  struct dm_io_region *where, 
+#endif
+			  int rw,
+			  void *data, io_notify_fn fn, void *context)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = fn,
+		.notify.context = context,
+		.client = flashcache_io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,29)
+/*
+ * Wrappers for doing DM sync IO, using DM async IO.
+ * It is a shame we need do this, but DM sync IO is interruptible :(
+ * And we want uninterruptible disk IO :)
+ * 
+ * This is fixed in 2.6.30, where sync DM IO is uninterruptible.
+ */
+#define FLASHCACHE_DM_IO_SYNC_INPROG	0x01
+
+static DECLARE_WAIT_QUEUE_HEAD(flashcache_dm_io_sync_waitqueue);
+static DEFINE_SPINLOCK(flashcache_dm_io_sync_spinlock);
+
+struct flashcache_dm_io_sync_state {
+	int			error;
+	int			flags;
+};
+
+static void
+flashcache_dm_io_sync_vm_callback(unsigned long error, void *context)
+{
+	struct flashcache_dm_io_sync_state *state = 
+		(struct flashcache_dm_io_sync_state *)context;
+	unsigned long flags;
+	
+	spin_lock_irqsave(&flashcache_dm_io_sync_spinlock, flags);
+	state->flags &= ~FLASHCACHE_DM_IO_SYNC_INPROG;
+	state->error = error;
+	wake_up(&flashcache_dm_io_sync_waitqueue);
+	spin_unlock_irqrestore(&flashcache_dm_io_sync_spinlock, flags);
+}
+
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			 struct io_region *where, 
+#else
+			  struct dm_io_region *where, 
+#endif
+			 int rw, void *data)
+{
+        DEFINE_WAIT(wait);
+	struct flashcache_dm_io_sync_state state;
+
+	state.error = -EINTR;
+	state.flags = FLASHCACHE_DM_IO_SYNC_INPROG;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	dm_io_async_vm(1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#else
+	flashcache_dm_io_async_vm(dmc, 1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#endif
+	spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	while (state.flags & FLASHCACHE_DM_IO_SYNC_INPROG) {
+		prepare_to_wait(&flashcache_dm_io_sync_waitqueue, &wait, 
+				TASK_UNINTERRUPTIBLE);
+		spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+		schedule();
+		spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	}
+	finish_wait(&flashcache_dm_io_sync_waitqueue, &wait);
+	spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+	return state.error;
+}
+#else
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where, int rw, void *data)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = NULL,
+		.client = flashcache_io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+void
+flashcache_update_sync_progress(struct cache_c *dmc)
+{
+	u_int64_t dirty_pct;
+	
+	if (dmc->flashcache_stats.cleanings % 1000)
+		return;
+	if (!dmc->nr_dirty || !dmc->size || !printk_ratelimit())
+		return;
+	dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+	printk(KERN_INFO "Flashcache: Cleaning %d Dirty blocks, Dirty Blocks pct %llu%%", 
+	       dmc->nr_dirty, dirty_pct);
+	printk(KERN_INFO "\r");
+}
+
+EXPORT_SYMBOL(flashcache_alloc_cache_job);
+EXPORT_SYMBOL(flashcache_free_cache_job);
+EXPORT_SYMBOL(flashcache_alloc_pending_job);
+EXPORT_SYMBOL(flashcache_free_pending_job);
+EXPORT_SYMBOL(pop);
+EXPORT_SYMBOL(push);
+EXPORT_SYMBOL(push_pending);
+EXPORT_SYMBOL(push_io);
+EXPORT_SYMBOL(push_md_io);
+EXPORT_SYMBOL(push_md_complete);
+EXPORT_SYMBOL(process_jobs);
+EXPORT_SYMBOL(do_work);
+EXPORT_SYMBOL(new_kcached_job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm_callback);
+#endif
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm);
+EXPORT_SYMBOL(flashcache_reclaim_lru_movetail);
+EXPORT_SYMBOL(flashcache_merge_writes);
+EXPORT_SYMBOL(flashcache_enq_pending);
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Kconfig linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Kconfig
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Kconfig	2016-12-13 21:54:57.930000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Kconfig	2016-12-13 21:55:12.015000000 +0800
@@ -369,6 +369,12 @@
 	  A target that discards writes, and returns all zeroes for
 	  reads.  Useful in some recovery situations.
 
+config DM_FLASHCACHE
+	tristate "Block level disk caching target (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	  A write back block caching target.
+
 config DM_MULTIPATH
 	tristate "Multipath target"
 	depends on BLK_DEV_DM
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Kconfig.orig linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Kconfig.orig
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Kconfig.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Kconfig.orig	2016-12-13 21:55:12.015000000 +0800
@@ -0,0 +1,457 @@
+#
+# Block device driver configuration
+#
+
+menuconfig MD
+	bool "Multiple devices driver support (RAID and LVM)"
+	depends on BLOCK
+	help
+	  Support multiple physical spindles through a single logical device.
+	  Required for RAID and logical volume management.
+
+if MD
+
+config BLK_DEV_MD
+	tristate "RAID support"
+	---help---
+	  This driver lets you combine several hard disk partitions into one
+	  logical block device. This can be used to simply append one
+	  partition to another one or to combine several redundant hard disks
+	  into a RAID1/4/5 device so as to provide protection against hard
+	  disk failures. This is called "Software RAID" since the combining of
+	  the partitions is done by the kernel. "Hardware RAID" means that the
+	  combining is done by a dedicated controller; if you have such a
+	  controller, you do not need to say Y here.
+
+	  More information about Software RAID on Linux is contained in the
+	  Software RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also learn
+	  where to get the supporting user space utilities raidtools.
+
+	  If unsure, say N.
+
+config MD_AUTODETECT
+	bool "Autodetect RAID arrays during kernel boot"
+	depends on BLK_DEV_MD=y
+	default y
+	---help---
+	  If you say Y here, then the kernel will try to autodetect raid
+	  arrays as part of its boot process. 
+
+	  If you don't use raid and say Y, this autodetection can cause 
+	  a several-second delay in the boot time due to various
+	  synchronisation steps that are part of this step.
+
+	  If unsure, say Y.
+
+config MD_LINEAR
+	tristate "Linear (append) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  If you say Y here, then your multiple devices driver will be able to
+	  use the so-called linear mode, i.e. it will combine the hard disk
+	  partitions by simply appending one to the other.
+
+	  To compile this as a module, choose M here: the module
+	  will be called linear.
+
+	  If unsure, say Y.
+
+config MD_RAID0
+	tristate "RAID-0 (striping) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  If you say Y here, then your multiple devices driver will be able to
+	  use the so-called raid0 mode, i.e. it will combine the hard disk
+	  partitions into one logical device in such a fashion as to fill them
+	  up evenly, one chunk here and one chunk there. This will increase
+	  the throughput rate if the partitions reside on distinct disks.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  To compile this as a module, choose M here: the module
+	  will be called raid0.
+
+	  If unsure, say Y.
+
+config MD_RAID1
+	tristate "RAID-1 (mirroring) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  A RAID-1 set consists of several disk drives which are exact copies
+	  of each other.  In the event of a mirror failure, the RAID driver
+	  will continue to use the operational mirrors in the set, providing
+	  an error free MD (multiple device) to the higher levels of the
+	  kernel.  In a set with N drives, the available space is the capacity
+	  of a single drive, and the set protects against a failure of (N - 1)
+	  drives.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>.  There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  If you want to use such a RAID-1 set, say Y.  To compile this code
+	  as a module, choose M here: the module will be called raid1.
+
+	  If unsure, say Y.
+
+config MD_RAID10
+	tristate "RAID-10 (mirrored striping) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  RAID-10 provides a combination of striping (RAID-0) and
+	  mirroring (RAID-1) with easier configuration and more flexible
+	  layout.
+	  Unlike RAID-0, but like RAID-1, RAID-10 requires all devices to
+	  be the same size (or at least, only as much as the smallest device
+	  will be used).
+	  RAID-10 provides a variety of layouts that provide different levels
+	  of redundancy and performance.
+
+	  RAID-10 requires mdadm-1.7.0 or later, available at:
+
+	  ftp://ftp.kernel.org/pub/linux/utils/raid/mdadm/
+
+	  If unsure, say Y.
+
+config MD_RAID456
+	tristate "RAID-4/RAID-5/RAID-6 mode"
+	depends on BLK_DEV_MD
+	select RAID6_PQ
+	select ASYNC_MEMCPY
+	select ASYNC_XOR
+	select ASYNC_PQ
+	select ASYNC_RAID6_RECOV
+	---help---
+	  A RAID-5 set of N drives with a capacity of C MB per drive provides
+	  the capacity of C * (N - 1) MB, and protects against a failure
+	  of a single drive. For a given sector (row) number, (N - 1) drives
+	  contain data sectors, and one drive contains the parity protection.
+	  For a RAID-4 set, the parity blocks are present on a single drive,
+	  while a RAID-5 set distributes the parity across the drives in one
+	  of the available parity distribution methods.
+
+	  A RAID-6 set of N drives with a capacity of C MB per drive
+	  provides the capacity of C * (N - 2) MB, and protects
+	  against a failure of any two drives. For a given sector
+	  (row) number, (N - 2) drives contain data sectors, and two
+	  drives contains two independent redundancy syndromes.  Like
+	  RAID-5, RAID-6 distributes the syndromes across the drives
+	  in one of the available parity distribution methods.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  If you want to use such a RAID-4/RAID-5/RAID-6 set, say Y.  To
+	  compile this code as a module, choose M here: the module
+	  will be called raid456.
+
+	  If unsure, say Y.
+
+config MD_MULTIPATH
+	tristate "Multipath I/O support"
+	depends on BLK_DEV_MD
+	help
+	  MD_MULTIPATH provides a simple multi-path personality for use
+	  the MD framework.  It is not under active development.  New
+	  projects should consider using DM_MULTIPATH which has more
+	  features and more testing.
+
+	  If unsure, say N.
+
+config MD_FAULTY
+	tristate "Faulty test module for MD"
+	depends on BLK_DEV_MD
+	help
+	  The "faulty" module allows for a block device that occasionally returns
+	  read or write errors.  It is useful for testing.
+
+	  In unsure, say N.
+
+source "drivers/md/bcache/Kconfig"
+
+config BLK_DEV_DM_BUILTIN
+	boolean
+
+config BLK_DEV_DM
+	tristate "Device mapper support"
+	select BLK_DEV_DM_BUILTIN
+	---help---
+	  Device-mapper is a low level volume manager.  It works by allowing
+	  people to specify mappings for ranges of logical sectors.  Various
+	  mapping types are available, in addition people may write their own
+	  modules containing custom mappings if they wish.
+
+	  Higher level volume managers such as LVM2 use this driver.
+
+	  To compile this as a module, choose M here: the module will be
+	  called dm-mod.
+
+	  If unsure, say N.
+
+config DM_MQ_DEFAULT
+	bool "request-based DM: use blk-mq I/O path by default"
+	depends on BLK_DEV_DM
+	---help---
+	  This option enables the blk-mq based I/O path for request-based
+	  DM devices by default.  With the option the dm_mod.use_blk_mq
+	  module/boot option defaults to Y, without it to N, but it can
+	  still be overriden either way.
+
+	  If unsure say N.
+
+config DM_DEBUG
+	boolean "Device mapper debugging support"
+	depends on BLK_DEV_DM
+	---help---
+	  Enable this for messages that may help debug device-mapper problems.
+
+	  If unsure, say N.
+
+config DM_BUFIO
+       tristate
+       depends on BLK_DEV_DM
+       ---help---
+	 This interface allows you to do buffered I/O on a device and acts
+	 as a cache, holding recently-read blocks in memory and performing
+	 delayed writes.
+
+config DM_BIO_PRISON
+       tristate
+       depends on BLK_DEV_DM
+       ---help---
+	 Some bio locking schemes used by other device-mapper targets
+	 including thin provisioning.
+
+source "drivers/md/persistent-data/Kconfig"
+
+config DM_CRYPT
+	tristate "Crypt target support"
+	depends on BLK_DEV_DM
+	select CRYPTO
+	select CRYPTO_CBC
+	---help---
+	  This device-mapper target allows you to create a device that
+	  transparently encrypts the data on it. You'll need to activate
+	  the ciphers you're going to use in the cryptoapi configuration.
+
+	  For further information on dm-crypt and userspace tools see:
+	  <http://code.google.com/p/cryptsetup/wiki/DMCrypt>
+
+	  To compile this code as a module, choose M here: the module will
+	  be called dm-crypt.
+
+	  If unsure, say N.
+
+config DM_SNAPSHOT
+       tristate "Snapshot target"
+       depends on BLK_DEV_DM
+       select DM_BUFIO
+       ---help---
+         Allow volume managers to take writable snapshots of a device.
+
+config DM_THIN_PROVISIONING
+       tristate "Thin provisioning target"
+       depends on BLK_DEV_DM
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         Provides thin provisioning and snapshots that share a data store.
+
+config DM_CACHE
+       tristate "Cache target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         dm-cache attempts to improve performance of a block device by
+         moving frequently used data to a smaller, higher performance
+         device.  Different 'policy' plugins can be used to change the
+         algorithms used to select which blocks are promoted, demoted,
+         cleaned etc.  It supports writeback and writethrough modes.
+
+config DM_CACHE_MQ
+       tristate "MQ Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A cache policy that uses a multiqueue ordered by recent hit
+         count to select which blocks should be promoted and demoted.
+         This is meant to be a general purpose policy.  It prioritises
+         reads over writes.
+
+config DM_CACHE_SMQ
+       tristate "Stochastic MQ Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A cache policy that uses a multiqueue ordered by recent hits
+         to select which blocks should be promoted and demoted.
+         This is meant to be a general purpose policy.  It prioritises
+         reads over writes.  This SMQ policy (vs MQ) offers the promise
+         of less memory utilization, improved performance and increased
+         adaptability in the face of changing workloads.
+
+config DM_CACHE_CLEANER
+       tristate "Cleaner Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A simple cache policy that writes back all data to the
+         origin.  Used when decommissioning a dm-cache.
+
+config DM_ERA
+       tristate "Era target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         dm-era tracks which parts of a block device are written to
+         over time.  Useful for maintaining cache coherency when using
+         vendor snapshots.
+
+config DM_MIRROR
+       tristate "Mirror target"
+       depends on BLK_DEV_DM
+       ---help---
+         Allow volume managers to mirror logical volumes, also
+         needed for live data migration tools such as 'pvmove'.
+
+config DM_LOG_USERSPACE
+	tristate "Mirror userspace logging"
+	depends on DM_MIRROR && NET
+	select CONNECTOR
+	---help---
+	  The userspace logging module provides a mechanism for
+	  relaying the dm-dirty-log API to userspace.  Log designs
+	  which are more suited to userspace implementation (e.g.
+	  shared storage logs) or experimental logs can be implemented
+	  by leveraging this framework.
+
+config DM_RAID
+       tristate "RAID 1/4/5/6/10 target"
+       depends on BLK_DEV_DM
+       select MD_RAID1
+       select MD_RAID10
+       select MD_RAID456
+       select BLK_DEV_MD
+       ---help---
+	 A dm target that supports RAID1, RAID10, RAID4, RAID5 and RAID6 mappings
+
+	 A RAID-5 set of N drives with a capacity of C MB per drive provides
+	 the capacity of C * (N - 1) MB, and protects against a failure
+	 of a single drive. For a given sector (row) number, (N - 1) drives
+	 contain data sectors, and one drive contains the parity protection.
+	 For a RAID-4 set, the parity blocks are present on a single drive,
+	 while a RAID-5 set distributes the parity across the drives in one
+	 of the available parity distribution methods.
+
+	 A RAID-6 set of N drives with a capacity of C MB per drive
+	 provides the capacity of C * (N - 2) MB, and protects
+	 against a failure of any two drives. For a given sector
+	 (row) number, (N - 2) drives contain data sectors, and two
+	 drives contains two independent redundancy syndromes.  Like
+	 RAID-5, RAID-6 distributes the syndromes across the drives
+	 in one of the available parity distribution methods.
+
+config DM_ZERO
+	tristate "Zero target"
+	depends on BLK_DEV_DM
+	---help---
+	  A target that discards writes, and returns all zeroes for
+	  reads.  Useful in some recovery situations.
+
+config DM_MULTIPATH
+	tristate "Multipath target"
+	depends on BLK_DEV_DM
+	# nasty syntax but means make DM_MULTIPATH independent
+	# of SCSI_DH if the latter isn't defined but if
+	# it is, DM_MULTIPATH must depend on it.  We get a build
+	# error if SCSI_DH=m and DM_MULTIPATH=y
+	depends on SCSI_DH || !SCSI_DH
+	---help---
+	  Allow volume managers to support multipath hardware.
+
+config DM_MULTIPATH_QL
+	tristate "I/O Path Selector based on the number of in-flight I/Os"
+	depends on DM_MULTIPATH
+	---help---
+	  This path selector is a dynamic load balancer which selects
+	  the path with the least number of in-flight I/Os.
+
+	  If unsure, say N.
+
+config DM_MULTIPATH_ST
+	tristate "I/O Path Selector based on the service time"
+	depends on DM_MULTIPATH
+	---help---
+	  This path selector is a dynamic load balancer which selects
+	  the path expected to complete the incoming I/O in the shortest
+	  time.
+
+	  If unsure, say N.
+
+config DM_DELAY
+	tristate "I/O delaying target"
+	depends on BLK_DEV_DM
+	---help---
+	A target that delays reads and/or writes and can send
+	them to different devices.  Useful for testing.
+
+	If unsure, say N.
+
+config DM_UEVENT
+	bool "DM uevents"
+	depends on BLK_DEV_DM
+	---help---
+	Generate udev events for DM events.
+
+config DM_FLAKEY
+       tristate "Flakey target"
+       depends on BLK_DEV_DM
+       ---help---
+         A target that intermittently fails I/O for debugging purposes.
+
+config DM_VERITY
+	tristate "Verity target support"
+	depends on BLK_DEV_DM
+	select CRYPTO
+	select CRYPTO_HASH
+	select DM_BUFIO
+	---help---
+	  This device-mapper target creates a read-only device that
+	  transparently validates the data on one underlying device against
+	  a pre-generated tree of cryptographic checksums stored on a second
+	  device.
+
+	  You'll need to activate the digests you're going to use in the
+	  cryptoapi configuration.
+
+	  To compile this code as a module, choose M here: the module will
+	  be called dm-verity.
+
+	  If unsure, say N.
+
+config DM_SWITCH
+	tristate "Switch target support (EXPERIMENTAL)"
+	depends on BLK_DEV_DM
+	---help---
+	  This device-mapper target creates a device that supports an arbitrary
+	  mapping of fixed-size regions of I/O across a fixed set of paths.
+	  The path used for any specific region can be switched dynamically
+	  by sending the target a message.
+
+	  To compile this code as a module, choose M here: the module will
+	  be called dm-switch.
+
+	  If unsure, say N.
+
+endif # MD
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Makefile linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Makefile
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Makefile	2016-12-13 21:54:57.930000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Makefile	2016-12-13 21:55:12.016000000 +0800
@@ -10,6 +10,8 @@
 dm-mirror-y	+= dm-raid1.o
 dm-log-userspace-y \
 		+= dm-log-userspace-base.o dm-log-userspace-transfer.o
+flashcache-y   +=  flashcache_conf.o flashcache_main.o flashcache_subr.o \
+                   flashcache_ioctl.o flashcache_procfs.o
 dm-thin-pool-y	+= dm-thin.o dm-thin-metadata.o
 dm-cache-y	+= dm-cache-target.o dm-cache-metadata.o dm-cache-policy.o
 dm-cache-mq-y   += dm-cache-policy-mq.o
@@ -49,6 +51,7 @@
 obj-$(CONFIG_DM_MIRROR)		+= dm-mirror.o dm-log.o dm-region-hash.o
 obj-$(CONFIG_DM_LOG_USERSPACE)	+= dm-log-userspace.o
 obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
+obj-$(CONFIG_DM_FLASHCACHE)    += flashcache.o
 obj-$(CONFIG_DM_RAID)	+= dm-raid.o
 obj-$(CONFIG_DM_THIN_PROVISIONING)	+= dm-thin-pool.o
 obj-$(CONFIG_DM_VERITY)		+= dm-verity.o
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Makefile.orig linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Makefile.orig
--- linux-3.10.0-327.36.3.el7.x86_64/drivers/md/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/drivers/md/Makefile.orig	2016-12-13 21:55:12.016000000 +0800
@@ -0,0 +1,63 @@
+#
+# Makefile for the kernel software RAID and LVM drivers.
+#
+
+dm-mod-y	+= dm.o dm-table.o dm-target.o dm-linear.o dm-stripe.o \
+		   dm-ioctl.o dm-io.o dm-kcopyd.o dm-sysfs.o dm-stats.o
+dm-multipath-y	+= dm-path-selector.o dm-mpath.o
+dm-snapshot-y	+= dm-snap.o dm-exception-store.o dm-snap-transient.o \
+		    dm-snap-persistent.o
+dm-mirror-y	+= dm-raid1.o
+dm-log-userspace-y \
+		+= dm-log-userspace-base.o dm-log-userspace-transfer.o
+dm-thin-pool-y	+= dm-thin.o dm-thin-metadata.o
+dm-cache-y	+= dm-cache-target.o dm-cache-metadata.o dm-cache-policy.o
+dm-cache-mq-y   += dm-cache-policy-mq.o
+dm-cache-smq-y   += dm-cache-policy-smq.o
+dm-cache-cleaner-y += dm-cache-policy-cleaner.o
+dm-era-y	+= dm-era-target.o
+md-mod-y	+= md.o bitmap.o
+raid456-y	+= raid5.o
+
+# Note: link order is important.  All raid personalities
+# and must come before md.o, as they each initialise 
+# themselves, and md.o may use the personalities when it 
+# auto-initialised.
+
+obj-$(CONFIG_MD_LINEAR)		+= linear.o
+obj-$(CONFIG_MD_RAID0)		+= raid0.o
+obj-$(CONFIG_MD_RAID1)		+= raid1.o
+obj-$(CONFIG_MD_RAID10)		+= raid10.o
+obj-$(CONFIG_MD_RAID456)	+= raid456.o
+obj-$(CONFIG_MD_MULTIPATH)	+= multipath.o
+obj-$(CONFIG_MD_FAULTY)		+= faulty.o
+obj-$(CONFIG_BCACHE)		+= bcache/
+obj-$(CONFIG_BLK_DEV_MD)	+= md-mod.o
+obj-$(CONFIG_BLK_DEV_DM)	+= dm-mod.o
+obj-$(CONFIG_BLK_DEV_DM_BUILTIN) += dm-builtin.o
+obj-$(CONFIG_DM_BUFIO)		+= dm-bufio.o
+obj-$(CONFIG_DM_BIO_PRISON)	+= dm-bio-prison.o
+obj-$(CONFIG_DM_CRYPT)		+= dm-crypt.o
+obj-$(CONFIG_DM_DELAY)		+= dm-delay.o
+obj-$(CONFIG_DM_FLAKEY)		+= dm-flakey.o
+obj-$(CONFIG_DM_MULTIPATH)	+= dm-multipath.o dm-round-robin.o
+obj-$(CONFIG_DM_MULTIPATH_QL)	+= dm-queue-length.o
+obj-$(CONFIG_DM_MULTIPATH_ST)	+= dm-service-time.o
+obj-$(CONFIG_DM_SWITCH)		+= dm-switch.o
+obj-$(CONFIG_DM_SNAPSHOT)	+= dm-snapshot.o
+obj-$(CONFIG_DM_PERSISTENT_DATA)	+= persistent-data/
+obj-$(CONFIG_DM_MIRROR)		+= dm-mirror.o dm-log.o dm-region-hash.o
+obj-$(CONFIG_DM_LOG_USERSPACE)	+= dm-log-userspace.o
+obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
+obj-$(CONFIG_DM_RAID)	+= dm-raid.o
+obj-$(CONFIG_DM_THIN_PROVISIONING)	+= dm-thin-pool.o
+obj-$(CONFIG_DM_VERITY)		+= dm-verity.o
+obj-$(CONFIG_DM_CACHE)		+= dm-cache.o
+obj-$(CONFIG_DM_CACHE_MQ)	+= dm-cache-mq.o
+obj-$(CONFIG_DM_CACHE_SMQ)	+= dm-cache-smq.o
+obj-$(CONFIG_DM_CACHE_CLEANER)	+= dm-cache-cleaner.o
+obj-$(CONFIG_DM_ERA)		+= dm-era.o
+
+ifeq ($(CONFIG_DM_UEVENT),y)
+dm-mod-objs			+= dm-uevent.o
+endif
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/fs/proc/proc_net.c linux-3.10.0-327.36.3.el7.toa.x86_64/fs/proc/proc_net.c
--- linux-3.10.0-327.36.3.el7.x86_64/fs/proc/proc_net.c	2016-12-13 21:54:58.437000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/fs/proc/proc_net.c	2016-12-13 21:55:12.017000000 +0800
@@ -183,6 +183,19 @@
 	.readdir	= proc_tgid_net_readdir,
 };
 
+struct proc_dir_entry *proc_net_fops_create(struct net *net,
+	const char *name, mode_t mode, const struct file_operations *fops)
+{
+	return proc_create(name, mode, net->proc_net, fops);
+}
+EXPORT_SYMBOL_GPL(proc_net_fops_create);
+
+void proc_net_remove(struct net *net, const char *name)
+{
+	remove_proc_entry(name, net->proc_net);
+}
+EXPORT_SYMBOL_GPL(proc_net_remove);
+
 static __net_init int proc_net_ns_init(struct net *net)
 {
 	struct proc_dir_entry *netd, *net_statd;
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/linux/hookers.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/linux/hookers.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/linux/hookers.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/linux/hookers.h	2016-12-13 21:55:12.017000000 +0800
@@ -0,0 +1,152 @@
+/*
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ *
+ *	Changes:
+ *		Li Yu :		Starting up.
+ */
+
+#ifndef _LINUX_HOOKER_H_
+#define _LINUX_HOOKER_H_
+
+#include <linux/types.h>
+
+struct hooked_place;
+
+/*
+ * This API allows us replace and restore the function pointer in any order.
+ *
+ * This is designed to satisfy hooker stack usage pattern. e.g.
+ *
+ *	In our TCP implemention, icsk_af_ops->syn_recv_sock is called
+ *	when thea three way handshake has completed, we need to hook it
+ *      sometimes, e.g. to compute some statistics data on fly, even to
+ *      add a private TCP option.
+ *
+ *  By hooking this function, we can attain the goal without any kernel
+ *  change or just some small changes, and hope that this can help to
+ *  reduce the cost of maintaining custom kernel release too. Of course,
+ *  this can't replace that design necessary extendible framework, but I
+ *  think that hooking is a good and cheep choice of starting all.
+ *
+ *	Assume that we have two hooks, we expect that the hooking could
+ *      produce below behavior:
+ *
+ *	First, install two hookers:
+ *
+ *          install(&syn_recv_sock, hook1)
+ *          install(&syn_recv_sock, hook2)
+ *
+ *	Now, we expect the invoking order is:
+ *
+ *	     orig_syn_recv_sock() , hook2() , hook1()
+ *
+ *	Then, remove a hooker:
+ *
+ *          uninstall(&syn_recv_sock, hook1)
+ *
+ *      Then, the invoking order should be:
+ *
+ *	   orig_syn_recv_sock(), hook2()
+ *
+ *	Last, remove all rest hookers:
+ *
+ *          uninstall(&syn_recv_sock, hook2)
+ *
+ *      The result just is:
+ *
+ *	    orig_syn_recv_sock()
+ *
+ *      See, it is function pointer stack here. however, if we just simplely
+ *	used address of hooker1 in "top" hooker function (hooker2),
+ *	we will get an invalid memory access exception when prior hookers
+ *      (hooker1) is uninstalled first. Under second simple design, we just
+ *      support the some fixed predefined hooking addresses, and manage hookers
+ *      by a simple linked list.
+ *
+ *
+ * Usage:
+ *
+ *	1. Install a hooker on address which you are interesting in.
+ *	   Assume that the kernel has a callback table as below:
+ *
+ *		struct icsk_ops {
+ *			...
+			 *int (*foo)(int a, char b);
+ *			...
+ *		};
+ *
+ *		struct icsk_ops icsk_ops = {
+ *			...
+ *			.foo = real_foo,
+ *			...
+ *		};
+ *
+ *	   Then we should hook &icsk_ops.foo by such way:
+ *
+ *		static int foo_hooker(int a, char b, int *p_ret)
+ *		{
+ *			int ret = *p_ret;
+ *
+ *			//do something that may overwrite return value.
+ *			//p_ret saves the result value of original function
+ *			//or other hookers.
+ *
+ *			//You should not have any assume for invoking order
+ *			//of hookers.
+ *
+ *			return ret;
+ *		}
+ *
+ *		struct hooker h = {
+ *			.func = foo_hooker,
+ *		};
+ *
+ *		hooker_install(&icsk_ops.foo ,&h);
+ *
+ *		The hooker and original function has same function signature, if
+ *		the original function has not return value, IOW, it's like
+ *
+ *			void foo(int a, char b) { ... }
+ *
+ *	2. Uninstall hooker is easy, just:
+ *
+ *		hooker_uninstall(&h);
+ *
+ */
+
+struct hooker {
+	struct hooked_place *hplace;
+	void *func;	/* the installed hooker function pointer */
+	struct list_head chain;
+};
+
+/*
+ * Install the hooker function at specified address.
+ * This function may sleep.
+ *
+ * Parameters:
+ *	place - the address that saves function pointer
+ *	hooker - the hooker to install, the caller must fill
+ *		 its func member first
+ *
+ * Return:
+ *	    0  - All OK, please note that hooker func may be called before
+ *		 this return
+ *	  < 0 -  any error, e.g. out of memory, existing same installed hooker
+ */
+extern int hooker_install(void *place, struct hooker *hooker);
+
+/*
+ * Remove the installed hooker function that saved in hooker->func.
+ * This function may sleep.
+ *
+ * Parameters:
+ *	place - the address that saves function pointer
+ *	hooker - the installed hooker struct
+ */
+extern void hooker_uninstall(struct hooker *hooker);
+
+#endif
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/linux/proc_fs.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/linux/proc_fs.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/linux/proc_fs.h	2016-12-13 21:54:58.374000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/linux/proc_fs.h	2016-12-13 21:56:51.992000000 +0800
@@ -72,6 +72,10 @@
 
 struct net;
 
+extern struct proc_dir_entry *proc_net_fops_create(struct net *net,
+	const char *name, mode_t mode, const struct file_operations *fops);
+extern void proc_net_remove(struct net *net, const char *name);
+
 static inline struct proc_dir_entry *proc_net_mkdir(
 	struct net *net, const char *name, struct proc_dir_entry *parent)
 {
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/net/inet_common.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/inet_common.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/net/inet_common.h	2016-12-13 21:54:58.285000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/inet_common.h	2016-12-13 21:55:12.017000000 +0800
@@ -1,7 +1,7 @@
 #ifndef _INET_COMMON_H
 #define _INET_COMMON_H
 
-extern const struct proto_ops inet_stream_ops;
+extern struct proto_ops inet_stream_ops;
 extern const struct proto_ops inet_dgram_ops;
 
 /*
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/net/ipv6.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/ipv6.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/net/ipv6.h	2016-12-13 21:54:58.292000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/ipv6.h	2016-12-13 21:55:12.018000000 +0800
@@ -835,7 +835,7 @@
 /*
  * reassembly.c
  */
-extern const struct proto_ops inet6_stream_ops;
+extern struct proto_ops inet6_stream_ops;
 extern const struct proto_ops inet6_dgram_ops;
 
 struct group_source_req;
@@ -880,6 +880,12 @@
 void ipv6_sysctl_unregister(void);
 #endif
 
+/* public func in tcp_ipv6.c */
+extern struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+					struct request_sock *req,
+					struct dst_entry *dst);
+extern struct inet_connection_sock_af_ops ipv6_specific;
+
 int ipv6_sock_mc_join(struct sock *sk, int ifindex,
 		      const struct in6_addr *addr);
 int ipv6_sock_mc_drop(struct sock *sk, int ifindex,
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/net/sock.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/sock.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/net/sock.h	2016-12-13 21:54:58.285000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/sock.h	2016-12-13 21:55:12.019000000 +0800
@@ -421,6 +421,7 @@
 #endif
 	__u32			sk_mark;
 	u32			sk_classid;
+		__be32					sk_toa_data[8];
 	struct cg_proto		*sk_cgrp;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/net/sock.h.orig linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/sock.h.orig
--- linux-3.10.0-327.36.3.el7.x86_64/include/net/sock.h.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/sock.h.orig	2016-12-13 21:55:12.022000000 +0800
@@ -0,0 +1,2352 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the AF_INET socket handler.
+ *
+ * Version:	@(#)sock.h	1.0.4	05/13/93
+ *
+ * Authors:	Ross Biro
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *		Corey Minyard <wf-rch!minyard@relay.EU.net>
+ *		Florian La Roche <flla@stud.uni-sb.de>
+ *
+ * Fixes:
+ *		Alan Cox	:	Volatiles in skbuff pointers. See
+ *					skbuff comments. May be overdone,
+ *					better to prove they can be removed
+ *					than the reverse.
+ *		Alan Cox	:	Added a zapped field for tcp to note
+ *					a socket is reset and must stay shut up
+ *		Alan Cox	:	New fields for options
+ *	Pauline Middelink	:	identd support
+ *		Alan Cox	:	Eliminate low level recv/recvfrom
+ *		David S. Miller	:	New socket lookup architecture.
+ *              Steve Whitehouse:       Default routines for sock_ops
+ *              Arnaldo C. Melo :	removed net_pinfo, tp_pinfo and made
+ *              			protinfo be just a void pointer, as the
+ *              			protocol specific parts were moved to
+ *              			respective headers and ipv4/v6, etc now
+ *              			use private slabcaches for its socks
+ *              Pedro Hortas	:	New flags field for socket options
+ *
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _SOCK_H
+#define _SOCK_H
+
+#include <linux/hardirq.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/list_nulls.h>
+#include <linux/timer.h>
+#include <linux/cache.h>
+#include <linux/bitops.h>
+#include <linux/lockdep.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>	/* struct sk_buff */
+#include <linux/mm.h>
+#include <linux/security.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/memcontrol.h>
+#include <linux/res_counter.h>
+#include <linux/static_key.h>
+#include <linux/aio.h>
+#include <linux/sched.h>
+
+#include <linux/filter.h>
+#include <linux/rculist_nulls.h>
+#include <linux/poll.h>
+
+#include <linux/atomic.h>
+#include <net/dst.h>
+#include <net/checksum.h>
+
+#include <linux/rh_kabi.h>
+#include <net/tcp_states.h>
+
+struct cgroup;
+struct cgroup_subsys;
+#ifdef CONFIG_NET
+int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss);
+void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg);
+#else
+static inline
+int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss)
+{
+	return 0;
+}
+static inline
+void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg)
+{
+}
+#endif
+/*
+ * This structure really needs to be cleaned up.
+ * Most of it is for TCP, and not used by any of
+ * the other protocols.
+ */
+
+/* Define this to get the SOCK_DBG debugging facility. */
+#define SOCK_DEBUGGING
+#ifdef SOCK_DEBUGGING
+#define SOCK_DEBUG(sk, msg...) do { if ((sk) && sock_flag((sk), SOCK_DBG)) \
+					printk(KERN_DEBUG msg); } while (0)
+#else
+/* Validate arguments and do nothing */
+static inline __printf(2, 3)
+void SOCK_DEBUG(const struct sock *sk, const char *msg, ...)
+{
+}
+#endif
+
+/* This is the per-socket lock.  The spinlock provides a synchronization
+ * between user contexts and software interrupt processing, whereas the
+ * mini-semaphore synchronizes multiple users amongst themselves.
+ */
+typedef struct {
+	spinlock_t		slock;
+	int			owned;
+	wait_queue_head_t	wq;
+	/*
+	 * We express the mutex-alike socket_lock semantics
+	 * to the lock validator by explicitly managing
+	 * the slock as a lock variant (in addition to
+	 * the slock itself):
+	 */
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} socket_lock_t;
+
+struct sock;
+struct proto;
+struct net;
+
+typedef __u32 __bitwise __portpair;
+typedef __u64 __bitwise __addrpair;
+
+/**
+ *	struct sock_common - minimal network layer representation of sockets
+ *	@skc_daddr: Foreign IPv4 addr
+ *	@skc_rcv_saddr: Bound local IPv4 addr
+ *	@skc_hash: hash value used with various protocol lookup tables
+ *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
+ *	@skc_dport: placeholder for inet_dport/tw_dport
+ *	@skc_num: placeholder for inet_num/tw_num
+ *	@skc_family: network address family
+ *	@skc_state: Connection state
+ *	@skc_reuse: %SO_REUSEADDR setting
+ *	@skc_reuseport: %SO_REUSEPORT setting
+ *	@skc_bound_dev_if: bound device index if != 0
+ *	@skc_bind_node: bind hash linkage for various protocol lookup tables
+ *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
+ *	@skc_prot: protocol handlers inside a network family
+ *	@skc_net: reference to the network namespace of this socket
+ *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
+ *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_refcnt: reference count
+ *
+ *	This is the minimal network layer representation of sockets, the header
+ *	for struct sock and struct inet_timewait_sock.
+ */
+struct sock_common {
+	/* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned
+	 * address on 64bit arches : cf INET_MATCH()
+	 */
+	union {
+		__addrpair	skc_addrpair;
+		struct {
+			__be32	skc_daddr;
+			__be32	skc_rcv_saddr;
+		};
+	};
+	union  {
+		unsigned int	skc_hash;
+		__u16		skc_u16hashes[2];
+	};
+	/* skc_dport && skc_num must be grouped as well */
+	union {
+		__portpair	skc_portpair;
+		struct {
+			__be16	skc_dport;
+			__u16	skc_num;
+		};
+	};
+
+	unsigned short		skc_family;
+	volatile unsigned char	skc_state;
+	unsigned char		skc_reuse:4;
+	unsigned char		skc_reuseport:4;
+	int			skc_bound_dev_if;
+	union {
+		struct hlist_node	skc_bind_node;
+		struct hlist_nulls_node skc_portaddr_node;
+	};
+	struct proto		*skc_prot;
+	possible_net_t		skc_net;
+
+#if IS_ENABLED(CONFIG_IPV6)
+	struct in6_addr		skc_v6_daddr;
+	struct in6_addr		skc_v6_rcv_saddr;
+#endif
+
+	/*
+	 * fields between dontcopy_begin/dontcopy_end
+	 * are not copied in sock_copy()
+	 */
+	/* private: */
+	int			skc_dontcopy_begin[0];
+	/* public: */
+	union {
+		struct hlist_node	skc_node;
+		struct hlist_nulls_node skc_nulls_node;
+	};
+	int			skc_tx_queue_mapping;
+	atomic_t		skc_refcnt;
+	/* private: */
+	int                     skc_dontcopy_end[0];
+	/* public: */
+};
+
+struct cg_proto;
+/**
+  *	struct sock - network layer representation of sockets
+  *	@__sk_common: shared layout with inet_timewait_sock
+  *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
+  *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
+  *	@sk_lock:	synchronizer
+  *	@sk_rcvbuf: size of receive buffer in bytes
+  *	@sk_wq: sock wait queue and async head
+  *	@sk_rx_dst: receive input route used by early tcp demux
+  *	@sk_dst_cache: destination cache
+  *	@sk_dst_lock: destination cache lock
+  *	@sk_policy: flow policy
+  *	@sk_receive_queue: incoming packets
+  *	@sk_wmem_alloc: transmit queue bytes committed
+  *	@sk_write_queue: Packet sending queue
+  *	@sk_omem_alloc: "o" is "option" or "other"
+  *	@sk_wmem_queued: persistent queue size
+  *	@sk_forward_alloc: space allocated forward
+  *	@sk_napi_id: id of the last napi context to receive data for sk
+  *	@sk_ll_usec: usecs to busypoll when there is no data
+  *	@sk_allocation: allocation mode
+  *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
+  *	@sk_sndbuf: size of send buffer in bytes
+  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
+  *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
+  *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
+  *	@sk_no_check_rx: allow zero checksum in RX packets
+  *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
+  *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
+  *	@sk_gso_max_size: Maximum GSO segment size to build
+  *	@sk_gso_max_segs: Maximum number of GSO segments
+  *	@sk_lingertime: %SO_LINGER l_linger setting
+  *	@sk_backlog: always used with the per-socket spinlock held
+  *	@sk_callback_lock: used with the callbacks in the end of this struct
+  *	@sk_error_queue: rarely used
+  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
+  *			  IPV6_ADDRFORM for instance)
+  *	@sk_err: last error
+  *	@sk_err_soft: errors that don't cause failure but are the cause of a
+  *		      persistent failure not just 'timed out'
+  *	@sk_drops: raw/udp drops counter
+  *	@sk_ack_backlog: current listen backlog
+  *	@sk_max_ack_backlog: listen backlog set in listen()
+  *	@sk_priority: %SO_PRIORITY setting
+  *	@sk_cgrp_prioidx: socket group's priority map index
+  *	@sk_type: socket type (%SOCK_STREAM, etc)
+  *	@sk_protocol: which protocol this socket belongs in this network family
+  *	@sk_peer_pid: &struct pid for this socket's peer
+  *	@sk_peer_cred: %SO_PEERCRED setting
+  *	@sk_rcvlowat: %SO_RCVLOWAT setting
+  *	@sk_rcvtimeo: %SO_RCVTIMEO setting
+  *	@sk_sndtimeo: %SO_SNDTIMEO setting
+  *	@sk_rxhash: flow hash received from netif layer
+  *	@sk_txhash: computed flow hash for use on transmit
+  *	@sk_filter: socket filtering instructions
+  *	@sk_protinfo: private area, net family specific, when not using slab
+  *	@sk_timer: sock cleanup timer
+  *	@sk_stamp: time stamp of last packet received
+  *	@sk_socket: Identd and reporting IO signals
+  *	@sk_user_data: RPC layer private data
+  *	@sk_frag: cached page frag
+  *	@sk_peek_off: current peek_offset value
+  *	@sk_send_head: front of stuff to transmit
+  *	@sk_security: used by security modules
+  *	@sk_mark: generic packet mark
+  *	@sk_classid: this socket's cgroup classid
+  *	@sk_cgrp: this socket's cgroup-specific proto data
+  *	@sk_write_pending: a write to stream socket waits to start
+  *	@sk_state_change: callback to indicate change in the state of the sock
+  *	@sk_data_ready: callback to indicate there is data to be processed
+  *	@sk_write_space: callback to indicate there is bf sending space available
+  *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
+  *	@sk_backlog_rcv: callback to process the backlog
+  *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
+ */
+struct sock {
+	/*
+	 * Now struct inet_timewait_sock also uses sock_common, so please just
+	 * don't add nothing before this first member (__sk_common) --acme
+	 */
+	struct sock_common	__sk_common;
+#define sk_node			__sk_common.skc_node
+#define sk_nulls_node		__sk_common.skc_nulls_node
+#define sk_refcnt		__sk_common.skc_refcnt
+#define sk_tx_queue_mapping	__sk_common.skc_tx_queue_mapping
+
+#define sk_dontcopy_begin	__sk_common.skc_dontcopy_begin
+#define sk_dontcopy_end		__sk_common.skc_dontcopy_end
+#define sk_hash			__sk_common.skc_hash
+#define sk_portpair		__sk_common.skc_portpair
+#define sk_num			__sk_common.skc_num
+#define sk_dport		__sk_common.skc_dport
+#define sk_addrpair		__sk_common.skc_addrpair
+#define sk_daddr		__sk_common.skc_daddr
+#define sk_rcv_saddr		__sk_common.skc_rcv_saddr
+#define sk_family		__sk_common.skc_family
+#define sk_state		__sk_common.skc_state
+#define sk_reuse		__sk_common.skc_reuse
+#define sk_reuseport		__sk_common.skc_reuseport
+#define sk_bound_dev_if		__sk_common.skc_bound_dev_if
+#define sk_bind_node		__sk_common.skc_bind_node
+#define sk_prot			__sk_common.skc_prot
+#define sk_net			__sk_common.skc_net
+#define sk_v6_daddr		__sk_common.skc_v6_daddr
+#define sk_v6_rcv_saddr	__sk_common.skc_v6_rcv_saddr
+
+	socket_lock_t		sk_lock;
+	struct sk_buff_head	sk_receive_queue;
+	/*
+	 * The backlog queue is special, it is always used with
+	 * the per-socket spinlock held and requires low latency
+	 * access. Therefore we special case it's implementation.
+	 * Note : rmem_alloc is in this structure to fill a hole
+	 * on 64bit arches, not because its logically part of
+	 * backlog.
+	 */
+	struct {
+		atomic_t	rmem_alloc;
+		int		len;
+		struct sk_buff	*head;
+		struct sk_buff	*tail;
+	} sk_backlog;
+#define sk_rmem_alloc sk_backlog.rmem_alloc
+	int			sk_forward_alloc;
+#ifdef CONFIG_RPS
+	__u32			sk_rxhash;
+#endif
+#ifdef CONFIG_NET_RX_BUSY_POLL
+	unsigned int		sk_napi_id;
+	unsigned int		sk_ll_usec;
+#endif
+	atomic_t		sk_drops;
+	int			sk_rcvbuf;
+
+	struct sk_filter __rcu	*sk_filter;
+	struct socket_wq __rcu	*sk_wq;
+
+#ifdef CONFIG_NET_DMA_RH_KABI
+	RH_KABI_DEPRECATE(struct sk_buff_head,	sk_async_wait_queue)
+#endif
+
+#ifdef CONFIG_XFRM
+	struct xfrm_policy	*sk_policy[2];
+#endif
+	unsigned long 		sk_flags;
+	struct dst_entry	*sk_rx_dst;
+	struct dst_entry __rcu	*sk_dst_cache;
+	spinlock_t		sk_dst_lock;
+	atomic_t		sk_wmem_alloc;
+	atomic_t		sk_omem_alloc;
+	int			sk_sndbuf;
+	struct sk_buff_head	sk_write_queue;
+	kmemcheck_bitfield_begin(flags);
+	unsigned int		sk_shutdown  : 2,
+#ifdef __GENKSYMS__
+				sk_no_check : 2,
+#else
+				sk_no_check_tx : 1,
+				sk_no_check_rx : 1,
+#endif
+				sk_userlocks : 4,
+				sk_protocol  : 8,
+				sk_type      : 16;
+	kmemcheck_bitfield_end(flags);
+	int			sk_wmem_queued;
+	gfp_t			sk_allocation;
+	u32			sk_pacing_rate; /* bytes per second */
+	netdev_features_t	sk_route_caps;
+	netdev_features_t	sk_route_nocaps;
+	int			sk_gso_type;
+	unsigned int		sk_gso_max_size;
+	u16			sk_gso_max_segs;
+	int			sk_rcvlowat;
+	unsigned long	        sk_lingertime;
+	struct sk_buff_head	sk_error_queue;
+	struct proto		*sk_prot_creator;
+	rwlock_t		sk_callback_lock;
+	int			sk_err,
+				sk_err_soft;
+	unsigned short		sk_ack_backlog;
+	unsigned short		sk_max_ack_backlog;
+	__u32			sk_priority;
+#if IS_ENABLED(CONFIG_NETPRIO_CGROUP)
+	__u32			sk_cgrp_prioidx;
+#endif
+	struct pid		*sk_peer_pid;
+	const struct cred	*sk_peer_cred;
+	long			sk_rcvtimeo;
+	long			sk_sndtimeo;
+	void			*sk_protinfo;
+	struct timer_list	sk_timer;
+	ktime_t			sk_stamp;
+	struct socket		*sk_socket;
+	void			*sk_user_data;
+	struct page_frag	sk_frag;
+	struct sk_buff		*sk_send_head;
+	__s32			sk_peek_off;
+	int			sk_write_pending;
+#ifdef CONFIG_SECURITY
+	void			*sk_security;
+#endif
+	__u32			sk_mark;
+	u32			sk_classid;
+	struct cg_proto		*sk_cgrp;
+	void			(*sk_state_change)(struct sock *sk);
+	void			(*sk_data_ready)(struct sock *sk, int bytes);
+	void			(*sk_write_space)(struct sock *sk);
+	void			(*sk_error_report)(struct sock *sk);
+	int			(*sk_backlog_rcv)(struct sock *sk,
+						  struct sk_buff *skb);
+	void                    (*sk_destruct)(struct sock *sk);
+
+	/* RHEL SPECIFIC
+	 *
+	 * The following padding has been inserted before ABI freeze to
+	 * allow extending the structure while preserve ABI. Feel free
+	 * to replace reserved slots with required structure field
+	 * additions of your backport.
+	 */
+	RH_KABI_USE2_P(1, __u32	sk_txhash, u32 sk_max_pacing_rate)
+	RH_KABI_RESERVE_P(2)
+	RH_KABI_RESERVE_P(3)
+	RH_KABI_RESERVE_P(4)
+	RH_KABI_RESERVE_P(5)
+	RH_KABI_RESERVE_P(6)
+	RH_KABI_RESERVE_P(7)
+	RH_KABI_RESERVE_P(8)
+};
+
+#define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))
+
+#define rcu_dereference_sk_user_data(sk)	rcu_dereference(__sk_user_data((sk)))
+#define rcu_assign_sk_user_data(sk, ptr)	rcu_assign_pointer(__sk_user_data((sk)), ptr)
+
+/*
+ * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK
+ * or not whether his port will be reused by someone else. SK_FORCE_REUSE
+ * on a socket means that the socket will reuse everybody else's port
+ * without looking at the other's sk_reuse value.
+ */
+
+#define SK_NO_REUSE	0
+#define SK_CAN_REUSE	1
+#define SK_FORCE_REUSE	2
+
+static inline int sk_peek_offset(struct sock *sk, int flags)
+{
+	if ((flags & MSG_PEEK) && (sk->sk_peek_off >= 0))
+		return sk->sk_peek_off;
+	else
+		return 0;
+}
+
+static inline void sk_peek_offset_bwd(struct sock *sk, int val)
+{
+	if (sk->sk_peek_off >= 0) {
+		if (sk->sk_peek_off >= val)
+			sk->sk_peek_off -= val;
+		else
+			sk->sk_peek_off = 0;
+	}
+}
+
+static inline void sk_peek_offset_fwd(struct sock *sk, int val)
+{
+	if (sk->sk_peek_off >= 0)
+		sk->sk_peek_off += val;
+}
+
+/*
+ * Hashed lists helper routines
+ */
+static inline struct sock *sk_entry(const struct hlist_node *node)
+{
+	return hlist_entry(node, struct sock, sk_node);
+}
+
+static inline struct sock *__sk_head(const struct hlist_head *head)
+{
+	return hlist_entry(head->first, struct sock, sk_node);
+}
+
+static inline struct sock *sk_head(const struct hlist_head *head)
+{
+	return hlist_empty(head) ? NULL : __sk_head(head);
+}
+
+static inline struct sock *__sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_entry(head->first, struct sock, sk_nulls_node);
+}
+
+static inline struct sock *sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_empty(head) ? NULL : __sk_nulls_head(head);
+}
+
+static inline struct sock *sk_next(const struct sock *sk)
+{
+	return sk->sk_node.next ?
+		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
+}
+
+static inline struct sock *sk_nulls_next(const struct sock *sk)
+{
+	return (!is_a_nulls(sk->sk_nulls_node.next)) ?
+		hlist_nulls_entry(sk->sk_nulls_node.next,
+				  struct sock, sk_nulls_node) :
+		NULL;
+}
+
+static inline bool sk_unhashed(const struct sock *sk)
+{
+	return hlist_unhashed(&sk->sk_node);
+}
+
+static inline bool sk_hashed(const struct sock *sk)
+{
+	return !sk_unhashed(sk);
+}
+
+static inline void sk_node_init(struct hlist_node *node)
+{
+	node->pprev = NULL;
+}
+
+static inline void sk_nulls_node_init(struct hlist_nulls_node *node)
+{
+	node->pprev = NULL;
+}
+
+static inline void __sk_del_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_node);
+}
+
+/* NB: equivalent to hlist_del_init_rcu */
+static inline bool __sk_del_node_init(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		__sk_del_node(sk);
+		sk_node_init(&sk->sk_node);
+		return true;
+	}
+	return false;
+}
+
+/* Grab socket reference count. This operation is valid only
+   when sk is ALREADY grabbed f.e. it is found in hash table
+   or a list and the lookup is made under lock preventing hash table
+   modifications.
+ */
+
+static inline void sock_hold(struct sock *sk)
+{
+	atomic_inc(&sk->sk_refcnt);
+}
+
+/* Ungrab socket in the context, which assumes that socket refcnt
+   cannot hit zero, f.e. it is true in context of any socketcall.
+ */
+static inline void __sock_put(struct sock *sk)
+{
+	atomic_dec(&sk->sk_refcnt);
+}
+
+static inline bool sk_del_node_init(struct sock *sk)
+{
+	bool rc = __sk_del_node_init(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+#define sk_del_node_init_rcu(sk)	sk_del_node_init(sk)
+
+static inline bool __sk_nulls_del_node_init_rcu(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		hlist_nulls_del_init_rcu(&sk->sk_nulls_node);
+		return true;
+	}
+	return false;
+}
+
+static inline bool sk_nulls_del_node_init_rcu(struct sock *sk)
+{
+	bool rc = __sk_nulls_del_node_init_rcu(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+
+static inline void __sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_node, list);
+}
+
+static inline void sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	__sk_add_node(sk, list);
+}
+
+static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	hlist_add_head_rcu(&sk->sk_node, list);
+}
+
+static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+{
+	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
+}
+
+static inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+{
+	sock_hold(sk);
+	__sk_nulls_add_node_rcu(sk, list);
+}
+
+static inline void __sk_del_bind_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_bind_node);
+}
+
+static inline void sk_add_bind_node(struct sock *sk,
+					struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_bind_node, list);
+}
+
+#define sk_for_each(__sk, list) \
+	hlist_for_each_entry(__sk, list, sk_node)
+#define sk_for_each_rcu(__sk, list) \
+	hlist_for_each_entry_rcu(__sk, list, sk_node)
+#define sk_nulls_for_each(__sk, node, list) \
+	hlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)
+#define sk_nulls_for_each_rcu(__sk, node, list) \
+	hlist_nulls_for_each_entry_rcu(__sk, node, list, sk_nulls_node)
+#define sk_for_each_from(__sk) \
+	hlist_for_each_entry_from(__sk, sk_node)
+#define sk_nulls_for_each_from(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \
+		hlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)
+#define sk_for_each_safe(__sk, tmp, list) \
+	hlist_for_each_entry_safe(__sk, tmp, list, sk_node)
+#define sk_for_each_bound(__sk, list) \
+	hlist_for_each_entry(__sk, list, sk_bind_node)
+
+static inline struct user_namespace *sk_user_ns(struct sock *sk)
+{
+	/* Careful only use this in a context where these parameters
+	 * can not change and must all be valid, such as recvmsg from
+	 * userspace.
+	 */
+	return sk->sk_socket->file->f_cred->user_ns;
+}
+
+/* Sock flags */
+enum sock_flags {
+	SOCK_DEAD,
+	SOCK_DONE,
+	SOCK_URGINLINE,
+	SOCK_KEEPOPEN,
+	SOCK_LINGER,
+	SOCK_DESTROY,
+	SOCK_BROADCAST,
+	SOCK_TIMESTAMP,
+	SOCK_ZAPPED,
+	SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
+	SOCK_DBG, /* %SO_DEBUG setting */
+	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
+	SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
+	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
+	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+	SOCK_MEMALLOC, /* VM depends on this socket for swapping */
+	SOCK_TIMESTAMPING_TX_HARDWARE,  /* %SOF_TIMESTAMPING_TX_HARDWARE */
+	SOCK_TIMESTAMPING_TX_SOFTWARE,  /* %SOF_TIMESTAMPING_TX_SOFTWARE */
+	SOCK_TIMESTAMPING_RX_HARDWARE,  /* %SOF_TIMESTAMPING_RX_HARDWARE */
+	SOCK_TIMESTAMPING_RX_SOFTWARE,  /* %SOF_TIMESTAMPING_RX_SOFTWARE */
+	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
+	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
+	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
+	SOCK_FASYNC, /* fasync() active */
+	SOCK_RXQ_OVFL,
+	SOCK_ZEROCOPY, /* buffers from userspace */
+	SOCK_WIFI_STATUS, /* push wifi status to userspace */
+	SOCK_NOFCS, /* Tell NIC not to do the Ethernet FCS.
+		     * Will use last 4 bytes of packet sent from
+		     * user-space instead.
+		     */
+	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
+	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
+};
+
+static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
+{
+	nsk->sk_flags = osk->sk_flags;
+}
+
+static inline void sock_set_flag(struct sock *sk, enum sock_flags flag)
+{
+	__set_bit(flag, &sk->sk_flags);
+}
+
+static inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)
+{
+	__clear_bit(flag, &sk->sk_flags);
+}
+
+static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
+{
+	return test_bit(flag, &sk->sk_flags);
+}
+
+#ifdef CONFIG_NET
+extern struct static_key memalloc_socks;
+static inline int sk_memalloc_socks(void)
+{
+	return static_key_false(&memalloc_socks);
+}
+#else
+
+static inline int sk_memalloc_socks(void)
+{
+	return 0;
+}
+
+#endif
+
+static inline gfp_t sk_gfp_atomic(struct sock *sk, gfp_t gfp_mask)
+{
+	return GFP_ATOMIC | (sk->sk_allocation & __GFP_MEMALLOC);
+}
+
+static inline void sk_acceptq_removed(struct sock *sk)
+{
+	sk->sk_ack_backlog--;
+}
+
+static inline void sk_acceptq_added(struct sock *sk)
+{
+	sk->sk_ack_backlog++;
+}
+
+static inline bool sk_acceptq_is_full(const struct sock *sk)
+{
+	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
+}
+
+/*
+ * Compute minimal free write space needed to queue new packets.
+ */
+static inline int sk_stream_min_wspace(const struct sock *sk)
+{
+	return sk->sk_wmem_queued >> 1;
+}
+
+static inline int sk_stream_wspace(const struct sock *sk)
+{
+	return sk->sk_sndbuf - sk->sk_wmem_queued;
+}
+
+extern void sk_stream_write_space(struct sock *sk);
+
+/* OOB backlog add */
+static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+{
+	/* dont let skb dst not refcounted, we are going to leave rcu lock */
+	skb_dst_force_safe(skb);
+
+	if (!sk->sk_backlog.tail)
+		sk->sk_backlog.head = skb;
+	else
+		sk->sk_backlog.tail->next = skb;
+
+	sk->sk_backlog.tail = skb;
+	skb->next = NULL;
+}
+
+/*
+ * Take into account size of receive queue and backlog queue
+ * Do not take into account this skb truesize,
+ * to allow even a single big packet to come.
+ */
+static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
+				     unsigned int limit)
+{
+	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
+
+	return qsize > limit;
+}
+
+/* The per-socket spinlock must be held here. */
+static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,
+					      unsigned int limit)
+{
+	if (sk_rcvqueues_full(sk, skb, limit))
+		return -ENOBUFS;
+
+	__sk_add_backlog(sk, skb);
+	sk->sk_backlog.len += skb->truesize;
+	return 0;
+}
+
+extern int __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb);
+
+static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	if (sk_memalloc_socks() && skb_pfmemalloc(skb))
+		return __sk_backlog_rcv(sk, skb);
+
+	return sk->sk_backlog_rcv(sk, skb);
+}
+
+static inline void sock_rps_record_flow(const struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	struct rps_sock_flow_table *sock_flow_table;
+
+	rcu_read_lock();
+	sock_flow_table = rcu_dereference(rps_sock_flow_table);
+	rps_record_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rcu_read_unlock();
+#endif
+}
+
+static inline void sock_rps_reset_flow(const struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	struct rps_sock_flow_table *sock_flow_table;
+
+	rcu_read_lock();
+	sock_flow_table = rcu_dereference(rps_sock_flow_table);
+	rps_reset_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rcu_read_unlock();
+#endif
+}
+
+static inline void sock_rps_save_rxhash(struct sock *sk,
+					const struct sk_buff *skb)
+{
+#ifdef CONFIG_RPS
+	if (unlikely(sk->sk_rxhash != skb->hash)) {
+		sock_rps_reset_flow(sk);
+		sk->sk_rxhash = skb->hash;
+	}
+#endif
+}
+
+static inline void sock_rps_reset_rxhash(struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	sock_rps_reset_flow(sk);
+	sk->sk_rxhash = 0;
+#endif
+}
+
+#define sk_wait_event(__sk, __timeo, __condition)			\
+	({	int __rc;						\
+		release_sock(__sk);					\
+		__rc = __condition;					\
+		if (!__rc) {						\
+			*(__timeo) = schedule_timeout(*(__timeo));	\
+		}							\
+		lock_sock(__sk);					\
+		__rc = __condition;					\
+		__rc;							\
+	})
+
+extern int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
+extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
+extern void sk_stream_wait_close(struct sock *sk, long timeo_p);
+extern int sk_stream_error(struct sock *sk, int flags, int err);
+extern void sk_stream_kill_queues(struct sock *sk);
+extern void sk_set_memalloc(struct sock *sk);
+extern void sk_clear_memalloc(struct sock *sk);
+
+int sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb);
+
+struct request_sock_ops;
+struct timewait_sock_ops;
+struct inet_hashinfo;
+struct raw_hashinfo;
+struct module;
+
+/*
+ * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes
+ * un-modified. Special care is taken when initializing object to zero.
+ */
+static inline void sk_prot_clear_nulls(struct sock *sk, int size)
+{
+	if (offsetof(struct sock, sk_node.next) != 0)
+		memset(sk, 0, offsetof(struct sock, sk_node.next));
+	memset(&sk->sk_node.pprev, 0,
+	       size - offsetof(struct sock, sk_node.pprev));
+}
+
+/* Networking protocol blocks we attach to sockets.
+ * socket layer -> transport layer interface
+ * transport -> network interface is defined by struct inet_proto
+ */
+struct proto {
+	void			(*close)(struct sock *sk,
+					long timeout);
+	int			(*connect)(struct sock *sk,
+					struct sockaddr *uaddr,
+					int addr_len);
+	int			(*disconnect)(struct sock *sk, int flags);
+
+	struct sock *		(*accept)(struct sock *sk, int flags, int *err);
+
+	int			(*ioctl)(struct sock *sk, int cmd,
+					 unsigned long arg);
+	int			(*init)(struct sock *sk);
+	void			(*destroy)(struct sock *sk);
+	void			(*shutdown)(struct sock *sk, int how);
+	int			(*setsockopt)(struct sock *sk, int level,
+					int optname, char __user *optval,
+					unsigned int optlen);
+	int			(*getsockopt)(struct sock *sk, int level,
+					int optname, char __user *optval,
+					int __user *option);
+#ifdef CONFIG_COMPAT
+	int			(*compat_setsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					unsigned int optlen);
+	int			(*compat_getsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					int __user *option);
+	int			(*compat_ioctl)(struct sock *sk,
+					unsigned int cmd, unsigned long arg);
+#endif
+	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg, size_t len);
+	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg,
+					   size_t len, int noblock, int flags,
+					   int *addr_len);
+	int			(*sendpage)(struct sock *sk, struct page *page,
+					int offset, size_t size, int flags);
+	int			(*bind)(struct sock *sk,
+					struct sockaddr *uaddr, int addr_len);
+
+	int			(*backlog_rcv) (struct sock *sk,
+						struct sk_buff *skb);
+
+	void		(*release_cb)(struct sock *sk);
+	void		(*mtu_reduced)(struct sock *sk);	/* not used anymore */
+
+	/* Keeping track of sk's, looking them up, and port selection methods. */
+	void			(*hash)(struct sock *sk);
+	void			(*unhash)(struct sock *sk);
+	void			(*rehash)(struct sock *sk);
+	int			(*get_port)(struct sock *sk, unsigned short snum);
+	void			(*clear_sk)(struct sock *sk, int size);
+
+	/* Keeping track of sockets in use */
+#ifdef CONFIG_PROC_FS
+	unsigned int		inuse_idx;
+#endif
+
+#ifndef __GENKSYMS__
+	bool			(*stream_memory_free)(const struct sock *sk);
+#endif
+
+	/* Memory pressure */
+	void			(*enter_memory_pressure)(struct sock *sk);
+	atomic_long_t		*memory_allocated;	/* Current allocated memory. */
+	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
+	/*
+	 * Pressure flag: try to collapse.
+	 * Technical note: it is used by multiple contexts non atomically.
+	 * All the __sk_mem_schedule() is of this nature: accounting
+	 * is strict, actions are advisory and have some latency.
+	 */
+	int			*memory_pressure;
+	long			*sysctl_mem;
+	int			*sysctl_wmem;
+	int			*sysctl_rmem;
+	int			max_header;
+	bool			no_autobind;
+
+	struct kmem_cache	*slab;
+	unsigned int		obj_size;
+	int			slab_flags;
+
+	struct percpu_counter	*orphan_count;
+
+	struct request_sock_ops	*rsk_prot;
+	struct timewait_sock_ops *twsk_prot;
+
+	union {
+		struct inet_hashinfo	*hashinfo;
+		struct udp_table	*udp_table;
+		struct raw_hashinfo	*raw_hash;
+	} h;
+
+	struct module		*owner;
+
+	char			name[32];
+
+	struct list_head	node;
+#ifdef SOCK_REFCNT_DEBUG
+	atomic_t		socks;
+#endif
+#ifdef CONFIG_MEMCG_KMEM
+	/*
+	 * cgroup specific init/deinit functions. Called once for all
+	 * protocols that implement it, from cgroups populate function.
+	 * This function has to setup any files the protocol want to
+	 * appear in the kmem cgroup filesystem.
+	 */
+	int			(*init_cgroup)(struct mem_cgroup *memcg,
+					       struct cgroup_subsys *ss);
+	void			(*destroy_cgroup)(struct mem_cgroup *memcg);
+	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
+#endif
+};
+
+/*
+ * Bits in struct cg_proto.flags
+ */
+enum cg_proto_flags {
+	/* Currently active and new sockets should be assigned to cgroups */
+	MEMCG_SOCK_ACTIVE,
+	/* It was ever activated; we must disarm static keys on destruction */
+	MEMCG_SOCK_ACTIVATED,
+};
+
+struct cg_proto {
+	void			(*enter_memory_pressure)(struct sock *sk);
+	struct res_counter	*memory_allocated;	/* Current allocated memory. */
+	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
+	int			*memory_pressure;
+	long			*sysctl_mem;
+	unsigned long		flags;
+	/*
+	 * memcg field is used to find which memcg we belong directly
+	 * Each memcg struct can hold more than one cg_proto, so container_of
+	 * won't really cut.
+	 *
+	 * The elegant solution would be having an inverse function to
+	 * proto_cgroup in struct proto, but that means polluting the structure
+	 * for everybody, instead of just for memcg users.
+	 */
+	struct mem_cgroup	*memcg;
+};
+
+extern int proto_register(struct proto *prot, int alloc_slab);
+extern void proto_unregister(struct proto *prot);
+
+static inline bool memcg_proto_active(struct cg_proto *cg_proto)
+{
+	return test_bit(MEMCG_SOCK_ACTIVE, &cg_proto->flags);
+}
+
+static inline bool memcg_proto_activated(struct cg_proto *cg_proto)
+{
+	return test_bit(MEMCG_SOCK_ACTIVATED, &cg_proto->flags);
+}
+
+#ifdef SOCK_REFCNT_DEBUG
+static inline void sk_refcnt_debug_inc(struct sock *sk)
+{
+	atomic_inc(&sk->sk_prot->socks);
+}
+
+static inline void sk_refcnt_debug_dec(struct sock *sk)
+{
+	atomic_dec(&sk->sk_prot->socks);
+	printk(KERN_DEBUG "%s socket %p released, %d are still alive\n",
+	       sk->sk_prot->name, sk, atomic_read(&sk->sk_prot->socks));
+}
+
+static inline void sk_refcnt_debug_release(const struct sock *sk)
+{
+	if (atomic_read(&sk->sk_refcnt) != 1)
+		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",
+		       sk->sk_prot->name, sk, atomic_read(&sk->sk_refcnt));
+}
+#else /* SOCK_REFCNT_DEBUG */
+#define sk_refcnt_debug_inc(sk) do { } while (0)
+#define sk_refcnt_debug_dec(sk) do { } while (0)
+#define sk_refcnt_debug_release(sk) do { } while (0)
+#endif /* SOCK_REFCNT_DEBUG */
+
+#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_NET)
+extern struct static_key memcg_socket_limit_enabled;
+static inline struct cg_proto *parent_cg_proto(struct proto *proto,
+					       struct cg_proto *cg_proto)
+{
+	return proto->proto_cgroup(parent_mem_cgroup(cg_proto->memcg));
+}
+#define mem_cgroup_sockets_enabled static_key_false(&memcg_socket_limit_enabled)
+#else
+#define mem_cgroup_sockets_enabled 0
+static inline struct cg_proto *parent_cg_proto(struct proto *proto,
+					       struct cg_proto *cg_proto)
+{
+	return NULL;
+}
+#endif
+
+static inline bool sk_stream_memory_free(const struct sock *sk)
+{
+	if (sk->sk_wmem_queued >= sk->sk_sndbuf)
+		return false;
+
+	return sk->sk_prot->stream_memory_free ?
+		sk->sk_prot->stream_memory_free(sk) : true;
+}
+
+static inline bool sk_stream_is_writeable(const struct sock *sk)
+{
+	return sk_stream_wspace(sk) >= sk_stream_min_wspace(sk) &&
+	       sk_stream_memory_free(sk);
+}
+
+
+static inline bool sk_has_memory_pressure(const struct sock *sk)
+{
+	return sk->sk_prot->memory_pressure != NULL;
+}
+
+static inline bool sk_under_memory_pressure(const struct sock *sk)
+{
+	if (!sk->sk_prot->memory_pressure)
+		return false;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return !!*sk->sk_cgrp->memory_pressure;
+
+	return !!*sk->sk_prot->memory_pressure;
+}
+
+static inline void sk_leave_memory_pressure(struct sock *sk)
+{
+	int *memory_pressure = sk->sk_prot->memory_pressure;
+
+	if (!memory_pressure)
+		return;
+
+	if (*memory_pressure)
+		*memory_pressure = 0;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+		struct proto *prot = sk->sk_prot;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			if (*cg_proto->memory_pressure)
+				*cg_proto->memory_pressure = 0;
+	}
+
+}
+
+static inline void sk_enter_memory_pressure(struct sock *sk)
+{
+	if (!sk->sk_prot->enter_memory_pressure)
+		return;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+		struct proto *prot = sk->sk_prot;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			cg_proto->enter_memory_pressure(sk);
+	}
+
+	sk->sk_prot->enter_memory_pressure(sk);
+}
+
+static inline long sk_prot_mem_limits(const struct sock *sk, int index)
+{
+	long *prot = sk->sk_prot->sysctl_mem;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		prot = sk->sk_cgrp->sysctl_mem;
+	return prot[index];
+}
+
+static inline void memcg_memory_allocated_add(struct cg_proto *prot,
+					      unsigned long amt,
+					      int *parent_status)
+{
+	struct res_counter *fail;
+	int ret;
+
+	ret = res_counter_charge_nofail(prot->memory_allocated,
+					amt << PAGE_SHIFT, &fail);
+	if (ret < 0)
+		*parent_status = OVER_LIMIT;
+}
+
+static inline void memcg_memory_allocated_sub(struct cg_proto *prot,
+					      unsigned long amt)
+{
+	res_counter_uncharge(prot->memory_allocated, amt << PAGE_SHIFT);
+}
+
+static inline u64 memcg_memory_allocated_read(struct cg_proto *prot)
+{
+	u64 ret;
+	ret = res_counter_read_u64(prot->memory_allocated, RES_USAGE);
+	return ret >> PAGE_SHIFT;
+}
+
+static inline long
+sk_memory_allocated(const struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return memcg_memory_allocated_read(sk->sk_cgrp);
+
+	return atomic_long_read(prot->memory_allocated);
+}
+
+static inline long
+sk_memory_allocated_add(struct sock *sk, int amt, int *parent_status)
+{
+	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		memcg_memory_allocated_add(sk->sk_cgrp, amt, parent_status);
+		/* update the root cgroup regardless */
+		atomic_long_add_return(amt, prot->memory_allocated);
+		return memcg_memory_allocated_read(sk->sk_cgrp);
+	}
+
+	return atomic_long_add_return(amt, prot->memory_allocated);
+}
+
+static inline void
+sk_memory_allocated_sub(struct sock *sk, int amt)
+{
+	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		memcg_memory_allocated_sub(sk->sk_cgrp, amt);
+
+	atomic_long_sub(amt, prot->memory_allocated);
+}
+
+static inline void sk_sockets_allocated_dec(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			percpu_counter_dec(cg_proto->sockets_allocated);
+	}
+
+	percpu_counter_dec(prot->sockets_allocated);
+}
+
+static inline void sk_sockets_allocated_inc(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			percpu_counter_inc(cg_proto->sockets_allocated);
+	}
+
+	percpu_counter_inc(prot->sockets_allocated);
+}
+
+static inline int
+sk_sockets_allocated_read_positive(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return percpu_counter_read_positive(sk->sk_cgrp->sockets_allocated);
+
+	return percpu_counter_read_positive(prot->sockets_allocated);
+}
+
+static inline int
+proto_sockets_allocated_sum_positive(struct proto *prot)
+{
+	return percpu_counter_sum_positive(prot->sockets_allocated);
+}
+
+static inline long
+proto_memory_allocated(struct proto *prot)
+{
+	return atomic_long_read(prot->memory_allocated);
+}
+
+static inline bool
+proto_memory_pressure(struct proto *prot)
+{
+	if (!prot->memory_pressure)
+		return false;
+	return !!*prot->memory_pressure;
+}
+
+
+#ifdef CONFIG_PROC_FS
+/* Called with local bh disabled */
+extern void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
+extern int sock_prot_inuse_get(struct net *net, struct proto *proto);
+#else
+static inline void sock_prot_inuse_add(struct net *net, struct proto *prot,
+		int inc)
+{
+}
+#endif
+
+
+/* With per-bucket locks this operation is not-atomic, so that
+ * this version is not worse.
+ */
+static inline void __sk_prot_rehash(struct sock *sk)
+{
+	sk->sk_prot->unhash(sk);
+	sk->sk_prot->hash(sk);
+}
+
+void sk_prot_clear_portaddr_nulls(struct sock *sk, int size);
+
+/* About 10 seconds */
+#define SOCK_DESTROY_TIME (10*HZ)
+
+/* Sockets 0-1023 can't be bound to unless you are superuser */
+#define PROT_SOCK	1024
+
+#define SHUTDOWN_MASK	3
+#define RCV_SHUTDOWN	1
+#define SEND_SHUTDOWN	2
+
+#define SOCK_SNDBUF_LOCK	1
+#define SOCK_RCVBUF_LOCK	2
+#define SOCK_BINDADDR_LOCK	4
+#define SOCK_BINDPORT_LOCK	8
+
+/* sock_iocb: used to kick off async processing of socket ios */
+struct sock_iocb {
+	struct list_head	list;
+
+	int			flags;
+	int			size;
+	struct socket		*sock;
+	struct sock		*sk;
+	struct scm_cookie	*scm;
+	struct msghdr		*msg, async_msg;
+	struct kiocb		*kiocb;
+};
+
+static inline struct sock_iocb *kiocb_to_siocb(struct kiocb *iocb)
+{
+	return (struct sock_iocb *)iocb->private;
+}
+
+static inline struct kiocb *siocb_to_kiocb(struct sock_iocb *si)
+{
+	return si->kiocb;
+}
+
+struct socket_alloc {
+	struct socket socket;
+	struct inode vfs_inode;
+};
+
+static inline struct socket *SOCKET_I(struct inode *inode)
+{
+	return &container_of(inode, struct socket_alloc, vfs_inode)->socket;
+}
+
+static inline struct inode *SOCK_INODE(struct socket *socket)
+{
+	return &container_of(socket, struct socket_alloc, socket)->vfs_inode;
+}
+
+/*
+ * Functions for memory accounting
+ */
+extern int __sk_mem_schedule(struct sock *sk, int size, int kind);
+extern void __sk_mem_reclaim(struct sock *sk);
+
+#define SK_MEM_QUANTUM ((int)PAGE_SIZE)
+#define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
+#define SK_MEM_SEND	0
+#define SK_MEM_RECV	1
+
+static inline int sk_mem_pages(int amt)
+{
+	return (amt + SK_MEM_QUANTUM - 1) >> SK_MEM_QUANTUM_SHIFT;
+}
+
+static inline bool sk_has_account(struct sock *sk)
+{
+	/* return true if protocol supports memory accounting */
+	return !!sk->sk_prot->memory_allocated;
+}
+
+static inline bool sk_wmem_schedule(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return true;
+	return size <= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_SEND);
+}
+
+static inline bool
+sk_rmem_schedule(struct sock *sk, struct sk_buff *skb, int size)
+{
+	if (!sk_has_account(sk))
+		return true;
+	return size<= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_RECV) ||
+		skb_pfmemalloc(skb);
+}
+
+static inline void sk_mem_reclaim(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc >= SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
+static inline void sk_mem_reclaim_partial(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc > SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
+static inline void sk_mem_charge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc -= size;
+}
+
+static inline void sk_mem_uncharge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc += size;
+}
+
+static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
+{
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued -= skb->truesize;
+	sk_mem_uncharge(sk, skb->truesize);
+	__kfree_skb(skb);
+}
+
+/* Used by processes to "lock" a socket state, so that
+ * interrupts and bottom half handlers won't change it
+ * from under us. It essentially blocks any incoming
+ * packets, so that we won't get any new data or any
+ * packets that change the state of the socket.
+ *
+ * While locked, BH processing will add new packets to
+ * the backlog queue.  This queue is processed by the
+ * owner of the socket lock right before it is released.
+ *
+ * Since ~2.3.5 it is also exclusive sleep lock serializing
+ * accesses from user process context.
+ */
+#define sock_owned_by_user(sk)	((sk)->sk_lock.owned)
+
+static inline void sock_release_ownership(struct sock *sk)
+{
+	sk->sk_lock.owned = 0;
+}
+
+/*
+ * Macro so as to not evaluate some arguments when
+ * lockdep is not enabled.
+ *
+ * Mark both the sk_lock and the sk_lock.slock as a
+ * per-address-family lock class.
+ */
+#define sock_lock_init_class_and_name(sk, sname, skey, name, key)	\
+do {									\
+	sk->sk_lock.owned = 0;						\
+	init_waitqueue_head(&sk->sk_lock.wq);				\
+	spin_lock_init(&(sk)->sk_lock.slock);				\
+	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\
+			sizeof((sk)->sk_lock));				\
+	lockdep_set_class_and_name(&(sk)->sk_lock.slock,		\
+				(skey), (sname));				\
+	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
+} while (0)
+
+extern void lock_sock_nested(struct sock *sk, int subclass);
+
+static inline void lock_sock(struct sock *sk)
+{
+	lock_sock_nested(sk, 0);
+}
+
+extern void release_sock(struct sock *sk);
+
+/* BH context may only use the following locking interface. */
+#define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
+#define bh_lock_sock_nested(__sk) \
+				spin_lock_nested(&((__sk)->sk_lock.slock), \
+				SINGLE_DEPTH_NESTING)
+#define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
+
+extern bool lock_sock_fast(struct sock *sk);
+/**
+ * unlock_sock_fast - complement of lock_sock_fast
+ * @sk: socket
+ * @slow: slow mode
+ *
+ * fast unlock socket for user context.
+ * If slow mode is on, we call regular release_sock()
+ */
+static inline void unlock_sock_fast(struct sock *sk, bool slow)
+{
+	if (slow)
+		release_sock(sk);
+	else
+		spin_unlock_bh(&sk->sk_lock.slock);
+}
+
+
+extern struct sock		*sk_alloc(struct net *net, int family,
+					  gfp_t priority,
+					  struct proto *prot);
+extern void			sk_free(struct sock *sk);
+extern void			sk_release_kernel(struct sock *sk);
+extern struct sock		*sk_clone_lock(const struct sock *sk,
+					       const gfp_t priority);
+
+extern struct sk_buff		*sock_wmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      gfp_t priority);
+extern struct sk_buff		*sock_rmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      gfp_t priority);
+extern void			sock_wfree(struct sk_buff *skb);
+extern void			sock_rfree(struct sk_buff *skb);
+extern void			sock_efree(struct sk_buff *skb);
+extern void			sock_edemux(struct sk_buff *skb);
+
+extern int			sock_setsockopt(struct socket *sock, int level,
+						int op, char __user *optval,
+						unsigned int optlen);
+
+extern int			sock_getsockopt(struct socket *sock, int level,
+						int op, char __user *optval,
+						int __user *optlen);
+extern struct sk_buff		*sock_alloc_send_skb(struct sock *sk,
+						     unsigned long size,
+						     int noblock,
+						     int *errcode);
+extern struct sk_buff		*sock_alloc_send_pskb(struct sock *sk,
+						      unsigned long header_len,
+						      unsigned long data_len,
+						      int noblock,
+						      int *errcode,
+						      int max_page_order);
+extern void *sock_kmalloc(struct sock *sk, int size,
+			  gfp_t priority);
+extern void sock_kfree_s(struct sock *sk, void *mem, int size);
+extern void sk_send_sigurg(struct sock *sk);
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * does not implement a particular function.
+ */
+extern int                      sock_no_bind(struct socket *,
+					     struct sockaddr *, int);
+extern int                      sock_no_connect(struct socket *,
+						struct sockaddr *, int, int);
+extern int                      sock_no_socketpair(struct socket *,
+						   struct socket *);
+extern int                      sock_no_accept(struct socket *,
+					       struct socket *, int);
+extern int                      sock_no_getname(struct socket *,
+						struct sockaddr *, int *, int);
+extern unsigned int             sock_no_poll(struct file *, struct socket *,
+					     struct poll_table_struct *);
+extern int                      sock_no_ioctl(struct socket *, unsigned int,
+					      unsigned long);
+extern int			sock_no_listen(struct socket *, int);
+extern int                      sock_no_shutdown(struct socket *, int);
+extern int			sock_no_getsockopt(struct socket *, int , int,
+						   char __user *, int __user *);
+extern int			sock_no_setsockopt(struct socket *, int, int,
+						   char __user *, unsigned int);
+extern int                      sock_no_sendmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t);
+extern int                      sock_no_recvmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t, int);
+extern int			sock_no_mmap(struct file *file,
+					     struct socket *sock,
+					     struct vm_area_struct *vma);
+extern ssize_t			sock_no_sendpage(struct socket *sock,
+						struct page *page,
+						int offset, size_t size,
+						int flags);
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * uses the inet style.
+ */
+extern int sock_common_getsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, int __user *optlen);
+extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
+			       struct msghdr *msg, size_t size, int flags);
+extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, unsigned int optlen);
+extern int compat_sock_common_getsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, int __user *optlen);
+extern int compat_sock_common_setsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, unsigned int optlen);
+
+extern void sk_common_release(struct sock *sk);
+
+/*
+ *	Default socket callbacks and setup code
+ */
+
+/* Initialise core socket variables */
+extern void sock_init_data(struct socket *sock, struct sock *sk);
+
+extern void sk_filter_release_rcu(struct rcu_head *rcu);
+
+/**
+ *	sk_filter_release - release a socket filter
+ *	@fp: filter to remove
+ *
+ *	Remove a filter from a socket and release its resources.
+ */
+
+static inline void sk_filter_release(struct sk_filter *fp)
+{
+	if (atomic_dec_and_test(&fp->refcnt))
+		call_rcu(&fp->rcu, sk_filter_release_rcu);
+}
+
+static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
+{
+	unsigned int size = sk_filter_len(fp);
+
+	atomic_sub(size, &sk->sk_omem_alloc);
+	sk_filter_release(fp);
+}
+
+static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+{
+	atomic_inc(&fp->refcnt);
+	atomic_add(sk_filter_len(fp), &sk->sk_omem_alloc);
+}
+
+/*
+ * Socket reference counting postulates.
+ *
+ * * Each user of socket SHOULD hold a reference count.
+ * * Each access point to socket (an hash table bucket, reference from a list,
+ *   running timer, skb in flight MUST hold a reference count.
+ * * When reference count hits 0, it means it will never increase back.
+ * * When reference count hits 0, it means that no references from
+ *   outside exist to this socket and current process on current CPU
+ *   is last user and may/should destroy this socket.
+ * * sk_free is called from any context: process, BH, IRQ. When
+ *   it is called, socket has no references from outside -> sk_free
+ *   may release descendant resources allocated by the socket, but
+ *   to the time when it is called, socket is NOT referenced by any
+ *   hash tables, lists etc.
+ * * Packets, delivered from outside (from network or from another process)
+ *   and enqueued on receive/error queues SHOULD NOT grab reference count,
+ *   when they sit in queue. Otherwise, packets will leak to hole, when
+ *   socket is looked up by one cpu and unhasing is made by another CPU.
+ *   It is true for udp/raw, netlink (leak to receive and error queues), tcp
+ *   (leak to backlog). Packet socket does all the processing inside
+ *   BR_NETPROTO_LOCK, so that it has not this race condition. UNIX sockets
+ *   use separate SMP lock, so that they are prone too.
+ */
+
+/* Ungrab socket and destroy it, if it was the last reference. */
+static inline void sock_put(struct sock *sk)
+{
+	if (atomic_dec_and_test(&sk->sk_refcnt))
+		sk_free(sk);
+}
+/* Generic version of sock_put(), dealing with all sockets
+ * (TCP_TIMEWAIT, ESTABLISHED...)
+ */
+void sock_gen_put(struct sock *sk);
+
+extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
+			  const int nested);
+
+static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
+{
+	sk->sk_tx_queue_mapping = tx_queue;
+}
+
+static inline void sk_tx_queue_clear(struct sock *sk)
+{
+	sk->sk_tx_queue_mapping = -1;
+}
+
+static inline int sk_tx_queue_get(const struct sock *sk)
+{
+	return sk ? sk->sk_tx_queue_mapping : -1;
+}
+
+static inline void sk_set_socket(struct sock *sk, struct socket *sock)
+{
+	sk_tx_queue_clear(sk);
+	sk->sk_socket = sock;
+}
+
+static inline wait_queue_head_t *sk_sleep(struct sock *sk)
+{
+	BUILD_BUG_ON(offsetof(struct socket_wq, wait) != 0);
+	return &rcu_dereference_raw(sk->sk_wq)->wait;
+}
+/* Detach socket from process context.
+ * Announce socket dead, detach it from wait queue and inode.
+ * Note that parent inode held reference count on this struct sock,
+ * we do not release it in this function, because protocol
+ * probably wants some additional cleanups or even continuing
+ * to work with this socket (TCP).
+ */
+static inline void sock_orphan(struct sock *sk)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sock_set_flag(sk, SOCK_DEAD);
+	sk_set_socket(sk, NULL);
+	sk->sk_wq  = NULL;
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+static inline void sock_graft(struct sock *sk, struct socket *parent)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sk->sk_wq = parent->wq;
+	parent->sk = sk;
+	sk_set_socket(sk, parent);
+	security_sock_graft(sk, parent);
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+extern kuid_t sock_i_uid(struct sock *sk);
+extern unsigned long sock_i_ino(struct sock *sk);
+
+static inline struct dst_entry *
+__sk_dst_get(struct sock *sk)
+{
+	return rcu_dereference_check(sk->sk_dst_cache, sock_owned_by_user(sk) ||
+						       lockdep_is_held(&sk->sk_lock.slock));
+}
+
+static inline struct dst_entry *
+sk_dst_get(struct sock *sk)
+{
+	struct dst_entry *dst;
+
+	rcu_read_lock();
+	dst = rcu_dereference(sk->sk_dst_cache);
+	if (dst && !atomic_inc_not_zero(&dst->__refcnt))
+		dst = NULL;
+	rcu_read_unlock();
+	return dst;
+}
+
+extern void sk_reset_txq(struct sock *sk);
+
+static inline void dst_negative_advice(struct sock *sk)
+{
+	struct dst_entry *ndst, *dst = __sk_dst_get(sk);
+
+	if (dst && dst->ops->negative_advice) {
+		ndst = dst->ops->negative_advice(dst);
+
+		if (ndst != dst) {
+			rcu_assign_pointer(sk->sk_dst_cache, ndst);
+			sk_reset_txq(sk);
+		}
+	}
+}
+
+static inline void
+__sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	struct dst_entry *old_dst;
+
+	sk_tx_queue_clear(sk);
+	/*
+	 * This can be called while sk is owned by the caller only,
+	 * with no state that can be checked in a rcu_dereference_check() cond
+	 */
+	old_dst = rcu_dereference_raw(sk->sk_dst_cache);
+	rcu_assign_pointer(sk->sk_dst_cache, dst);
+	dst_release(old_dst);
+}
+
+static inline void
+sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	struct dst_entry *old_dst;
+
+	sk_tx_queue_clear(sk);
+	old_dst = xchg(&sk->sk_dst_cache, dst);
+	dst_release(old_dst);
+}
+
+static inline void
+__sk_dst_reset(struct sock *sk)
+{
+	__sk_dst_set(sk, NULL);
+}
+
+static inline void
+sk_dst_reset(struct sock *sk)
+{
+	sk_dst_set(sk, NULL);
+}
+
+extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
+
+extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
+
+bool sk_mc_loop(struct sock *sk);
+
+static inline bool sk_can_gso(const struct sock *sk)
+{
+	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
+}
+
+extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
+
+static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
+{
+	sk->sk_route_nocaps |= flags;
+	sk->sk_route_caps &= ~flags;
+}
+
+static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
+					   char __user *from, char *to,
+					   int copy, int offset)
+{
+	if (skb->ip_summed == CHECKSUM_NONE) {
+		int err = 0;
+		__wsum csum = csum_and_copy_from_user(from, to, copy, 0, &err);
+		if (err)
+			return err;
+		skb->csum = csum_block_add(skb->csum, csum, offset);
+	} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {
+		if (!access_ok(VERIFY_READ, from, copy) ||
+		    __copy_from_user_nocache(to, from, copy))
+			return -EFAULT;
+	} else if (copy_from_user(to, from, copy))
+		return -EFAULT;
+
+	return 0;
+}
+
+static inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,
+				       char __user *from, int copy)
+{
+	int err, offset = skb->len;
+
+	err = skb_do_copy_data_nocache(sk, skb, from, skb_put(skb, copy),
+				       copy, offset);
+	if (err)
+		__skb_trim(skb, offset);
+
+	return err;
+}
+
+static inline int skb_copy_to_page_nocache(struct sock *sk, char __user *from,
+					   struct sk_buff *skb,
+					   struct page *page,
+					   int off, int copy)
+{
+	int err;
+
+	err = skb_do_copy_data_nocache(sk, skb, from, page_address(page) + off,
+				       copy, skb->len);
+	if (err)
+		return err;
+
+	skb->len	     += copy;
+	skb->data_len	     += copy;
+	skb->truesize	     += copy;
+	sk->sk_wmem_queued   += copy;
+	sk_mem_charge(sk, copy);
+	return 0;
+}
+
+static inline int skb_copy_to_page(struct sock *sk, char __user *from,
+				   struct sk_buff *skb, struct page *page,
+				   int off, int copy)
+{
+	if (skb->ip_summed == CHECKSUM_NONE) {
+		int err = 0;
+		__wsum csum = csum_and_copy_from_user(from,
+						     page_address(page) + off,
+							    copy, 0, &err);
+		if (err)
+			return err;
+		skb->csum = csum_block_add(skb->csum, csum, skb->len);
+	} else if (copy_from_user(page_address(page) + off, from, copy))
+		return -EFAULT;
+
+	skb->len	     += copy;
+	skb->data_len	     += copy;
+	skb->truesize	     += copy;
+	sk->sk_wmem_queued   += copy;
+	sk_mem_charge(sk, copy);
+	return 0;
+}
+
+/**
+ * sk_wmem_alloc_get - returns write allocations
+ * @sk: socket
+ *
+ * Returns sk_wmem_alloc minus initial offset of one
+ */
+static inline int sk_wmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_wmem_alloc) - 1;
+}
+
+/**
+ * sk_rmem_alloc_get - returns read allocations
+ * @sk: socket
+ *
+ * Returns sk_rmem_alloc
+ */
+static inline int sk_rmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_rmem_alloc);
+}
+
+/**
+ * sk_has_allocations - check if allocations are outstanding
+ * @sk: socket
+ *
+ * Returns true if socket has write or read allocations
+ */
+static inline bool sk_has_allocations(const struct sock *sk)
+{
+	return sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);
+}
+
+/**
+ * wq_has_sleeper - check if there are any waiting processes
+ * @wq: struct socket_wq
+ *
+ * Returns true if socket_wq has waiting processes
+ *
+ * The purpose of the wq_has_sleeper and sock_poll_wait is to wrap the memory
+ * barrier call. They were added due to the race found within the tcp code.
+ *
+ * Consider following tcp code paths:
+ *
+ * CPU1                  CPU2
+ *
+ * sys_select            receive packet
+ *   ...                 ...
+ *   __add_wait_queue    update tp->rcv_nxt
+ *   ...                 ...
+ *   tp->rcv_nxt check   sock_def_readable
+ *   ...                 {
+ *   schedule               rcu_read_lock();
+ *                          wq = rcu_dereference(sk->sk_wq);
+ *                          if (wq && waitqueue_active(&wq->wait))
+ *                              wake_up_interruptible(&wq->wait)
+ *                          ...
+ *                       }
+ *
+ * The race for tcp fires when the __add_wait_queue changes done by CPU1 stay
+ * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
+ * could then endup calling schedule and sleep forever if there are no more
+ * data on the socket.
+ *
+ */
+static inline bool wq_has_sleeper(struct socket_wq *wq)
+{
+	/* We need to be sure we are in sync with the
+	 * add_wait_queue modifications to the wait queue.
+	 *
+	 * This memory barrier is paired in the sock_poll_wait.
+	 */
+	smp_mb();
+	return wq && waitqueue_active(&wq->wait);
+}
+
+/**
+ * sock_poll_wait - place memory barrier behind the poll_wait call.
+ * @filp:           file
+ * @wait_address:   socket wait queue
+ * @p:              poll_table
+ *
+ * See the comments in the wq_has_sleeper function.
+ */
+static inline void sock_poll_wait(struct file *filp,
+		wait_queue_head_t *wait_address, poll_table *p)
+{
+	if (!poll_does_not_wait(p) && wait_address) {
+		poll_wait(filp, wait_address, p);
+		/* We need to be sure we are in sync with the
+		 * socket flags modification.
+		 *
+		 * This memory barrier is paired in the wq_has_sleeper.
+		 */
+		smp_mb();
+	}
+}
+
+static inline void skb_set_hash_from_sk(struct sk_buff *skb, struct sock *sk)
+{
+	if (sk->sk_txhash) {
+		skb->l4_hash = 1;
+		skb->hash = sk->sk_txhash;
+	}
+}
+
+/*
+ *	Queue a received datagram if it will fit. Stream and sequenced
+ *	protocols can't normally use this as they need to fit buffers in
+ *	and play with them.
+ *
+ *	Inlined as it's very short and called for pretty much every
+ *	packet ever received.
+ */
+
+static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
+{
+	skb_orphan(skb);
+	skb->sk = sk;
+	skb->destructor = sock_wfree;
+	skb_set_hash_from_sk(skb, sk);
+	/*
+	 * We used to take a refcount on sk, but following operation
+	 * is enough to guarantee sk_free() wont free this sock until
+	 * all in-flight packets are completed
+	 */
+	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
+}
+
+static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
+{
+	skb_orphan(skb);
+	skb->sk = sk;
+	skb->destructor = sock_rfree;
+	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+	sk_mem_charge(sk, skb->truesize);
+}
+
+extern void sk_reset_timer(struct sock *sk, struct timer_list *timer,
+			   unsigned long expires);
+
+extern void sk_stop_timer(struct sock *sk, struct timer_list *timer);
+
+extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
+
+extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
+
+/*
+ *	Recover an error report and clear atomically
+ */
+
+static inline int sock_error(struct sock *sk)
+{
+	int err;
+	if (likely(!sk->sk_err))
+		return 0;
+	err = xchg(&sk->sk_err, 0);
+	return -err;
+}
+
+static inline unsigned long sock_wspace(struct sock *sk)
+{
+	int amt = 0;
+
+	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
+		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
+		if (amt < 0)
+			amt = 0;
+	}
+	return amt;
+}
+
+static inline void sk_wake_async(struct sock *sk, int how, int band)
+{
+	if (sock_flag(sk, SOCK_FASYNC))
+		sock_wake_async(sk->sk_socket, how, band);
+}
+
+/* Since sk_{r,w}mem_alloc sums skb->truesize, even a small frame might
+ * need sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak.
+ * Note: for send buffers, TCP works better if we can build two skbs at
+ * minimum.
+ */
+#define TCP_SKB_MIN_TRUESIZE	(2048 + SKB_DATA_ALIGN(sizeof(struct sk_buff)))
+
+#define SOCK_MIN_SNDBUF		(TCP_SKB_MIN_TRUESIZE * 2)
+#define SOCK_MIN_RCVBUF		 TCP_SKB_MIN_TRUESIZE
+
+static inline void sk_stream_moderate_sndbuf(struct sock *sk)
+{
+	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
+		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
+		sk->sk_sndbuf = max_t(u32, sk->sk_sndbuf, SOCK_MIN_SNDBUF);
+	}
+}
+
+struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp);
+
+/**
+ * sk_page_frag - return an appropriate page_frag
+ * @sk: socket
+ *
+ * If socket allocation mode allows current thread to sleep, it means its
+ * safe to use the per task page_frag instead of the per socket one.
+ */
+static inline struct page_frag *sk_page_frag(struct sock *sk)
+{
+	if (sk->sk_allocation & __GFP_WAIT)
+		return &current->task_frag;
+
+	return &sk->sk_frag;
+}
+
+extern bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
+
+/*
+ *	Default write policy as shown to user space via poll/select/SIGIO
+ */
+static inline bool sock_writeable(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
+}
+
+static inline gfp_t gfp_any(void)
+{
+	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
+}
+
+static inline long sock_rcvtimeo(const struct sock *sk, bool noblock)
+{
+	return noblock ? 0 : sk->sk_rcvtimeo;
+}
+
+static inline long sock_sndtimeo(const struct sock *sk, bool noblock)
+{
+	return noblock ? 0 : sk->sk_sndtimeo;
+}
+
+static inline int sock_rcvlowat(const struct sock *sk, int waitall, int len)
+{
+	return (waitall ? len : min_t(int, sk->sk_rcvlowat, len)) ? : 1;
+}
+
+/* Alas, with timeout socket operations are not restartable.
+ * Compare this to poll().
+ */
+static inline int sock_intr_errno(long timeo)
+{
+	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
+}
+
+extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
+	struct sk_buff *skb);
+extern void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
+	struct sk_buff *skb);
+
+static inline void
+sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
+{
+	ktime_t kt = skb->tstamp;
+	struct skb_shared_hwtstamps *hwtstamps = skb_hwtstamps(skb);
+
+	/*
+	 * generate control messages if
+	 * - receive time stamping in software requested (SOCK_RCVTSTAMP
+	 *   or SOCK_TIMESTAMPING_RX_SOFTWARE)
+	 * - software time stamp available and wanted
+	 *   (SOCK_TIMESTAMPING_SOFTWARE)
+	 * - hardware time stamps available and wanted
+	 *   (SOCK_TIMESTAMPING_SYS_HARDWARE or
+	 *   SOCK_TIMESTAMPING_RAW_HARDWARE)
+	 */
+	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
+	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
+	    (kt.tv64 && sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE)) ||
+	    (hwtstamps->hwtstamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)) ||
+	    (hwtstamps->syststamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE)))
+		__sock_recv_timestamp(msg, sk, skb);
+	else
+		sk->sk_stamp = kt;
+
+	if (sock_flag(sk, SOCK_WIFI_STATUS) && skb->wifi_acked_valid)
+		__sock_recv_wifi_status(msg, sk, skb);
+}
+
+extern void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
+				     struct sk_buff *skb);
+
+static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
+					  struct sk_buff *skb)
+{
+#define FLAGS_TS_OR_DROPS ((1UL << SOCK_RXQ_OVFL)			| \
+			   (1UL << SOCK_RCVTSTAMP)			| \
+			   (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE)	| \
+			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
+			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE)	| \
+			   (1UL << SOCK_TIMESTAMPING_SYS_HARDWARE))
+
+	if (sk->sk_flags & FLAGS_TS_OR_DROPS)
+		__sock_recv_ts_and_drops(msg, sk, skb);
+	else
+		sk->sk_stamp = skb->tstamp;
+}
+
+/**
+ * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
+ * @sk:		socket sending this packet
+ * @tx_flags:	filled with instructions for time stamping
+ *
+ * Currently only depends on SOCK_TIMESTAMPING* flags.
+ */
+extern void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
+
+/**
+ * sk_eat_skb - Release a skb if it is no longer needed
+ * @sk: socket to eat this skb from
+ * @skb: socket buffer to eat
+ *
+ * This routine must be called with interrupts disabled or with the socket
+ * locked so that the sk_buff queue operation is ok.
+*/
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
+{
+	__skb_unlink(skb, &sk->sk_receive_queue);
+	__kfree_skb(skb);
+}
+
+static inline
+struct net *sock_net(const struct sock *sk)
+{
+	return read_pnet(&sk->sk_net);
+}
+
+static inline
+void sock_net_set(struct sock *sk, struct net *net)
+{
+	write_pnet(&sk->sk_net, net);
+}
+
+/*
+ * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
+ * They should not hold a reference to a namespace in order to allow
+ * to stop it.
+ * Sockets after sk_change_net should be released using sk_release_kernel
+ */
+static inline void sk_change_net(struct sock *sk, struct net *net)
+{
+	struct net *current_net = sock_net(sk);
+
+	if (!net_eq(current_net, net)) {
+		put_net(current_net);
+		sock_net_set(sk, net);
+	}
+}
+
+static inline struct sock *skb_steal_sock(struct sk_buff *skb)
+{
+	if (skb->sk) {
+		struct sock *sk = skb->sk;
+
+		skb->destructor = NULL;
+		skb->sk = NULL;
+		return sk;
+	}
+	return NULL;
+}
+
+extern void sock_enable_timestamp(struct sock *sk, int flag);
+extern int sock_get_timestamp(struct sock *, struct timeval __user *);
+extern int sock_get_timestampns(struct sock *, struct timespec __user *);
+
+/*
+ *	Enable debug/info messages
+ */
+extern int net_msg_warn;
+#define NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn) printk(fmt,##args); } while (0)
+
+#define LIMIT_NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
+
+/* This helper checks if a socket is a full socket,
+ * ie _not_ a timewait socket.
+ */
+static inline bool sk_fullsock(const struct sock *sk)
+{
+	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT);
+}
+
+extern __u32 sysctl_wmem_max;
+extern __u32 sysctl_rmem_max;
+
+extern int sysctl_optmem_max;
+
+extern __u32 sysctl_wmem_default;
+extern __u32 sysctl_rmem_default;
+
+#endif	/* _SOCK_H */
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/include/net/transp_v6.h linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/transp_v6.h
--- linux-3.10.0-327.36.3.el7.x86_64/include/net/transp_v6.h	2016-12-13 21:54:58.284000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/include/net/transp_v6.h	2016-12-13 21:55:12.022000000 +0800
@@ -57,7 +57,8 @@
 /*
  *	address family specific functions
  */
-extern const struct inet_connection_sock_af_ops ipv4_specific;
+extern struct inet_connection_sock_af_ops ipv6_mapped;
+extern struct inet_connection_sock_af_ops ipv4_specific;
 
 extern void inet6_destroy_sock(struct sock *sk);
 
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/kernel/hookers.c linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/hookers.c
--- linux-3.10.0-327.36.3.el7.x86_64/kernel/hookers.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/hookers.c	2016-12-13 21:55:12.023000000 +0800
@@ -0,0 +1,286 @@
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/rculist.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <linux/hookers.h>
+
+#include <net/net_namespace.h>
+#include <net/tcp.h>
+#include <net/transp_v6.h>
+#include <net/inet_common.h>
+#include <net/ipv6.h>
+#include <linux/inet.h>
+
+struct hooked_place {
+	char *name;	/* position information shown in procfs */
+	void *place;	/* the kernel address to be hook */
+	void *orig;	/* original content at hooked place */
+	void *stub;	/* hooker function stub */
+	int nr_hookers;	/* how many hookers are linked at below chain */
+	struct list_head chain;	/* hookers chain */
+};
+
+static spinlock_t hookers_lock;
+
+static struct sock *ipv4_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static struct sock *ipv6_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static struct sock *ipv6_mapped_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static int inet_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer);
+static int inet6_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer);
+
+static struct hooked_place place_table[] = {
+
+	{
+		.name = "ipv4_specific.syn_recv_sock",
+		.place = &ipv4_specific.syn_recv_sock,
+		.stub = ipv4_specific_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "ipv6_specific.syn_recv_sock",
+		.place = &ipv6_specific.syn_recv_sock,
+		.stub = ipv6_specific_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "ipv6_mapped.syn_recv_sock",
+		.place = &ipv6_mapped.syn_recv_sock,
+		.stub = ipv6_mapped_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "inet_stream_ops.getname",
+		.place = &inet_stream_ops.getname,
+		.stub = inet_stream_ops_getname_stub,
+	},
+
+	{
+		.name = "inet6_stream_ops.getname",
+		.place = &inet6_stream_ops.getname,
+		.stub = inet6_stream_ops_getname_stub,
+	},
+
+};
+
+static struct sock *__syn_recv_sock_hstub(struct hooked_place *place,
+				struct sock *sk, struct sk_buff *skb,
+			  struct request_sock *req, struct dst_entry *dst)
+{
+	struct hooker *iter;
+	struct sock *(*hooker_func)(struct sock *sk, struct sk_buff *skb,
+		  struct request_sock *req, struct dst_entry *dst,
+						struct sock **ret);
+	struct sock *(*orig_func)(struct sock *sk, struct sk_buff *skb,
+		  struct request_sock *req, struct dst_entry *dst);
+	struct sock *ret;
+
+	orig_func = place->orig;
+	ret = orig_func(sk, skb, req, dst);
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(iter, &place->chain, chain) {
+		hooker_func = iter->func;
+		hooker_func(sk, skb, req, dst, &ret);
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
+static int __getname_hstub(struct hooked_place *place,
+				struct socket *sock, struct sockaddr *uaddr,
+						int *uaddr_len, int peer)
+{
+	struct hooker *iter;
+	int (*hooker_func)(struct socket *sock, struct sockaddr *uaddr,
+			 int *uaddr_len, int peer, int *ret);
+	int (*orig_func)(struct socket *sock, struct sockaddr *uaddr,
+			 int *uaddr_len, int peer);
+	int ret;
+
+	orig_func = place->orig;
+	ret = orig_func(sock, uaddr, uaddr_len, peer);
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(iter, &place->chain, chain) {
+		hooker_func = iter->func;
+		hooker_func(sock, uaddr, uaddr_len, peer, &ret);
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
+static struct sock *ipv4_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[0], sk, skb, req, dst);
+}
+
+static struct sock *ipv6_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[1], sk, skb, req, dst);
+}
+
+static struct sock *ipv6_mapped_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[2], sk, skb, req, dst);
+}
+
+static int inet_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer)
+{
+	return __getname_hstub(&place_table[3], sock, uaddr, uaddr_len, peer);
+}
+
+static int inet6_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer)
+{
+	return __getname_hstub(&place_table[4], sock, uaddr, uaddr_len, peer);
+}
+
+#define PLACE_TABLE_SZ	(sizeof((place_table))/sizeof((place_table)[0]))
+
+int hooker_install(void *place, struct hooker *h)
+{
+	int i;
+	struct hooked_place *hplace;
+
+	might_sleep(); /* synchronize_rcu() */
+
+	if (!place || !h || !h->func)
+		return -EINVAL;
+
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		hplace = &place_table[i];
+		if (hplace->place == place) {
+			INIT_LIST_HEAD(&h->chain);
+			spin_lock(&hookers_lock);
+			hplace->nr_hookers++;
+			h->hplace = hplace;
+			list_add_tail_rcu(&h->chain, &place_table[i].chain);
+			spin_unlock(&hookers_lock);
+			synchronize_rcu();
+			break;
+		}
+	}
+
+	return (i >= PLACE_TABLE_SZ) ? -EINVAL : 0;
+}
+EXPORT_SYMBOL_GPL(hooker_install);
+
+void hooker_uninstall(struct hooker *h)
+{
+	might_sleep(); /* synchronize_rcu(); */
+
+	spin_lock(&hookers_lock);
+	list_del_rcu(&h->chain);
+	h->hplace->nr_hookers--;
+	h->hplace = NULL;
+	spin_unlock(&hookers_lock);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL_GPL(hooker_uninstall);
+
+static void *hookers_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	if (*pos < PLACE_TABLE_SZ)
+		return &place_table[*pos];
+	return NULL;
+}
+
+static void *hookers_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	if (++(*pos) >= PLACE_TABLE_SZ)
+		return NULL;
+
+	return (void *)&place_table[*pos];
+}
+
+static void hookers_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int hookers_seq_show(struct seq_file *seq, void *v)
+{
+	struct hooked_place *hplace = (struct hooked_place *)v;
+
+	seq_printf(seq, "name:%-24s addr:0x%p hookers:%-10d\n",
+			hplace->name, hplace->place, hplace->nr_hookers);
+	return 0;
+}
+
+static const struct seq_operations hookers_seq_ops = {
+	.start = hookers_seq_start,
+	.next  = hookers_seq_next,
+	.stop  = hookers_seq_stop,
+	.show  = hookers_seq_show,
+};
+
+static int hookers_seq_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &hookers_seq_ops);
+}
+
+static const struct file_operations hookers_seq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = hookers_seq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int hookers_init(void)
+{
+	int i;
+
+	if (!proc_create("hookers", 0, NULL, &hookers_seq_fops))
+		return -ENODEV;
+
+	spin_lock_init(&hookers_lock);
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		void **place = place_table[i].place;
+
+		place_table[i].orig = *place;
+		INIT_LIST_HEAD(&place_table[i].chain);
+		if (!place_table[i].stub)
+			break;
+		*place = place_table[i].stub;
+	}
+
+	return 0;
+}
+
+static void hookers_exit(void)
+{
+	int i;
+
+	remove_proc_entry("hookers", NULL);
+
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		void **place = place_table[i].place;
+		*place = place_table[i].orig;
+	}
+	synchronize_rcu();
+}
+
+module_init(hookers_init);
+module_exit(hookers_exit);
+MODULE_LICENSE("GPL");
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/kernel/Makefile linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/Makefile
--- linux-3.10.0-327.36.3.el7.x86_64/kernel/Makefile	2016-12-13 21:54:57.187000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/Makefile	2016-12-13 21:55:12.023000000 +0800
@@ -109,6 +109,7 @@
 obj-$(CONFIG_PERF_EVENTS) += events/
 
 obj-$(CONFIG_USER_RETURN_NOTIFIER) += user-return-notifier.o
+obj-$(CONFIG_HOOKERS) += hookers.o
 obj-$(CONFIG_PADATA) += padata.o
 obj-$(CONFIG_CRASH_DUMP) += crash_dump.o
 obj-$(CONFIG_JUMP_LABEL) += jump_label.o
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/kernel/Makefile.orig linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/Makefile.orig
--- linux-3.10.0-327.36.3.el7.x86_64/kernel/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/kernel/Makefile.orig	2016-12-13 21:55:12.024000000 +0800
@@ -0,0 +1,241 @@
+#
+# Makefile for the linux kernel.
+#
+
+obj-y     = fork.o exec_domain.o panic.o printk.o \
+	    cpu.o exit.o itimer.o time.o softirq.o resource.o \
+	    sysctl.o sysctl_binary.o capability.o ptrace.o timer.o user.o \
+	    signal.o sys.o kmod.o workqueue.o pid.o task_work.o \
+	    rcupdate.o extable.o params.o posix-timers.o \
+	    kthread.o wait.o sys_ni.o posix-cpu-timers.o mutex.o \
+	    hrtimer.o rwsem.o nsproxy.o srcu.o semaphore.o \
+	    notifier.o ksysfs.o cred.o \
+	    async.o range.o groups.o lglock.o smpboot.o \
+	    rh_taint.o rh_kabi.o rh_shadowman.o mcs_spinlock.o
+
+ifdef CONFIG_FUNCTION_TRACER
+# Do not trace debug files and internal ftrace files
+CFLAGS_REMOVE_lockdep.o = -pg
+CFLAGS_REMOVE_lockdep_proc.o = -pg
+CFLAGS_REMOVE_mutex-debug.o = -pg
+CFLAGS_REMOVE_rtmutex-debug.o = -pg
+CFLAGS_REMOVE_cgroup-debug.o = -pg
+CFLAGS_REMOVE_irq_work.o = -pg
+endif
+
+obj-y += sched/
+obj-y += power/
+obj-y += cpu/
+
+obj-$(CONFIG_CHECKPOINT_RESTORE) += kcmp.o
+obj-$(CONFIG_FREEZER) += freezer.o
+obj-$(CONFIG_PROFILING) += profile.o
+obj-$(CONFIG_STACKTRACE) += stacktrace.o
+obj-y += time/
+obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+obj-$(CONFIG_LOCKDEP) += lockdep.o
+ifeq ($(CONFIG_PROC_FS),y)
+obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
+endif
+obj-$(CONFIG_FUTEX) += futex.o
+ifeq ($(CONFIG_COMPAT),y)
+obj-$(CONFIG_FUTEX) += futex_compat.o
+endif
+obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
+obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
+obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
+obj-$(CONFIG_GENERIC_ISA_DMA) += dma.o
+obj-$(CONFIG_SMP) += smp.o
+ifneq ($(CONFIG_SMP),y)
+obj-y += up.o
+endif
+obj-$(CONFIG_SMP) += spinlock.o
+obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
+obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
+obj-$(CONFIG_UID16) += uid16.o
+obj-$(CONFIG_SYSTEM_TRUSTED_KEYRING) += system_keyring.o system_certificates.o
+obj-$(CONFIG_MODULES) += module.o
+obj-$(CONFIG_MODULE_SIG) += module_signing.o
+obj-$(CONFIG_MODULE_SIG_UEFI) += modsign_uefi.o
+obj-$(CONFIG_KALLSYMS) += kallsyms.o
+obj-$(CONFIG_BSD_PROCESS_ACCT) += acct.o
+obj-$(CONFIG_KEXEC) += kexec.o
+obj-$(CONFIG_BACKTRACE_SELF_TEST) += backtracetest.o
+obj-$(CONFIG_COMPAT) += compat.o
+obj-$(CONFIG_CGROUPS) += cgroup.o
+obj-$(CONFIG_CGROUP_FREEZER) += cgroup_freezer.o
+obj-$(CONFIG_CPUSETS) += cpuset.o
+obj-$(CONFIG_UTS_NS) += utsname.o
+obj-$(CONFIG_USER_NS) += user_namespace.o
+obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_IKCONFIG) += configs.o
+obj-$(CONFIG_RESOURCE_COUNTERS) += res_counter.o
+obj-$(CONFIG_SMP) += stop_machine.o
+obj-$(CONFIG_KPROBES_SANITY_TEST) += test_kprobes.o
+obj-$(CONFIG_AUDIT) += audit.o auditfilter.o
+obj-$(CONFIG_AUDITSYSCALL) += auditsc.o
+obj-$(CONFIG_AUDIT_WATCH) += audit_watch.o
+obj-$(CONFIG_AUDIT_TREE) += audit_tree.o
+obj-$(CONFIG_GCOV_KERNEL) += gcov/
+obj-$(CONFIG_KPROBES) += kprobes.o
+obj-$(CONFIG_KGDB) += debug/
+obj-$(CONFIG_DETECT_HUNG_TASK) += hung_task.o
+obj-$(CONFIG_LOCKUP_DETECTOR) += watchdog.o
+obj-$(CONFIG_GENERIC_HARDIRQS) += irq/
+obj-$(CONFIG_SECCOMP) += seccomp.o
+obj-$(CONFIG_RCU_TORTURE_TEST) += rcutorture.o
+obj-$(CONFIG_TREE_RCU) += rcutree.o
+obj-$(CONFIG_TREE_PREEMPT_RCU) += rcutree.o
+obj-$(CONFIG_TREE_RCU_TRACE) += rcutree_trace.o
+obj-$(CONFIG_TINY_RCU) += rcutiny.o
+obj-$(CONFIG_TINY_PREEMPT_RCU) += rcutiny.o
+obj-$(CONFIG_RELAY) += relay.o
+obj-$(CONFIG_SYSCTL) += utsname_sysctl.o
+obj-$(CONFIG_TASK_DELAY_ACCT) += delayacct.o
+obj-$(CONFIG_TASKSTATS) += taskstats.o tsacct.o
+obj-$(CONFIG_TRACEPOINTS) += tracepoint.o
+obj-$(CONFIG_LATENCYTOP) += latencytop.o
+obj-$(CONFIG_BINFMT_ELF) += elfcore.o
+obj-$(CONFIG_COMPAT_BINFMT_ELF) += elfcore.o
+obj-$(CONFIG_BINFMT_ELF_FDPIC) += elfcore.o
+obj-$(CONFIG_FUNCTION_TRACER) += trace/
+obj-$(CONFIG_TRACING) += trace/
+obj-$(CONFIG_TRACE_CLOCK) += trace/
+obj-$(CONFIG_RING_BUFFER) += trace/
+obj-$(CONFIG_TRACEPOINTS) += trace/
+obj-$(CONFIG_IRQ_WORK) += irq_work.o
+obj-$(CONFIG_CPU_PM) += cpu_pm.o
+
+obj-$(CONFIG_PERF_EVENTS) += events/
+
+obj-$(CONFIG_USER_RETURN_NOTIFIER) += user-return-notifier.o
+obj-$(CONFIG_PADATA) += padata.o
+obj-$(CONFIG_CRASH_DUMP) += crash_dump.o
+obj-$(CONFIG_JUMP_LABEL) += jump_label.o
+obj-$(CONFIG_CONTEXT_TRACKING) += context_tracking.o
+
+$(obj)/configs.o: $(obj)/config_data.h
+
+$(obj)/modsign_uefi.o: KBUILD_CFLAGS += -fshort-wchar
+
+# config_data.h contains the same information as ikconfig.h but gzipped.
+# Info from config_data can be extracted from /proc/config*
+targets += config_data.gz
+$(obj)/config_data.gz: $(KCONFIG_CONFIG) FORCE
+	$(call if_changed,gzip)
+
+      filechk_ikconfiggz = (echo "static const char kernel_config_data[] __used = MAGIC_START"; cat $< | scripts/basic/bin2c; echo "MAGIC_END;")
+targets += config_data.h
+$(obj)/config_data.h: $(obj)/config_data.gz FORCE
+	$(call filechk,ikconfiggz)
+
+$(obj)/time.o: $(obj)/timeconst.h
+
+quiet_cmd_hzfile = HZFILE  $@
+      cmd_hzfile = echo "hz=$(CONFIG_HZ)" > $@
+
+targets += hz.bc
+$(obj)/hz.bc: $(objtree)/include/config/hz.h FORCE
+	$(call if_changed,hzfile)
+
+quiet_cmd_bc  = BC      $@
+      cmd_bc  = bc -q $(filter-out FORCE,$^) > $@
+
+targets += timeconst.h
+$(obj)/timeconst.h: $(obj)/hz.bc $(src)/timeconst.bc FORCE
+	$(call if_changed,bc)
+
+###############################################################################
+#
+# Roll all the X.509 certificates that we can find together and pull them into
+# the kernel so that they get loaded into the system trusted keyring during
+# boot.
+#
+# We look in the source root and the build root for all files whose name ends
+# in ".x509".  Unfortunately, this will generate duplicate filenames, so we
+# have make canonicalise the pathnames and then sort them to discard the
+# duplicates.
+#
+###############################################################################
+ifeq ($(CONFIG_SYSTEM_TRUSTED_KEYRING),y)
+X509_CERTIFICATES-y := $(wildcard *.x509) $(wildcard $(srctree)/*.x509)
+X509_CERTIFICATES-$(CONFIG_MODULE_SIG) += signing_key.x509
+X509_CERTIFICATES := $(sort $(foreach CERT,$(X509_CERTIFICATES-y), \
+				$(or $(realpath $(CERT)),$(CERT))))
+
+ifeq ($(X509_CERTIFICATES),)
+$(warning *** No X.509 certificates found ***)
+endif
+
+ifneq ($(wildcard $(obj)/.x509.list),)
+ifneq ($(shell cat $(obj)/.x509.list),$(X509_CERTIFICATES))
+$(info X.509 certificate list changed)
+$(shell rm $(obj)/.x509.list)
+endif
+endif
+
+kernel/system_certificates.o: $(obj)/x509_certificate_list
+
+quiet_cmd_x509certs  = CERTS   $@
+      cmd_x509certs  = cat $(X509_CERTIFICATES) /dev/null >$@ $(foreach X509,$(X509_CERTIFICATES),; echo "  - Including cert $(X509)")
+
+targets += $(obj)/x509_certificate_list
+$(obj)/x509_certificate_list: $(X509_CERTIFICATES) $(obj)/.x509.list
+	$(call if_changed,x509certs)
+
+targets += $(obj)/.x509.list
+$(obj)/.x509.list:
+	@echo $(X509_CERTIFICATES) >$@
+
+clean-files := x509_certificate_list .x509.list
+endif
+
+ifeq ($(CONFIG_MODULE_SIG),y)
+###############################################################################
+#
+# If module signing is requested, say by allyesconfig, but a key has not been
+# supplied, then one will need to be generated to make sure the build does not
+# fail and that the kernel may be used afterwards.
+#
+###############################################################################
+ifndef CONFIG_MODULE_SIG_HASH
+$(error Could not determine digest type to use from kernel config)
+endif
+
+signing_key.priv signing_key.x509: x509.genkey
+	@echo "###"
+	@echo "### Now generating an X.509 key pair to be used for signing modules."
+	@echo "###"
+	@echo "### If this takes a long time, you might wish to run rngd in the"
+	@echo "### background to keep the supply of entropy topped up.  It"
+	@echo "### needs to be run as root, and uses a hardware random"
+	@echo "### number generator if one is available."
+	@echo "###"
+	openssl req -new -nodes -utf8 -$(CONFIG_MODULE_SIG_HASH) -days 36500 \
+		-batch -x509 -config x509.genkey \
+		-outform DER -out signing_key.x509 \
+		-keyout signing_key.priv 2>&1
+	@echo "###"
+	@echo "### Key pair generated."
+	@echo "###"
+
+x509.genkey:
+	@echo Generating X.509 key generation config
+	@echo  >x509.genkey "[ req ]"
+	@echo >>x509.genkey "default_bits = 4096"
+	@echo >>x509.genkey "distinguished_name = req_distinguished_name"
+	@echo >>x509.genkey "prompt = no"
+	@echo >>x509.genkey "string_mask = utf8only"
+	@echo >>x509.genkey "x509_extensions = myexts"
+	@echo >>x509.genkey
+	@echo >>x509.genkey "[ req_distinguished_name ]"
+	@echo >>x509.genkey "O = Magrathea"
+	@echo >>x509.genkey "CN = Glacier signing key"
+	@echo >>x509.genkey "emailAddress = slartibartfast@magrathea.h2g2"
+	@echo >>x509.genkey
+	@echo >>x509.genkey "[ myexts ]"
+	@echo >>x509.genkey "basicConstraints=critical,CA:FALSE"
+	@echo >>x509.genkey "keyUsage=digitalSignature"
+	@echo >>x509.genkey "subjectKeyIdentifier=hash"
+	@echo >>x509.genkey "authorityKeyIdentifier=keyid"
+endif
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/ipv4/af_inet.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv4/af_inet.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/ipv4/af_inet.c	2016-12-13 21:54:56.309000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv4/af_inet.c	2016-12-13 21:55:12.025000000 +0800
@@ -896,7 +896,7 @@
 }
 #endif
 
-const struct proto_ops inet_stream_ops = {
+struct proto_ops inet_stream_ops = {
 	.family		   = PF_INET,
 	.owner		   = THIS_MODULE,
 	.release	   = inet_release,
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/ipv4/tcp_ipv4.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv4/tcp_ipv4.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/ipv4/tcp_ipv4.c	2016-12-13 21:54:56.316000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv4/tcp_ipv4.c	2016-12-13 21:55:12.026000000 +0800
@@ -1793,7 +1793,7 @@
 }
 EXPORT_SYMBOL(inet_sk_rx_dst_set);
 
-const struct inet_connection_sock_af_ops ipv4_specific = {
+struct inet_connection_sock_af_ops ipv4_specific = {
 	.queue_xmit	   = ip_queue_xmit,
 	.send_check	   = tcp_v4_send_check,
 	.rebuild_header	   = inet_sk_rebuild_header,
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/ipv6/af_inet6.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv6/af_inet6.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/ipv6/af_inet6.c	2016-12-13 21:54:56.366000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv6/af_inet6.c	2016-12-13 21:55:12.026000000 +0800
@@ -504,7 +504,7 @@
 }
 EXPORT_SYMBOL(inet6_ioctl);
 
-const struct proto_ops inet6_stream_ops = {
+struct proto_ops inet6_stream_ops = {
 	.family		   = PF_INET6,
 	.owner		   = THIS_MODULE,
 	.release	   = inet6_release,
@@ -530,6 +530,8 @@
 #endif
 };
 
+EXPORT_SYMBOL(inet6_stream_ops);
+
 const struct proto_ops inet6_dgram_ops = {
 	.family		   = PF_INET6,
 	.owner		   = THIS_MODULE,
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/ipv6/tcp_ipv6.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv6/tcp_ipv6.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/ipv6/tcp_ipv6.c	2016-12-13 21:54:56.361000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/ipv6/tcp_ipv6.c	2016-12-13 21:55:12.027000000 +0800
@@ -76,8 +76,8 @@
 
 static int	tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
 
-static const struct inet_connection_sock_af_ops ipv6_mapped;
-static const struct inet_connection_sock_af_ops ipv6_specific;
+struct inet_connection_sock_af_ops ipv6_mapped;
+struct inet_connection_sock_af_ops ipv6_specific;
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_specific;
 static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;
@@ -1037,7 +1037,7 @@
 	return 0; /* don't send reset */
 }
 
-static struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 					 struct request_sock *req,
 					 struct dst_entry *dst)
 {
@@ -1232,6 +1232,7 @@
 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
 	return NULL;
 }
+EXPORT_SYMBOL(tcp_v6_syn_recv_sock);
 
 /* The socket must have it's spinlock held when we get
  * here.
@@ -1567,7 +1568,7 @@
 	.twsk_destructor = tcp_twsk_destructor,
 };
 
-static const struct inet_connection_sock_af_ops ipv6_specific = {
+struct inet_connection_sock_af_ops ipv6_specific = {
 	.queue_xmit	   = inet6_csk_xmit,
 	.send_check	   = tcp_v6_send_check,
 	.rebuild_header	   = inet6_sk_rebuild_header,
@@ -1587,6 +1588,7 @@
 #endif
 	.mtu_reduced	   = tcp_v6_mtu_reduced,
 };
+EXPORT_SYMBOL(ipv6_specific);
 
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {
@@ -1599,7 +1601,7 @@
 /*
  *	TCP over IPv4 via INET6 API
  */
-static const struct inet_connection_sock_af_ops ipv6_mapped = {
+struct inet_connection_sock_af_ops ipv6_mapped = {
 	.queue_xmit	   = ip_queue_xmit,
 	.send_check	   = tcp_v4_send_check,
 	.rebuild_header	   = inet_sk_rebuild_header,
@@ -1618,6 +1620,7 @@
 #endif
 	.mtu_reduced	   = tcp_v4_mtu_reduced,
 };
+EXPORT_SYMBOL(ipv6_mapped);
 
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/Kconfig linux-3.10.0-327.36.3.el7.toa.x86_64/net/Kconfig
--- linux-3.10.0-327.36.3.el7.x86_64/net/Kconfig	2016-12-13 21:54:56.300000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/Kconfig	2016-12-13 21:55:12.028000000 +0800
@@ -49,6 +49,7 @@
 source "net/unix/Kconfig"
 source "net/xfrm/Kconfig"
 source "net/iucv/Kconfig"
+source "net/toa/Kconfig"
 
 config INET
 	bool "TCP/IP networking"
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/Makefile linux-3.10.0-327.36.3.el7.toa.x86_64/net/Makefile
--- linux-3.10.0-327.36.3.el7.x86_64/net/Makefile	2016-12-13 21:54:56.309000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/Makefile	2016-12-13 21:55:12.028000000 +0800
@@ -62,6 +62,7 @@
 obj-$(CONFIG_SYSCTL)		+= sysctl_net.o
 endif
 obj-$(CONFIG_WIMAX)		+= wimax/
+obj-$(CONFIG_TOA)		+= toa/
 obj-$(CONFIG_DNS_RESOLVER)	+= dns_resolver/
 obj-$(CONFIG_CEPH_LIB)		+= ceph/
 obj-$(CONFIG_BATMAN_ADV)	+= batman-adv/
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/Makefile.orig linux-3.10.0-327.36.3.el7.toa.x86_64/net/Makefile.orig
--- linux-3.10.0-327.36.3.el7.x86_64/net/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/Makefile.orig	2016-12-13 21:55:12.029000000 +0800
@@ -0,0 +1,71 @@
+#
+# Makefile for the linux networking.
+#
+# 2 Sep 2000, Christoph Hellwig <hch@infradead.org>
+# Rewritten to use lists instead of if-statements.
+#
+
+obj-$(CONFIG_NET)		:= socket.o core/
+
+tmp-$(CONFIG_COMPAT) 		:= compat.o
+obj-$(CONFIG_NET)		+= $(tmp-y)
+
+# LLC has to be linked before the files in net/802/
+obj-$(CONFIG_LLC)		+= llc/
+obj-$(CONFIG_NET)		+= ethernet/ 802/ sched/ netlink/
+obj-$(CONFIG_NETFILTER)		+= netfilter/
+obj-$(CONFIG_INET)		+= ipv4/
+obj-$(CONFIG_XFRM)		+= xfrm/
+obj-$(CONFIG_UNIX)		+= unix/
+obj-$(CONFIG_NET)		+= ipv6/
+obj-$(CONFIG_PACKET)		+= packet/
+obj-$(CONFIG_NET_KEY)		+= key/
+obj-$(CONFIG_BRIDGE)		+= bridge/
+obj-$(CONFIG_NET_DSA)		+= dsa/
+obj-$(CONFIG_IPX)		+= ipx/
+obj-$(CONFIG_ATALK)		+= appletalk/
+obj-$(CONFIG_X25)		+= x25/
+obj-$(CONFIG_LAPB)		+= lapb/
+obj-$(CONFIG_NETROM)		+= netrom/
+obj-$(CONFIG_ROSE)		+= rose/
+obj-$(CONFIG_AX25)		+= ax25/
+obj-$(CONFIG_CAN)		+= can/
+obj-$(CONFIG_IRDA)		+= irda/
+obj-$(CONFIG_BT)		+= bluetooth/
+obj-$(CONFIG_SUNRPC)		+= sunrpc/
+obj-$(CONFIG_AF_RXRPC)		+= rxrpc/
+obj-$(CONFIG_ATM)		+= atm/
+obj-$(CONFIG_L2TP)		+= l2tp/
+obj-$(CONFIG_DECNET)		+= decnet/
+obj-$(CONFIG_PHONET)		+= phonet/
+ifneq ($(CONFIG_VLAN_8021Q),)
+obj-y				+= 8021q/
+endif
+obj-$(CONFIG_IP_DCCP)		+= dccp/
+obj-$(CONFIG_IP_SCTP)		+= sctp/
+obj-$(CONFIG_RDS)		+= rds/
+obj-$(CONFIG_WIRELESS)		+= wireless/
+obj-$(CONFIG_MAC80211)		+= mac80211/
+obj-$(CONFIG_TIPC)		+= tipc/
+obj-$(CONFIG_NETLABEL)		+= netlabel/
+obj-$(CONFIG_IUCV)		+= iucv/
+obj-$(CONFIG_RFKILL)		+= rfkill/
+obj-$(CONFIG_NET_9P)		+= 9p/
+obj-$(CONFIG_CAIF)		+= caif/
+ifneq ($(CONFIG_DCB),)
+obj-y				+= dcb/
+endif
+obj-$(CONFIG_IEEE802154)	+= ieee802154/
+obj-$(CONFIG_MAC802154)		+= mac802154/
+
+ifeq ($(CONFIG_NET),y)
+obj-$(CONFIG_SYSCTL)		+= sysctl_net.o
+endif
+obj-$(CONFIG_WIMAX)		+= wimax/
+obj-$(CONFIG_DNS_RESOLVER)	+= dns_resolver/
+obj-$(CONFIG_CEPH_LIB)		+= ceph/
+obj-$(CONFIG_BATMAN_ADV)	+= batman-adv/
+obj-$(CONFIG_NFC)		+= nfc/
+obj-$(CONFIG_OPENVSWITCH)	+= openvswitch/
+obj-$(CONFIG_VSOCKETS)	+= vmw_vsock/
+obj-$(CONFIG_NET_MPLS_GSO)	+= mpls/
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/netfilter/ipvs/ip_vs_conn.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/netfilter/ipvs/ip_vs_conn.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/netfilter/ipvs/ip_vs_conn.c	2016-12-13 21:54:56.338000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/netfilter/ipvs/ip_vs_conn.c	2016-12-13 21:55:12.029000000 +0800
@@ -73,7 +73,7 @@
 /*
  *  Fine locking granularity for big connection hash table
  */
-#define CT_LOCKARRAY_BITS  5
+#define CT_LOCKARRAY_BITS  10
 #define CT_LOCKARRAY_SIZE  (1<<CT_LOCKARRAY_BITS)
 #define CT_LOCKARRAY_MASK  (CT_LOCKARRAY_SIZE-1)
 
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/toa/Kconfig linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/Kconfig
--- linux-3.10.0-327.36.3.el7.x86_64/net/toa/Kconfig	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/Kconfig	2016-12-13 21:55:12.030000000 +0800
@@ -0,0 +1,9 @@
+config	TOA
+	tristate "The private TCP option for support Taobao LVS full-NAT feature"
+	default m
+	depends on HOOKERS
+	---help---
+	  This option saves the original IP address and source port of a TCP segment
+	  after LVS performed NAT on it. So far, this module supports IPv4 and IPv6.
+
+	  Say m if unsure.
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/toa/Makefile linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/Makefile
--- linux-3.10.0-327.36.3.el7.x86_64/net/toa/Makefile	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/Makefile	2016-12-13 21:55:12.030000000 +0800
@@ -0,0 +1,4 @@
+#
+# Makefile for TOA module.
+#
+obj-$(CONFIG_TOA) += toa.o
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/toa/toa.c linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/toa.c
--- linux-3.10.0-327.36.3.el7.x86_64/net/toa/toa.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/toa.c	2016-12-13 21:55:12.031000000 +0800
@@ -0,0 +1,393 @@
+#include "toa.h"
+
+/*
+ *	TOA: Address is a new TCP Option
+ *	Address include ip+port, Now support IPv4/IPv6
+ */
+
+
+/*
+ * Statistics of toa in proc /proc/net/toa_stats
+ */
+
+struct toa_stats_entry toa_stats[] = {
+	TOA_STAT_ITEM("syn_recv_sock_toa", SYN_RECV_SOCK_TOA_CNT),
+	TOA_STAT_ITEM("syn_recv_sock_no_toa", SYN_RECV_SOCK_NO_TOA_CNT),
+	TOA_STAT_ITEM("getname_toa_ok_v4", GETNAME_TOA_OK_CNT_V4),
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	TOA_STAT_ITEM("getname_toa_ok_v6", GETNAME_TOA_OK_CNT_V6),
+	TOA_STAT_ITEM("getname_toa_ok_mapped", GETNAME_TOA_OK_CNT_MAPPED),
+#endif
+	TOA_STAT_ITEM("getname_toa_mismatch", GETNAME_TOA_MISMATCH_CNT),
+	TOA_STAT_ITEM("getname_toa_bypass", GETNAME_TOA_BYPASS_CNT),
+	TOA_STAT_ITEM("getname_toa_empty", GETNAME_TOA_EMPTY_CNT),
+	TOA_STAT_END
+};
+
+struct toa_stat_mib *ext_stats;
+/*
+ * Funcs for toa hooks
+ */
+
+/* Parse TCP options in skb, try to get client ip, port
+ * @param skb [in] received skb, it should be a ack/get-ack packet.
+ * @return NULL if we don't get client ip/port;
+ *         value of toa_data in ret_ptr if we get client ip/port.
+ */
+static int get_toa_data(struct sk_buff *skb, void *sk_toa_data)
+{
+	struct tcphdr *th;
+	int length;
+	unsigned char *ptr;
+
+	struct toa_data *tdata;
+
+	TOA_DBG("get_toa_data called\n");
+
+	if (NULL != skb) {
+		th = tcp_hdr(skb);
+		length = (th->doff * 4) - sizeof(struct tcphdr);
+		ptr = (unsigned char *) (th + 1);
+
+		while (length > 0) {
+			int opcode = *ptr++;
+			int opsize;
+			switch (opcode) {
+			case TCPOPT_EOL:
+				return 0;
+			case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+				length--;
+				continue;
+			default:
+				opsize = *ptr++;
+				if (opsize < 2)	/* "silly options" */
+					return 0;
+				if (opsize > length)
+					/* don't parse partial options */
+					return 0;
+				if ((TCPOPT_TOA == opcode &&
+						TCPOLEN_TOA == opsize)) {
+					memset(sk_toa_data, 0,
+							sizeof(struct toa_data));
+					memcpy(sk_toa_data, ptr - 2,
+							TCPOLEN_TOA);
+					tdata = (struct toa_data*)sk_toa_data;
+					TOA_DBG("find toa data: ip = " \
+						"%u.%u.%u.%u, port = %u\n",
+						NIPQUAD(tdata->ip),
+						ntohs(tdata->port));
+					return 1;
+				}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+				else if (TCPOPT_TOA_V6 == opcode &&
+						TCPOLEN_TOA_V6 == opsize){
+					memset(sk_toa_data, 0,
+							sizeof(struct toa_data));
+					memcpy(sk_toa_data, ptr - 2,
+							TCPOLEN_TOA_V6);
+					tdata = (struct toa_data*)sk_toa_data;
+					TOA_DBG("find toa data: ipv6 = " \
+						"%pI6, port = %u\n",
+						&tdata->in6,
+						ntohs(tdata->port));
+					return 1;
+				}
+#endif
+				ptr += opsize - 2;
+				length -= opsize;
+			}
+		}
+	}
+	return 0;
+}
+
+/* get client ip from socket
+ * @param sock [in] the socket to getpeername() or getsockname()
+ * @param uaddr [out] the place to put client ip, port
+ * @param uaddr_len [out] lenth of @uaddr
+ * @peer [in] if(peer), try to get remote address; if(!peer),
+ *  try to get local address
+ * @return return what the original inet_getname() returns.
+ */
+static int
+inet_getname_toa(struct socket *sock, struct sockaddr *uaddr,
+		int *uaddr_len, int peer, int *p_retval)
+{
+	int retval = *p_retval;
+	struct sock *sk = sock->sk;
+	struct sockaddr_in *sin = (struct sockaddr_in *) uaddr;
+	struct toa_data tdata;
+
+	TOA_DBG("inet_getname_toa called\n");
+
+	/* set our value if need */
+	if (retval == 0 && peer) {
+		memcpy(&tdata, sk->sk_toa_data, sizeof(tdata));
+		if (TCPOPT_TOA == tdata.opcode &&
+		    TCPOLEN_TOA == tdata.opsize) {
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_V4);
+			TOA_DBG("inet_getname_toa: set new sockaddr, " \
+				"ip %u.%u.%u.%u -> %u.%u.%u.%u, port "
+				"%u -> %u\n",
+				NIPQUAD(sin->sin_addr.s_addr),
+				NIPQUAD(tdata.ip), ntohs(sin->sin_port),
+				ntohs(tdata.port));
+				sin->sin_port = tdata.port;
+				sin->sin_addr.s_addr = tdata.ip;
+		} else { /* doesn't belong to us */
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_MISMATCH_CNT);
+			TOA_DBG("inet_getname_toa: invalid toa data, " \
+				"ip %u.%u.%u.%u port %u opcode %u "
+				"opsize %u\n",
+				NIPQUAD(tdata.ip), ntohs(tdata.port),
+				tdata.opcode, tdata.opsize);
+		}
+	} else { /* no need to get client ip */
+		TOA_INC_STATS(ext_stats, GETNAME_TOA_EMPTY_CNT);
+	}
+
+	return retval;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static int
+inet6_getname_toa(struct socket *sock, struct sockaddr *uaddr,
+		  int *uaddr_len, int peer, int *p_retval)
+{
+	int retval = *p_retval;
+	struct sock *sk = sock->sk;
+	struct sockaddr_in6 *sin = (struct sockaddr_in6 *) uaddr;
+	struct toa_data tdata;
+
+	TOA_DBG("inet6_getname_toa called\n");
+
+	/* set our value if need */
+	if (retval == 0 && peer) {
+		memcpy(&tdata, sk->sk_toa_data, sizeof(tdata));
+		if (TCPOPT_TOA_V6 == tdata.opcode &&
+				TCPOLEN_TOA_V6 == tdata.opsize){
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_V6);
+			sin->sin6_port = tdata.port;
+			sin->sin6_addr = tdata.in6;
+			TOA_DBG("inet6_getname_toa: ipv6 = " \
+						"%pI6, port = %u\n",
+						&sin->sin6_addr,
+						ntohs(sin->sin6_port));
+		}else if (TCPOPT_TOA == tdata.opcode &&
+				TCPOLEN_TOA == tdata.opsize) {
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_MAPPED);
+			sin->sin6_port = tdata.port;
+			ipv6_addr_set(&sin->sin6_addr, 0, 0,
+					htonl(0x0000FFFF), tdata.ip);
+			TOA_DBG("inet6_getname_toa: ipv6_mapped = " \
+						"%pI6, port = %u\n",
+						&sin->sin6_addr,
+						ntohs(sin->sin6_port));
+		} else { /* doesn't belong to us */
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_MISMATCH_CNT);
+		}
+	} else { /* no need to get client ip */
+		TOA_INC_STATS(ext_stats, GETNAME_TOA_EMPTY_CNT);
+	}
+
+	return retval;
+}
+#endif
+
+/* The three way handshake has completed - we got a valid synack -
+ * now create the new socket.
+ * We need to save toa data into the new socket.
+ * @param sk [out]  the socket
+ * @param skb [in] the ack/ack-get packet
+ * @param req [in] the open request for this connection
+ * @param dst [out] route cache entry
+ * @return NULL if fail new socket if succeed.
+ */
+static struct sock *
+tcp_v4_syn_recv_sock_toa(struct sock *sk, struct sk_buff *skb,
+			struct request_sock *req, struct dst_entry *dst,
+						struct sock **p_newsock)
+{
+	struct sock *newsock = *p_newsock;
+
+	TOA_DBG("tcp_v4_syn_recv_sock_toa called\n");
+
+	if (!sk || !skb)
+		return NULL;
+
+	/* set our value if need */
+	if (NULL != newsock) {
+		if (get_toa_data(skb, newsock->sk_toa_data))
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_TOA_CNT);
+		else
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_NO_TOA_CNT);
+		TOA_DBG("tcp_v4_syn_recv_sock_toa: set " \
+			"sk->sk_toa_data\n");
+	}
+	return newsock;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static struct sock *
+tcp_v6_syn_recv_sock_toa(struct sock *sk, struct sk_buff *skb,
+			 struct request_sock *req, struct dst_entry *dst,
+					struct sock **p_newsock)
+{
+	struct sock *newsock = *p_newsock;
+
+	TOA_DBG("tcp_v6_syn_recv_sock_toa called\n");
+
+	if (!sk || !skb)
+		return NULL;
+
+	/* set our value if need */
+	if (NULL != newsock) {
+		if (get_toa_data(skb, newsock->sk_toa_data))
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_TOA_CNT);
+		else
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_NO_TOA_CNT);
+	}
+	return newsock;
+}
+#endif
+
+/*
+ * HOOK FUNCS
+ */
+
+static struct hooker inet_getname_hooker = {
+	.func = inet_getname_toa,
+};
+
+static struct hooker inet_tcp_hooker = {
+	.func = tcp_v4_syn_recv_sock_toa,
+};
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static struct hooker inet6_getname_hooker = {
+	.func = inet6_getname_toa,
+};
+
+static struct hooker inet6_tcp_hooker = {
+	.func = tcp_v6_syn_recv_sock_toa,
+};
+#endif
+
+/* replace the functions with our functions */
+static inline int
+hook_toa_functions(void)
+{
+	int ret;
+
+	ret = hooker_install(&inet_stream_ops.getname, &inet_getname_hooker);
+	ret |= hooker_install(&ipv4_specific.syn_recv_sock, &inet_tcp_hooker);
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	ret |= hooker_install(&inet6_stream_ops.getname, &inet6_getname_hooker);
+	ret |= hooker_install(&ipv6_specific.syn_recv_sock, &inet6_tcp_hooker);
+#endif
+	return ret;
+}
+
+/* replace the functions to original ones */
+static void
+unhook_toa_functions(void)
+{
+	hooker_uninstall(&inet_getname_hooker);
+	hooker_uninstall(&inet_tcp_hooker);
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	hooker_uninstall(&inet6_getname_hooker);
+	hooker_uninstall(&inet6_tcp_hooker);
+#endif
+}
+
+/*
+ * Statistics of toa in proc /proc/net/toa_stats
+ */
+static int toa_stats_show(struct seq_file *seq, void *v)
+{
+	int i, j, cpu_nr;
+
+	/* print CPU first */
+	seq_printf(seq, "                                  ");
+	cpu_nr = num_possible_cpus();
+	for (i = 0; i < cpu_nr; i++)
+		if (cpu_online(i))
+			seq_printf(seq, "CPU%d       ", i);
+	seq_putc(seq, '\n');
+
+	i = 0;
+	while (NULL != toa_stats[i].name) {
+		seq_printf(seq, "%-25s:", toa_stats[i].name);
+		for (j = 0; j < cpu_nr; j++) {
+			if (cpu_online(j)) {
+				seq_printf(seq, "%10lu ", *(
+					((unsigned long *) per_cpu_ptr(
+					ext_stats, j)) + toa_stats[i].entry
+					));
+			}
+		}
+		seq_putc(seq, '\n');
+		i++;
+	}
+	return 0;
+}
+
+static int toa_stats_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, toa_stats_show, NULL);
+}
+
+static const struct file_operations toa_stats_fops = {
+	.owner = THIS_MODULE,
+	.open = toa_stats_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * TOA module init and destory
+ */
+
+/* module init */
+static int __init
+toa_init(void)
+{
+	/* alloc statistics array for toa */
+	ext_stats = alloc_percpu(struct toa_stat_mib);
+	if (!ext_stats)
+		return -ENOMEM;
+
+	if (!proc_net_fops_create(&init_net, "toa_stats", 0, &toa_stats_fops)) {
+		TOA_INFO("cannot create procfs /proc/net/toa_stats.\n");
+		goto err_percpu;
+	}
+
+	/* hook funcs for parse and get toa */
+	if (hook_toa_functions())
+		goto err_proc;
+
+	return 0;
+
+err_proc:
+	proc_net_remove(&init_net, "toa_stats");
+err_percpu:
+	free_percpu(ext_stats);
+	return -ENODEV;
+}
+
+/* module cleanup*/
+static void __exit
+toa_exit(void)
+{
+	unhook_toa_functions();
+
+	proc_net_remove(&init_net, "toa_stats");
+	free_percpu(ext_stats);
+}
+
+module_init(toa_init);
+module_exit(toa_exit);
+MODULE_LICENSE("GPL");
diff -uNr linux-3.10.0-327.36.3.el7.x86_64/net/toa/toa.h linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/toa.h
--- linux-3.10.0-327.36.3.el7.x86_64/net/toa/toa.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.0-327.36.3.el7.toa.x86_64/net/toa/toa.h	2016-12-13 21:55:12.031000000 +0800
@@ -0,0 +1,105 @@
+#ifndef __NET__TOA_H__
+#define __NET__TOA_H__
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/err.h>
+#include <linux/time.h>
+#include <linux/skbuff.h>
+#include <net/tcp.h>
+#include <net/inet_common.h>
+#include <linux/uaccess.h>
+#include <linux/netdevice.h>
+#include <net/net_namespace.h>
+#include <linux/fs.h>
+#include <linux/sysctl.h>
+#include <linux/proc_fs.h>
+#include <linux/kallsyms.h>
+
+#include <linux/hookers.h>
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#include <net/ipv6.h>
+#include <net/transp_v6.h>
+#endif
+
+#define TOA_VERSION "1.0.0.2"
+
+#ifdef TOA_DEBUG
+#define TOA_DBG(msg...)				\
+	do {					\
+		printk(KERN_DEBUG "[DEBUG] TOA: " msg); \
+	} while (0)
+#else
+#define TOA_DBG(msg...)
+#endif
+
+#define TOA_INFO(msg...)				\
+	do {						\
+		if (net_ratelimit())			\
+			printk(KERN_INFO "TOA: " msg);	\
+	} while (0)
+
+#define TCPOPT_TOA  254
+
+/* MUST be 4n !!!! */
+#define TCPOLEN_TOA 8		/* |opcode|size|ip+port| = 1 + 1 + 6 */
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#define TCPOPT_TOA_V6	253
+#define TCPOLEN_TOA_V6	20	/* |opcode|size|port|ipv6| = 1 + 1 + 2 + 16 */
+#endif
+
+/* MUST be 4 bytes alignment */
+struct toa_data {
+	__u8 opcode;
+	__u8 opsize;
+	__be16 port;
+	union {
+		__be32 ip;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		struct in6_addr in6;
+#endif
+	};
+};
+
+/* statistics about toa in proc /proc/net/toa_stat */
+enum {
+	SYN_RECV_SOCK_TOA_CNT = 1,
+	SYN_RECV_SOCK_NO_TOA_CNT,
+	GETNAME_TOA_OK_CNT_V4,
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	GETNAME_TOA_OK_CNT_V6,
+	GETNAME_TOA_OK_CNT_MAPPED,
+#endif
+	GETNAME_TOA_MISMATCH_CNT,
+	GETNAME_TOA_BYPASS_CNT,
+	GETNAME_TOA_EMPTY_CNT,
+	TOA_STAT_LAST
+};
+
+struct toa_stats_entry {
+	char *name;
+	int entry;
+};
+
+#define TOA_STAT_ITEM(_name, _entry) { \
+	.name = _name,		\
+	.entry = _entry,	\
+}
+
+#define TOA_STAT_END {	\
+	NULL,		\
+	0,		\
+}
+
+struct toa_stat_mib {
+	unsigned long mibs[TOA_STAT_LAST];
+};
+
+#define TOA_INC_STATS(mib, field)         \
+	(per_cpu_ptr(mib, smp_processor_id())->mibs[field]++)
+
+#endif