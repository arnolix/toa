diff -uNr linux-2.6.32-642.11.1.el6.x86_64/arch/Kconfig linux-2.6.32-642.11.1.el6.toa.x86_64/arch/Kconfig
--- linux-2.6.32-642.11.1.el6.x86_64/arch/Kconfig	2016-12-13 17:22:01.398074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/arch/Kconfig	2016-12-13 17:25:10.422084237 +0800
@@ -159,4 +159,11 @@
 config ARCH_HAVE_NMI_SAFE_CMPXCHG
 	bool
 
+config HOOKERS
+	tristate "Hooker service"
+	default m
+	help
+	  Allow replacing and restore the function pointer in any order.
+	  See include/linux/hookers.h for details.
+
 source "kernel/gcov/Kconfig"
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/arch/Kconfig.orig linux-2.6.32-642.11.1.el6.toa.x86_64/arch/Kconfig.orig
--- linux-2.6.32-642.11.1.el6.x86_64/arch/Kconfig.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/arch/Kconfig.orig	2016-12-13 17:23:55.898074270 +0800
@@ -0,0 +1,162 @@
+#
+# General architecture dependent options
+#
+
+config OPROFILE
+	tristate "OProfile system profiling (EXPERIMENTAL)"
+	depends on PROFILING
+	depends on HAVE_OPROFILE
+	select RING_BUFFER
+	select RING_BUFFER_ALLOW_SWAP
+	help
+	  OProfile is a profiling system capable of profiling the
+	  whole system, include the kernel, kernel modules, libraries,
+	  and applications.
+
+	  If unsure, say N.
+
+config OPROFILE_EVENT_MULTIPLEX
+	bool "OProfile multiplexing support (EXPERIMENTAL)"
+	default n
+	depends on OPROFILE && X86
+	help
+	  The number of hardware counters is limited. The multiplexing
+	  feature enables OProfile to gather more events than counters
+	  are provided by the hardware. This is realized by switching
+	  between events at an user specified time interval.
+
+	  If unsure, say N.
+
+config HAVE_OPROFILE
+	bool
+
+config KPROBES
+	bool "Kprobes"
+	depends on KALLSYMS && MODULES
+	depends on HAVE_KPROBES
+	help
+	  Kprobes allows you to trap at almost any kernel address and
+	  execute a callback function.  register_kprobe() establishes
+	  a probepoint and specifies the callback.  Kprobes is useful
+	  for kernel debugging, non-intrusive instrumentation and testing.
+	  If in doubt, say "N".
+
+config OPTPROBES
+	bool "Kprobes jump optimization support (EXPERIMENTAL)"
+	default y
+	depends on KPROBES
+	depends on !PREEMPT
+	depends on HAVE_OPTPROBES
+	select KALLSYMS_ALL
+	help
+	  This option will allow kprobes to optimize breakpoint to
+	  a jump for reducing its overhead.
+
+config HAVE_EFFICIENT_UNALIGNED_ACCESS
+	bool
+	help
+	  Some architectures are unable to perform unaligned accesses
+	  without the use of get_unaligned/put_unaligned. Others are
+	  unable to perform such accesses efficiently (e.g. trap on
+	  unaligned access and require fixing it up in the exception
+	  handler.)
+
+	  This symbol should be selected by an architecture if it can
+	  perform unaligned accesses efficiently to allow different
+	  code paths to be selected for these cases. Some network
+	  drivers, for example, could opt to not fix up alignment
+	  problems with received packets if doing so would not help
+	  much.
+
+	  See Documentation/unaligned-memory-access.txt for more
+	  information on the topic of unaligned memory accesses.
+
+config HAVE_SYSCALL_WRAPPERS
+	bool
+
+config KRETPROBES
+	def_bool y
+	depends on KPROBES && HAVE_KRETPROBES
+
+config USER_RETURN_NOTIFIER
+	bool
+	depends on HAVE_USER_RETURN_NOTIFIER
+	help
+	  Provide a kernel-internal notification when a cpu is about to
+	  switch to user mode.
+
+config HAVE_IOREMAP_PROT
+	bool
+
+config HAVE_KPROBES
+	bool
+
+config HAVE_KRETPROBES
+	bool
+
+config HAVE_OPTPROBES
+	bool
+#
+# An arch should select this if it provides all these things:
+#
+#	task_pt_regs()		in asm/processor.h or asm/ptrace.h
+#	arch_has_single_step()	if there is hardware single-step support
+#	arch_has_block_step()	if there is hardware block-step support
+#	asm/syscall.h		supplying asm-generic/syscall.h interface
+#	linux/regset.h		user_regset interfaces
+#	CORE_DUMP_USE_REGSET	#define'd in linux/elf.h
+#	TIF_SYSCALL_TRACE	calls tracehook_report_syscall_{entry,exit}
+#	TIF_NOTIFY_RESUME	calls tracehook_notify_resume()
+#	signal delivery		calls tracehook_signal_handler()
+#
+config HAVE_ARCH_TRACEHOOK
+	bool
+
+config HAVE_DMA_ATTRS
+	bool
+
+config USE_GENERIC_SMP_HELPERS
+	bool
+
+config HAVE_CLK
+	bool
+	help
+	  The <linux/clk.h> calls support software clock gating and
+	  thus are a key power management tool on many systems.
+
+config HAVE_DMA_API_DEBUG
+	bool
+
+config HAVE_DEFAULT_NO_SPIN_MUTEXES
+	bool
+
+config HAVE_USER_RETURN_NOTIFIER
+	bool
+
+config HAVE_PERF_EVENTS_NMI
+	bool
+	help
+	  System hardware can generate an NMI using the perf event
+	  subsystem.  Also has support for calculating CPU cycle events
+	  to determine how many clock cycles in a given period.
+
+config HAVE_PERF_REGS
+	bool
+	help
+	  Support selective register dumps for perf events. This includes
+	  bit-mapping of each registers and a unique architecture id.
+
+config HAVE_PERF_USER_STACK_DUMP
+	bool
+	help
+	  Support user stack dumps for perf event samples. This needs
+	  access to the user stack pointer which is not unified across
+	  architectures.
+
+config HAVE_ARCH_MUTEX_CPU_RELAX
+	bool
+
+config ARCH_HAVE_NMI_SAFE_CMPXCHG
+	bool
+
+source "kernel/gcov/Kconfig"
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_conf.c linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_conf.c
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_conf.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_conf.c	2016-12-13 17:25:21.066077451 +0800
@@ -0,0 +1,1738 @@
+/****************************************************************************
+ *  flashcache_conf.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+struct cache_c *cache_list_head = NULL;
+struct work_struct _kcached_wq;
+u_int64_t size_hist[33];
+
+struct kmem_cache *_job_cache;
+mempool_t *_job_pool;
+struct kmem_cache *_pending_job_cache;
+mempool_t *_pending_job_pool;
+
+atomic_t nr_cache_jobs;
+atomic_t nr_pending_jobs;
+
+extern struct list_head *_pending_jobs;
+extern struct list_head *_io_jobs;
+extern struct list_head *_md_io_jobs;
+extern struct list_head *_md_complete_jobs;
+
+struct flashcache_control_s {
+	unsigned long synch_flags;
+};
+
+struct flashcache_control_s *flashcache_control;
+
+/* Bit offsets for wait_on_bit_lock() */
+#define FLASHCACHE_UPDATE_LIST		0
+
+static int flashcache_notify_reboot(struct notifier_block *this,
+				    unsigned long code, void *x);
+static void flashcache_sync_for_remove(struct cache_c *dmc);
+
+extern char *flashcache_sw_version;
+
+static int
+flashcache_wait_schedule(void *unused)
+{
+	schedule();
+	return 0;
+}
+
+static int 
+flashcache_jobs_init(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL, NULL);
+#else
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL);
+#endif
+	if (!_job_cache)
+		return -ENOMEM;
+
+	_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+	                           mempool_free_slab, _job_cache);
+	if (!_job_pool) {
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL, NULL);
+#else
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL);
+#endif
+	if (!_pending_job_cache) {
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	_pending_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+					   mempool_free_slab, _pending_job_cache);
+	if (!_pending_job_pool) {
+		kmem_cache_destroy(_pending_job_cache);
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void 
+flashcache_jobs_exit(void)
+{
+	VERIFY(flashcache_pending_empty());
+	VERIFY(flashcache_io_empty());
+	VERIFY(flashcache_md_io_empty());
+	VERIFY(flashcache_md_complete_empty());
+
+	mempool_destroy(_job_pool);
+	kmem_cache_destroy(_job_cache);
+	_job_pool = NULL;
+	_job_cache = NULL;
+	mempool_destroy(_pending_job_pool);
+	kmem_cache_destroy(_pending_job_cache);
+	_pending_job_pool = NULL;
+	_pending_job_cache = NULL;
+}
+
+static int 
+flashcache_kcached_init(struct cache_c *dmc)
+{
+	init_waitqueue_head(&dmc->destroyq);
+	atomic_set(&dmc->nr_jobs, 0);
+	atomic_set(&dmc->remove_in_prog, 0);
+	return 0;
+}
+
+/*
+ * Write out the metadata one sector at a time.
+ * Then dump out the superblock.
+ */
+static int 
+flashcache_writeback_md_store(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	int num_valid = 0, num_dirty = 0;
+	int error;
+	int write_errors = 0;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_writeback_md_store: Unable to allocate memory");
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata !");
+		return 1;
+	}	
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			num_valid++;
+		if (dmc->cache[i].cache_state & DIRTY)
+			num_dirty++;
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state & 
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/* 
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * 
+					MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+				if (error) {
+					write_errors++;
+					DMERR("flashcache_writeback_md_store: Could not write out cache metadata block %lu error %d !",
+					      where.sector, error);
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+		if (error) {
+			write_errors++;
+				DMERR("flashcache_writeback_md_store: Could not write out cache metadata block %lu error %d !",
+				      where.sector, error);
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_writeback_md_store" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_writeback_md_store: sector mismatch\n");
+	}
+
+	vfree((void *)meta_data_cacheblock);
+
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_writeback_md_store: Unable to allocate memory");
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata !");
+		return 1;
+	}	
+	memset(header, 0, MD_BLOCK_BYTES(dmc));
+	
+	/* Write the header out last */
+	if (write_errors == 0) {
+		if (num_dirty == 0)
+			header->cache_sb_state = CACHE_MD_STATE_CLEAN;
+		else
+			header->cache_sb_state = CACHE_MD_STATE_FASTCLEAN;			
+	} else
+		header->cache_sb_state = CACHE_MD_STATE_UNSTABLE;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+	
+	DPRINTK("Store metadata to disk: block size(%u), md block size(%u), cache size(%llu)" \
+	        "associativity(%u)",
+	        header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		write_errors++;
+		DMERR("flashcache_writeback_md_store: Could not write out cache metadata superblock %lu error %d !",
+		      where.sector, error);
+	}
+
+	vfree((void *)header);
+
+	if (write_errors == 0)
+		DMINFO("Cache metadata saved to disk");
+	else {
+		DMINFO("CRITICAL : There were %d errors in saving cache metadata saved to disk", 
+		       write_errors);
+		if (num_dirty)
+			DMINFO("CRITICAL : You have likely lost %d dirty blocks", num_dirty);
+	}
+
+	DMINFO("flashcache_writeback_md_store: valid blocks = %d dirty blocks = %d md_sectors = %d\n", 
+	       num_valid, num_dirty, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+
+	return 0;
+}
+
+static int 
+flashcache_writethrough_create(struct cache_c *dmc)
+{
+	sector_t cache_size, dev_size;
+	sector_t order;
+	int i;
+	
+	/* 
+	 * Convert size (in sectors) to blocks.
+	 * Then round size (in blocks now) down to a multiple of associativity 
+	 */
+	dmc->size /= dmc->block_size;
+	dmc->size = (dmc->size / dmc->assoc) * dmc->assoc;
+
+	/* Check cache size against device size */
+	dev_size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	cache_size = dmc->size * dmc->block_size;
+	if (cache_size > dev_size) {
+		DMERR("Requested cache size exeeds the cache device's capacity" \
+		      "(%lu>%lu)",
+  		      cache_size, dev_size);
+		return 1;
+	}
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%luB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       cache_size >> (20-SECTOR_SHIFT), dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		DMERR("flashcache_writethrough_create: Unable to allocate cache md");
+		return 1;
+	}
+	/* Initialize the cache structs */
+	for (i = 0; i < dmc->size ; i++) {
+		dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		dmc->cache[i].checksum = 0;
+#endif
+		dmc->cache[i].cache_state = INVALID;
+		dmc->cache[i].nr_queued = 0;
+	}
+	dmc->md_blocks = 0;
+	return 0;
+}
+
+static int 
+flashcache_writeback_create(struct cache_c *dmc, int force)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j, error;
+	sector_t cache_size, dev_size;
+	sector_t order;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+	
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_writeback_create: Unable to allocate sector");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Could not read cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;
+	}
+	if (!force &&
+	    ((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Existing Cache Detected, use force to re-create");
+		return 1;
+	}
+	/* Compute the size of the metadata, including header. 
+	   Note dmc->size is in raw sectors */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size / dmc->block_size) + 1 + 1;
+	dmc->size -= dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc);	/* total sectors available for cache */
+	dmc->size /= dmc->block_size;
+	dmc->size = (dmc->size / dmc->assoc) * dmc->assoc;	
+	/* Recompute since dmc->size was possibly trunc'ed down */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_writeback_create: md_blocks = %d, md_sectors = %d\n", 
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+	dev_size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	cache_size = dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + (dmc->size * dmc->block_size);
+	if (cache_size > dev_size) {
+		DMERR("Requested cache size exceeds the cache device's capacity" \
+		      "(%lu>%lu)",
+  		      cache_size, dev_size);
+		vfree((void *)header);
+		return 1;
+	}
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%luB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       cache_size >> (20-SECTOR_SHIFT), dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_create: Unable to allocate cache md");
+		return 1;
+	}
+	/* Initialize the cache structs */
+	for (i = 0; i < dmc->size ; i++) {
+		dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		dmc->cache[i].checksum = 0;
+#endif
+		dmc->cache[i].cache_state = INVALID;
+		dmc->cache[i].nr_queued = 0;
+	}
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_writeback_create: Unable to allocate memory");
+		DMERR("flashcache_writeback_create: Could not write out cache metadata !");
+		return 1;
+	}	
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state & 
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/* 
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, 
+								 meta_data_cacheblock);
+				if (error) {
+					vfree((void *)header);
+					vfree((void *)meta_data_cacheblock);
+					vfree(dmc->cache);
+					DMERR("flashcache_writeback_create: Could not write cache metadata block %lu error %d !",
+					      where.sector, error);
+					return 1;
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next metadata block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+		if (error) {
+			vfree((void *)header);
+			vfree((void *)meta_data_cacheblock);
+			vfree(dmc->cache);
+			DMERR("flashcache_writeback_create: Could not write cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;		
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_writeback_create" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_writeback_create: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/* Write the header */
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	dmc->on_ssd_version = header->cache_version = FLASHCACHE_VERSION;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	
+	printk("flashcache-dbg: cachedev check - %s %s", header->cache_devname,
+				dmc->dm_vdevname);
+	
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_create: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;		
+	}
+	vfree((void *)header);
+	return 0;
+}
+
+static int 
+flashcache_writeback_load(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	u_int64_t size, slots_read;
+	int clean_shutdown;
+	int dirty_loaded = 0;
+	sector_t order, data_size;
+	int num_valid = 0;
+	int error;
+	int sectors_read = 0, sectors_expected = 0;	/* Debug */
+
+	/* 
+	 * We don't know what the preferred block size is, just read off 
+	 * the default md blocksize.
+	 */
+	header = (struct flash_superblock *)vmalloc(DEFAULT_MD_BLOCK_SIZE);
+	if (!header) {
+		DMERR("flashcache_writeback_load: Unable to allocate memory");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = DEFAULT_MD_BLOCK_SIZE;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Could not read cache superblock %lu error %d!",
+		      where.sector, error);
+		return 1;
+	}
+
+	if (header->cache_version == 1) {
+		/* Backwards compatibility, md was 512 bytes always in V1.0 */
+		header->md_block_size = 1;
+	} else if (header->cache_version > FLASHCACHE_VERSION) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Unknown version %d found in superblock!", header->cache_version);
+		return 1;
+	}
+	dmc->on_ssd_version = header->cache_version;
+		
+	DPRINTK("Loaded cache conf: version(%d), block size(%u), md block size(%u), cache size(%llu), " \
+	        "associativity(%u)",
+	        header->cache_version, header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+	if (!((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_writeback_load: Corrupt Cache Superblock");
+		return 1;
+	}
+	if (header->cache_sb_state == CACHE_MD_STATE_DIRTY) {
+		DMINFO("Unclean Shutdown Detected");
+		printk(KERN_ALERT "Only DIRTY blocks exist in cache");
+		clean_shutdown = 0;
+	} else if (header->cache_sb_state == CACHE_MD_STATE_CLEAN) {
+		DMINFO("Slow (clean) Shutdown Detected");
+		printk(KERN_ALERT "Only CLEAN blocks exist in cache");
+		clean_shutdown = 1;
+	} else {
+		DMINFO("Fast (clean) Shutdown Detected");
+		printk(KERN_ALERT "Both CLEAN and DIRTY blocks exist in cache");
+		clean_shutdown = 1;
+	}
+	dmc->block_size = header->block_size;
+	dmc->md_block_size = header->md_block_size;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+	dmc->size = header->size;
+	dmc->assoc = header->assoc;
+	dmc->assoc_shift = ffs(dmc->assoc) - 1;
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_writeback_load: md_blocks = %d, md_sectors = %d, md_block_size = %d\n", 
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc), dmc->md_block_size);
+	data_size = dmc->size * dmc->block_size;
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%ldB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       (dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + data_size) >> (20-SECTOR_SHIFT), 
+	       dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		DMERR("load_metadata: Unable to allocate memory");
+		vfree((void *)header);
+		return 1;
+	}
+	/* Read the metadata in large blocks and populate incore state */
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_load: Unable to allocate memory");
+		return 1;
+	}
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	size = dmc->size;
+	i = 0;
+	while (size > 0) {
+		slots_read = min(size, (u_int64_t)(MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)));
+		if (slots_read % MD_SLOTS_PER_BLOCK(dmc))
+			where.count = (1 + (slots_read / MD_SLOTS_PER_BLOCK(dmc))) * MD_SECTORS_PER_BLOCK(dmc);
+		else
+			where.count = (slots_read / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		sectors_read += where.count;	/* Debug */
+		error = flashcache_dm_io_sync_vm(dmc, &where, READ, meta_data_cacheblock);
+		if (error) {
+			vfree((void *)header);
+			vfree(dmc->cache);
+			vfree((void *)meta_data_cacheblock);
+			DMERR("flashcache_writeback_load: Could not read cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;
+		}
+		where.sector += where.count;
+		next_ptr = meta_data_cacheblock;
+		for (j = 0 ; j < slots_read ; j++) {
+			/*
+			 * XXX - Now that we force each on-ssd metadata cache slot to be a ^2, where
+			 * we are guaranteed that the slots will exactly fit within a sector (and 
+			 * a metadata block), we can simplify this logic. We don't need this next test.
+			 */
+			if ((j % MD_SLOTS_PER_BLOCK(dmc)) == 0) {
+				/* Move onto next block */
+				next_ptr = (struct flash_cacheblock *)
+					((caddr_t)meta_data_cacheblock + MD_BLOCK_BYTES(dmc) * (j / MD_SLOTS_PER_BLOCK(dmc)));
+			}
+			dmc->cache[i].nr_queued = 0;
+			/* 
+			 * If unclean shutdown, only the DIRTY blocks are loaded.
+			 */
+			if (clean_shutdown || (next_ptr->cache_state & DIRTY)) {
+				if (next_ptr->cache_state & DIRTY)
+					dirty_loaded++;
+				dmc->cache[i].cache_state = next_ptr->cache_state;
+				VERIFY((dmc->cache[i].cache_state & (VALID | INVALID)) 
+				       != (VALID | INVALID));
+				if (dmc->cache[i].cache_state & VALID)
+					num_valid++;
+				dmc->cache[i].dbn = next_ptr->dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				if (clean_shutdown)
+					dmc->cache[i].checksum = next_ptr->checksum;
+				else {
+					error = flashcache_read_compute_checksum(dmc, i, block);
+					if (error) {
+						vfree((void *)header);
+						vfree(dmc->cache);
+						vfree((void *)meta_data_cacheblock);
+						DMERR("flashcache_writeback_load: Could not read cache metadata block %lu error %d !",
+						      dmc->cache[i].dbn, error);
+						return 1;				
+					}						
+				}
+#endif
+			} else {
+				dmc->cache[i].cache_state = INVALID;
+				dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				dmc->cache[i].checksum = 0;
+#endif
+			}
+			next_ptr++;
+			i++;
+		}
+		size -= slots_read;
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_read) {
+		printk("flashcache_writeback_load" "Sector Mismatch ! sectors_expected=%d, sectors_read=%d\n",
+		       sectors_expected, sectors_read);
+		panic("flashcache_writeback_load: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/*
+	 * For writing the superblock out, use the preferred blocksize that 
+	 * we read from the superblock above.
+	 */
+	if (DEFAULT_MD_BLOCK_SIZE != dmc->md_block_size) {
+		vfree((void *)header);
+		header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+		if (!header) {
+			DMERR("flashcache_writeback_load: Unable to allocate memory");
+			return 1;
+		}
+	}	
+	/* Before we finish loading, we need to dirty the superblock and 
+	   write it out */
+	header->size = dmc->size;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->assoc = dmc->assoc;
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->dm_vdevname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_writeback_load: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;		
+	}
+	vfree((void *)header);
+	DMINFO("flashcache_writeback_load: Cache metadata loaded from disk with %d valid %d DIRTY blocks", 
+	       num_valid, dirty_loaded);
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+static void
+flashcache_clean_all_sets(void *data)
+{
+	struct cache_c *dmc = (struct cache_c *)data;
+#else
+static void
+flashcache_clean_all_sets(struct work_struct *work)
+{
+	struct cache_c *dmc = container_of(work, struct cache_c, 
+					   delayed_clean.work);
+#endif
+	int i;
+	
+	for (i = 0 ; i < dmc->num_sets ; i++)
+		flashcache_clean_set(dmc, i);
+}
+
+static int inline
+flashcache_get_dev(struct dm_target *ti, char *pth, struct dm_dev **dmd,
+		   char *dmc_dname, sector_t tilen)
+{
+	int rc;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34)
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else
+#if defined(RHEL_MAJOR) && RHEL_MAJOR == 6
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else 
+	rc = dm_get_device(ti, pth, 0, tilen,
+			   dm_table_get_mode(ti->table), dmd);
+#endif
+#endif
+	if (!rc)
+		strncpy(dmc_dname, pth, DEV_PATHLEN);
+	return rc;
+}
+
+/*
+ * Construct a cache mapping.
+ *  arg[0]: path to source device
+ *  arg[1]: path to cache device
+ *  arg[2]: md virtual device name
+ *  arg[3]: cache mode (from flashcache.h)
+ *  arg[4]: cache persistence (if set, cache conf is loaded from disk)
+ * Cache configuration parameters (if not set, default values are used.
+ *  arg[5]: cache block size (in sectors)
+ *  arg[6]: cache size (in blocks)
+ *  arg[7]: cache associativity
+ *  arg[8]: md block size (in sectors)
+ */
+int 
+flashcache_ctr(struct dm_target *ti, unsigned int argc, char **argv)
+{
+	struct cache_c *dmc;
+	sector_t i, order;
+	int r = -EINVAL;
+	int persistence = 0;
+	
+	if (argc < 3) {
+		ti->error = "flashcache: Need at least 3 arguments";
+		goto bad;
+	}
+
+	dmc = kzalloc(sizeof(*dmc), GFP_KERNEL);
+	if (dmc == NULL) {
+		ti->error = "flashcache: Failed to allocate cache context";
+		r = ENOMEM;
+		goto bad;
+	}
+
+	dmc->tgt = ti;
+	if ((r = flashcache_get_dev(ti, argv[0], &dmc->disk_dev, 
+				    dmc->disk_devname, ti->len))) {
+		if (r == -EBUSY)
+			ti->error = "flashcache: Disk device is busy, cannot create cache";
+		else
+			ti->error = "flashcache: Disk device lookup failed";
+		goto bad1;
+	}
+	if ((r = flashcache_get_dev(ti, argv[1], &dmc->cache_dev,
+				    dmc->cache_devname, 0))) {
+		if (r == -EBUSY)
+			ti->error = "flashcache: Cache device is busy, cannot create cache";
+		else
+			ti->error = "flashcache: Cache device lookup failed";
+		goto bad2;
+	}
+
+	if (sscanf(argv[2], "%s", (char *)&dmc->dm_vdevname) != 1) {
+		ti->error = "flashcache: Virtual device name lookup failed";
+		goto bad3;
+	}
+	
+	r = flashcache_kcached_init(dmc);
+	if (r) {
+		ti->error = "Failed to initialize kcached";
+		goto bad3;
+	}
+
+	if (sscanf(argv[3], "%u", &dmc->cache_mode) != 1) {
+		ti->error = "flashcache: sscanf failed, invalid cache mode";
+		r = -EINVAL;
+		goto bad3;
+	}
+	if (dmc->cache_mode < FLASHCACHE_WRITE_BACK || 
+	    dmc->cache_mode > FLASHCACHE_WRITE_AROUND) {
+		DMERR("cache_mode = %d", dmc->cache_mode);
+		ti->error = "flashcache: Invalid cache mode";
+		r = -EINVAL;
+		goto bad3;
+	}
+	
+	/* 
+	 * XXX - Persistence is totally ignored for write through and write around.
+	 * Maybe this should really be moved to the end of the param list ?
+	 */
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		if (argc >= 5) {
+			if (sscanf(argv[4], "%u", &persistence) != 1) {
+				ti->error = "flashcache: sscanf failed, invalid cache persistence";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (persistence < CACHE_RELOAD || persistence > CACHE_FORCECREATE) {
+				DMERR("persistence = %d", persistence);
+				ti->error = "flashcache: Invalid cache persistence";
+				r = -EINVAL;
+				goto bad3;
+			}			
+		}
+		if (persistence == CACHE_RELOAD) {
+			if (flashcache_writeback_load(dmc)) {
+				ti->error = "flashcache: Cache reload failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+			goto init; /* Skip reading cache parameters from command line */
+		}
+	} else
+		persistence = CACHE_CREATE;
+
+	if (argc >= 6) {
+		if (sscanf(argv[5], "%u", &dmc->block_size) != 1) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad3;
+		}
+		if (!dmc->block_size || (dmc->block_size & (dmc->block_size - 1))) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+	
+	if (!dmc->block_size)
+		dmc->block_size = DEFAULT_BLOCK_SIZE;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+
+	/* dmc->size is specified in sectors here, and converted to blocks later */
+	if (argc >= 7) {
+		if (sscanf(argv[6], "%lu", &dmc->size) != 1) {
+			ti->error = "flashcache: Invalid cache size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+	
+	if (!dmc->size)
+		dmc->size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+
+	if (argc >= 8) {
+		if (sscanf(argv[7], "%u", &dmc->assoc) != 1) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad3;
+		}
+		if (!dmc->assoc || (dmc->assoc & (dmc->assoc - 1)) ||
+		    dmc->assoc > FLASHCACHE_MAX_ASSOC ||
+		    dmc->assoc < FLASHCACHE_MIN_ASSOC ||
+		    dmc->size < dmc->assoc) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+
+	if (!dmc->assoc)
+		dmc->assoc = DEFAULT_CACHE_ASSOC;
+	dmc->assoc_shift = ffs(dmc->assoc) - 1;
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		if (argc >= 9) {
+			if (sscanf(argv[8], "%u", &dmc->md_block_size) != 1) {
+				ti->error = "flashcache: Invalid metadata block size";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (!dmc->md_block_size || (dmc->md_block_size & (dmc->md_block_size - 1)) ||
+			    dmc->md_block_size > FLASHCACHE_MAX_MD_BLOCK_SIZE) {
+				ti->error = "flashcache: Invalid metadata block size";
+				r = -EINVAL;
+				goto bad3;
+			}
+			if (dmc->assoc < 
+			    (dmc->md_block_size * 512 / sizeof(struct flash_cacheblock))) {
+				ti->error = "flashcache: Please choose a smaller metadata block size or larger assoc";
+				r = -EINVAL;
+				goto bad3;
+			}
+		}
+
+		if (!dmc->md_block_size)
+			dmc->md_block_size = DEFAULT_MD_BLOCK_SIZE;
+
+		if (dmc->md_block_size * 512 < dmc->cache_dev->bdev->bd_block_size) {
+			ti->error = "flashcache: Metadata block size must be >= cache device sector size";
+			r = -EINVAL;
+			goto bad3;
+		}
+	}
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {	
+		if (persistence == CACHE_CREATE) {
+			if (flashcache_writeback_create(dmc, 0)) {
+				ti->error = "flashcache: Cache Create Failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+		} else {
+			if (flashcache_writeback_create(dmc, 1)) {
+				ti->error = "flashcache: Cache Force Create Failed";
+				r = -EINVAL;
+				goto bad3;
+			}
+		}
+	} else
+		flashcache_writethrough_create(dmc);
+
+init:
+	dmc->num_sets = dmc->size >> dmc->assoc_shift;
+	order = dmc->num_sets * sizeof(struct cache_set);
+	dmc->cache_sets = (struct cache_set *)vmalloc(order);
+	if (!dmc->cache_sets) {
+		ti->error = "Unable to allocate memory";
+		r = -ENOMEM;
+		vfree((void *)dmc->cache);
+		goto bad3;
+	}				
+
+	for (i = 0 ; i < dmc->num_sets ; i++) {
+		dmc->cache_sets[i].set_fifo_next = i * dmc->assoc;
+		dmc->cache_sets[i].set_clean_next = i * dmc->assoc;
+		dmc->cache_sets[i].nr_dirty = 0;
+		dmc->cache_sets[i].clean_inprog = 0;
+		dmc->cache_sets[i].dirty_fallow = 0;
+		dmc->cache_sets[i].fallow_tstamp = jiffies;
+		dmc->cache_sets[i].fallow_next_cleaning = jiffies;
+		dmc->cache_sets[i].lru_tail = FLASHCACHE_LRU_NULL;
+		dmc->cache_sets[i].lru_head = FLASHCACHE_LRU_NULL;
+	}
+
+	/* Push all blocks into the set specific LRUs */
+	for (i = 0 ; i < dmc->size ; i++) {
+		dmc->cache[i].lru_prev = FLASHCACHE_LRU_NULL;
+		dmc->cache[i].lru_next = FLASHCACHE_LRU_NULL;
+		flashcache_reclaim_lru_movetail(dmc, i);
+	}
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		order = (dmc->md_blocks - 1) * sizeof(struct cache_md_block_head);
+		dmc->md_blocks_buf = (struct cache_md_block_head *)vmalloc(order);
+		if (!dmc->md_blocks_buf) {
+			ti->error = "Unable to allocate memory";
+			r = -ENOMEM;
+			vfree((void *)dmc->cache);
+			vfree((void *)dmc->cache_sets);
+			goto bad3;
+		}		
+
+		for (i = 0 ; i < dmc->md_blocks - 1 ; i++) {
+			dmc->md_blocks_buf[i].nr_in_prog = 0;
+			dmc->md_blocks_buf[i].queued_updates = NULL;
+		}
+	}
+
+	spin_lock_init(&dmc->cache_spin_lock);
+
+	dmc->sync_index = 0;
+	dmc->clean_inprog = 0;
+
+	ti->split_io = dmc->block_size;
+	ti->private = dmc;
+
+	/* Cleaning Thresholds */
+	dmc->sysctl_dirty_thresh = DIRTY_THRESH_DEF;
+	dmc->dirty_thresh_set = (dmc->assoc * dmc->sysctl_dirty_thresh) / 100;
+	dmc->max_clean_ios_total = MAX_CLEAN_IOS_TOTAL;
+	dmc->max_clean_ios_set = MAX_CLEAN_IOS_SET;
+
+	/* Other sysctl defaults */
+	dmc->sysctl_io_latency_hist = 0;
+	dmc->sysctl_do_sync = 0;
+	dmc->sysctl_stop_sync = 0;
+	dmc->sysctl_pid_do_expiry = 0;
+	dmc->sysctl_max_pids = MAX_PIDS;
+	dmc->sysctl_pid_expiry_secs = PID_EXPIRY_SECS;
+	dmc->sysctl_reclaim_policy = FLASHCACHE_FIFO;
+	dmc->sysctl_zerostats = 0;
+	dmc->sysctl_error_inject = 0;
+	dmc->sysctl_fast_remove = 0;
+	dmc->sysctl_cache_all = 1;
+	dmc->sysctl_fallow_clean_speed = FALLOW_CLEAN_SPEED;
+	dmc->sysctl_fallow_delay = FALLOW_DELAY;
+	dmc->sysctl_skip_seq_thresh_kb = SKIP_SEQUENTIAL_THRESHOLD;
+
+	/* Sequential i/o spotting */	
+	for (i = 0; i < SEQUENTIAL_TRACKER_QUEUE_DEPTH; i++) {
+		dmc->seq_recent_ios[i].most_recent_sector = 0;
+		dmc->seq_recent_ios[i].sequential_count = 0;
+		dmc->seq_recent_ios[i].prev = (struct sequential_io *)NULL;
+		dmc->seq_recent_ios[i].next = (struct sequential_io *)NULL;
+		seq_io_move_to_lruhead(dmc, &dmc->seq_recent_ios[i]);
+	}
+	dmc->seq_io_tail = &dmc->seq_recent_ios[0];
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, TASK_UNINTERRUPTIBLE);
+	dmc->next_cache = cache_list_head;
+	cache_list_head = dmc;
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			dmc->cached_blocks++;
+		if (dmc->cache[i].cache_state & DIRTY) {
+			dmc->cache_sets[i / dmc->assoc].nr_dirty++;
+			dmc->nr_dirty++;
+		}
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&dmc->delayed_clean, flashcache_clean_all_sets, dmc);
+#else
+	INIT_DELAYED_WORK(&dmc->delayed_clean, flashcache_clean_all_sets);
+#endif
+
+	dmc->whitelist_head = NULL;
+	dmc->whitelist_tail = NULL;
+	dmc->blacklist_head = NULL;
+	dmc->blacklist_tail = NULL;
+	dmc->num_whitelist_pids = 0;
+	dmc->num_blacklist_pids = 0;
+
+	flashcache_ctr_procfs(dmc);
+
+	return 0;
+
+bad3:
+	dm_put_device(ti, dmc->cache_dev);
+bad2:
+	dm_put_device(ti, dmc->disk_dev);
+bad1:
+	kfree(dmc);
+bad:
+	return r;
+}
+
+static void
+flashcache_dtr_stats_print(struct cache_c *dmc)
+{
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+	u_int64_t  cache_pct, dirty_pct;
+	char *cache_mode;
+	int i;
+	
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	
+	DMINFO("stats: \n\treads(%lu), writes(%lu)", stats->reads, stats->writes);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" \
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n" ,
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->dirty_write_hits, dirty_write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tmetadata dirties(%lu), metadata cleans(%lu)\n" \
+		       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+		       "\tcleanings(%lu) fallow cleanings(%lu)\n"	\
+		       "\tno room(%lu) front merge(%lu) back merge(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->md_write_dirty, stats->md_write_clean,
+		       stats->md_write_batch, stats->md_ssd_writes,
+		       stats->cleanings, stats->fallow_cleanings, 
+		       stats->noroom, stats->front_merge, stats->back_merge);
+	} else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\treplacement(%lu)\n"				\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->replace,
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	} else 	{	/* WRITE_AROUND */
+		DMINFO("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\treplacement(%lu)\n"				\
+		       "\tinvalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->replace,
+		       stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMINFO("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	}
+	/* All modes */
+        DMINFO("\tdisk reads(%lu), disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+               "\tuncached reads(%lu), uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tuncached sequential reads(%lu), uncached sequential writes(%lu)\n" \
+               "\tpid_adds(%lu), pid_dels(%lu), pid_drops(%lu) pid_expiry(%lu)",
+               stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+               stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->uncached_sequential_reads, stats->uncached_sequential_writes,
+               stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	if (dmc->size > 0) {
+		dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+		cache_pct = ((u_int64_t)dmc->cached_blocks * 100) / dmc->size;
+	} else {
+		cache_pct = 0;
+		dirty_pct = 0;
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		cache_mode = "WRITE_BACK";
+	else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+		cache_mode = "WRITE_THROUGH";
+	else
+		cache_mode = "WRITE_AROUND";
+	DMINFO("conf:\n"						\
+	       "\tvirt dev (%s), ssd dev (%s), disk dev (%s) cache mode(%s)\n"		\
+	       "\tcapacity(%luM), associativity(%u), data block size(%uK) metadata block size(%ub)\n" \
+	       "\tskip sequential thresh(%uK)\n" \
+	       "\ttotal blocks(%lu), cached blocks(%lu), cache percent(%d)\n" \
+	       "\tdirty blocks(%d), dirty percent(%d)\n",
+	       dmc->dm_vdevname, dmc->cache_devname, dmc->disk_devname,
+	       cache_mode,
+	       dmc->size*dmc->block_size>>11, dmc->assoc,
+	       dmc->block_size>>(10-SECTOR_SHIFT), 
+	       dmc->md_block_size * 512, 
+	       dmc->sysctl_skip_seq_thresh_kb,
+	       dmc->size, dmc->cached_blocks, 
+	       (int)cache_pct, dmc->nr_dirty, (int)dirty_pct);
+	DMINFO("\tnr_queued(%lu)\n", dmc->pending_jobs_count);
+	DMINFO("Size Hist: ");
+	for (i = 1 ; i <= 32 ; i++) {
+		if (size_hist[i] > 0)
+			DMINFO("%d:%llu ", i*512, size_hist[i]);
+	}
+}
+
+/*
+ * Destroy the cache mapping.
+ */
+void 
+flashcache_dtr(struct dm_target *ti)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct cache_c **nodepp;
+	int i;
+	int nr_queued = 0;
+
+	flashcache_dtr_procfs(dmc);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		flashcache_sync_for_remove(dmc);
+		flashcache_writeback_md_store(dmc);
+	}
+	if (!dmc->sysctl_fast_remove && dmc->nr_dirty > 0)
+		DMERR("Could not sync %d blocks to disk, cache still dirty", 
+		      dmc->nr_dirty);
+	DMINFO("cache jobs %d, pending jobs %d", atomic_read(&nr_cache_jobs), 
+	       atomic_read(&nr_pending_jobs));
+	for (i = 0 ; i < dmc->size ; i++)
+		nr_queued += dmc->cache[i].nr_queued;
+	DMINFO("cache queued jobs %d", nr_queued);	
+	flashcache_dtr_stats_print(dmc);
+
+	vfree((void *)dmc->cache);
+	vfree((void *)dmc->cache_sets);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		vfree((void *)dmc->md_blocks_buf);
+	flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 1);
+	flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 1);
+	VERIFY(dmc->num_whitelist_pids == 0);
+	VERIFY(dmc->num_blacklist_pids == 0);
+	dm_put_device(ti, dmc->disk_dev);
+	dm_put_device(ti, dmc->cache_dev);
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, 
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, 
+			       TASK_UNINTERRUPTIBLE);
+	nodepp = &cache_list_head;
+	while (*nodepp != NULL) {
+		if (*nodepp == dmc) {
+			*nodepp = dmc->next_cache;
+			break;
+		}
+		nodepp = &((*nodepp)->next_cache);
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	kfree(dmc);
+}
+
+void
+flashcache_status_info(struct cache_c *dmc, status_type_t type,
+		       char *result, unsigned int maxlen)
+{
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+	int sz = 0; /* DMEMIT */
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+
+	
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	DMEMIT("stats: \n\treads(%lu), writes(%lu)\n", 
+	       stats->reads, stats->writes);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" \
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->dirty_write_hits, dirty_write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tmetadata dirties(%lu), metadata cleans(%lu)\n" \
+		       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+		       "\tcleanings(%lu) fallow cleanings(%lu)\n"	\
+		       "\tno room(%lu) front merge(%lu) back merge(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->md_write_dirty, stats->md_write_clean,
+		       stats->md_write_batch, stats->md_ssd_writes,
+		       stats->cleanings, stats->fallow_cleanings, 
+		       stats->noroom, stats->front_merge, stats->back_merge);
+	} else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\twrite hits(%lu) write hit percent(%d)\n"	\
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\twrite invalidates(%lu), read invalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->write_hits, write_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->wr_invalidates, stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	} else {	/* WRITE_AROUND */
+		DMEMIT("\tread hits(%lu), read hit percent(%d)\n"	\
+		       "\treplacement(%lu), write replacement(%lu)\n"	\
+		       "\tinvalidates(%lu)\n",
+		       stats->read_hits, read_hit_pct,
+		       stats->replace, stats->wr_replace, 
+		       stats->rd_invalidates);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMEMIT("\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n",
+		       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+		DMEMIT("\tpending enqueues(%lu), pending inval(%lu)\n"	\
+		       "\tno room(%lu)\n",
+		       stats->enqueues, stats->pending_inval,
+		       stats->noroom);
+	}
+	/* All modes */
+	DMEMIT("\tdisk reads(%lu), disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+	       "\tuncached reads(%lu), uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tuncached sequential reads(%lu), uncached sequential writes(%lu)\n" \
+	       "\tpid_adds(%lu), pid_dels(%lu), pid_drops(%lu) pid_expiry(%lu)",
+	       stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+	       stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->uncached_sequential_reads, stats->uncached_sequential_writes,
+	       stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	if (dmc->sysctl_io_latency_hist) {
+		int i;
+		
+		DMEMIT("\nIO Latency Histogram: \n");
+		for (i = 1 ; i <= IO_LATENCY_BUCKETS ; i++) {
+			DMEMIT("< %d\tusecs : %lu\n", i * IO_LATENCY_GRAN_USECS, dmc->latency_hist[i - 1]);
+		}
+		DMEMIT("> 10\tmsecs : %lu", dmc->latency_hist_10ms);		
+	}
+}
+
+static void
+flashcache_status_table(struct cache_c *dmc, status_type_t type,
+			     char *result, unsigned int maxlen)
+{
+	u_int64_t  cache_pct, dirty_pct;
+	int i;
+	int sz = 0; /* DMEMIT */
+	char *cache_mode;
+	
+
+	if (dmc->size > 0) {
+		dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+		cache_pct = ((u_int64_t)dmc->cached_blocks * 100) / dmc->size;
+	} else {
+		cache_pct = 0;
+		dirty_pct = 0;
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		cache_mode = "WRITE_BACK";
+	else if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+		cache_mode = "WRITE_THROUGH";
+	else
+		cache_mode = "WRITE_AROUND";
+	DMEMIT("conf:\n");
+	DMEMIT("\tssd dev (%s), disk dev (%s) cache mode(%s)\n",
+	       dmc->cache_devname, dmc->disk_devname,
+	       cache_mode);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tcapacity(%luM), associativity(%u), data block size(%uK) metadata block size(%ub)\n",
+		       dmc->size*dmc->block_size>>11, dmc->assoc,
+		       dmc->block_size>>(10-SECTOR_SHIFT), 
+		       dmc->md_block_size * 512);
+	} else {
+		DMEMIT("\tcapacity(%luM), associativity(%u), data block size(%uK)\n",
+		       dmc->size*dmc->block_size>>11, dmc->assoc,
+		       dmc->block_size>>(10-SECTOR_SHIFT));
+	}
+	DMEMIT("\tskip sequential thresh(%uK)\n",
+	       dmc->sysctl_skip_seq_thresh_kb);
+	DMEMIT("\ttotal blocks(%lu), cached blocks(%lu), cache percent(%d)\n",
+	       dmc->size, dmc->cached_blocks,
+	       (int)cache_pct);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		DMEMIT("\tdirty blocks(%d), dirty percent(%d)\n",
+		       dmc->nr_dirty, (int)dirty_pct);
+	}
+	DMEMIT("\tnr_queued(%lu)\n", dmc->pending_jobs_count);
+	DMEMIT("Size Hist: ");
+	for (i = 1 ; i <= 32 ; i++) {
+		if (size_hist[i] > 0)
+			DMEMIT("%d:%llu ", i*512, size_hist[i]);
+	}
+}
+
+/*
+ * Report cache status:
+ *  Output cache stats upon request of device status;
+ *  Output cache configuration upon request of table status.
+ */
+int 
+flashcache_status(struct dm_target *ti, status_type_t type,
+	     char *result, unsigned int maxlen)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	
+	switch (type) {
+	case STATUSTYPE_INFO:
+		flashcache_status_info(dmc, type, result, maxlen);
+		break;
+	case STATUSTYPE_TABLE:
+		flashcache_status_table(dmc, type, result, maxlen);
+		break;
+	}
+	return 0;
+}
+
+static struct target_type flashcache_target = {
+	.name   = "flashcache",
+	.version= {1, 0, 3},
+	.module = THIS_MODULE,
+	.ctr    = flashcache_ctr,
+	.dtr    = flashcache_dtr,
+	.map    = flashcache_map,
+	.status = flashcache_status,
+	.ioctl 	= flashcache_ioctl,
+};
+
+static void
+flashcache_sync_for_remove(struct cache_c *dmc)
+{
+	do {
+		atomic_set(&dmc->remove_in_prog, SLOW_REMOVE); /* Stop cleaning of sets */
+		if (!dmc->sysctl_fast_remove) {
+			/* 
+			 * Kick off cache cleaning. client_destroy will wait for cleanings
+			 * to finish.
+			 */
+			printk(KERN_ALERT "Cleaning %d blocks please WAIT", dmc->nr_dirty);
+			/* Tune up the cleaning parameters to clean very aggressively */
+			dmc->max_clean_ios_total = 20;
+			dmc->max_clean_ios_set = 10;
+			flashcache_sync_all(dmc);
+		} else {
+			/* Needed to abort any in-progress cleanings, leave blocks DIRTY */
+			atomic_set(&dmc->remove_in_prog, FAST_REMOVE);
+			printk(KERN_ALERT "Fast flashcache remove Skipping cleaning of %d blocks", 
+			       dmc->nr_dirty);
+		}
+		/* 
+		 * We've prevented new cleanings from starting (for the fast remove case)
+		 * and we will wait for all in progress cleanings to exit.
+		 * Wait a few seconds for everything to quiesce before writing out the 
+		 * cache metadata.
+		 */
+		msleep(FLASHCACHE_SYNC_REMOVE_DELAY);
+		/* Wait for all the dirty blocks to get written out, and any other IOs */
+		wait_event(dmc->destroyq, !atomic_read(&dmc->nr_jobs));
+		cancel_delayed_work(&dmc->delayed_clean);
+		flush_scheduled_work();
+	} while (!dmc->sysctl_fast_remove && dmc->nr_dirty > 0);
+}
+
+static int 
+flashcache_notify_reboot(struct notifier_block *this,
+			 unsigned long code, void *x)
+{
+	struct cache_c *dmc;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, 
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, 
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ; 
+	     dmc != NULL ; 
+	     dmc = dmc->next_cache) {
+		if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+			flashcache_sync_for_remove(dmc);
+			flashcache_writeback_md_store(dmc);
+		}
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return NOTIFY_DONE;
+}
+
+/*
+ * The notifiers are registered in descending order of priority and
+ * executed in descending order or priority. We should be run before
+ * any notifiers of ssd's or other block devices. Typically, devices
+ * use a priority of 0.
+ * XXX - If in the future we happen to use a md device as the cache
+ * block device, we have a problem because md uses a priority of 
+ * INT_MAX as well. But we want to run before the md's reboot notifier !
+ */
+static struct notifier_block flashcache_notifier = {
+	.notifier_call	= flashcache_notify_reboot,
+	.next		= NULL,
+	.priority	= INT_MAX, /* should be > ssd pri's and disk dev pri's */
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+struct dm_kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#else
+struct kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+/*
+ * Initiate a cache target.
+ */
+int __init 
+flashcache_init(void)
+{
+	int r;
+
+	r = flashcache_jobs_init();
+	if (r)
+		return r;
+	atomic_set(&nr_cache_jobs, 0);
+	atomic_set(&nr_pending_jobs, 0);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	r = dm_io_get(FLASHCACHE_ASYNC_SIZE);
+	if (r) {
+		DMERR("flashcache_init: Could not size dm io pool");
+		return r;
+	}
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+	if (r) {
+		DMERR("flashcache_init: Failed to initialize kcopyd client");
+		dm_io_put(FLASHCACHE_ASYNC_SIZE);
+		return r;
+	}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22) */
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)) || (defined(RHEL_RELEASE_CODE) && (RHEL_RELEASE_CODE >= 1538))
+	flashcache_io_client = dm_io_client_create();
+#else
+	flashcache_io_client = dm_io_client_create(FLASHCACHE_COPY_PAGES);
+#endif
+	if (IS_ERR(flashcache_io_client)) {
+		DMERR("flashcache_init: Failed to initialize DM IO client");
+		return r;
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+#elif (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)) || (defined(RHEL_RELEASE_CODE) && (RHEL_RELEASE_CODE >= 1538))
+	flashcache_kcp_client = dm_kcopyd_client_create();
+	if ((r = IS_ERR(flashcache_kcp_client))) {
+		r = PTR_ERR(flashcache_kcp_client);
+	}
+#else /* .26 <= VERSION < 3.0.0 */
+	r = dm_kcopyd_client_create(FLASHCACHE_COPY_PAGES, &flashcache_kcp_client);
+#endif /* .26 <= VERSION < 3.0.0 */
+
+	if (r) {
+		dm_io_client_destroy(flashcache_io_client);
+		DMERR("flashcache_init: Failed to initialize kcopyd client");
+		return r;
+	}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22) */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&_kcached_wq, do_work, NULL);
+#else
+	INIT_WORK(&_kcached_wq, do_work);
+#endif
+	for (r = 0 ; r < 33 ; r++)
+		size_hist[r] = 0;
+	r = dm_register_target(&flashcache_target);
+	if (r < 0) {
+		DMERR("cache: register failed %d", r);
+	}
+
+        printk("flashcache: %s initialized\n", flashcache_sw_version);
+
+	flashcache_module_procfs_init();
+	flashcache_control = (struct flashcache_control_s *)
+		kmalloc(sizeof(struct flashcache_control_s), GFP_KERNEL);
+	flashcache_control->synch_flags = 0;
+	register_reboot_notifier(&flashcache_notifier);
+	return r;
+}
+
+/*
+ * Destroy a cache target.
+ */
+void 
+flashcache_exit(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	int r = dm_unregister_target(&flashcache_target);
+
+	if (r < 0)
+		DMERR("cache: unregister failed %d", r);
+#else
+	dm_unregister_target(&flashcache_target);
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	kcopyd_client_destroy(flashcache_kcp_client);
+#else
+	dm_kcopyd_client_destroy(flashcache_kcp_client);
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	dm_io_client_destroy(flashcache_io_client);
+#else
+	dm_io_put(FLASHCACHE_ASYNC_SIZE);
+#endif
+	unregister_reboot_notifier(&flashcache_notifier);
+	flashcache_jobs_exit();
+	flashcache_module_procfs_releae();
+	kfree(flashcache_control);
+}
+
+module_init(flashcache_init);
+module_exit(flashcache_exit);
+
+EXPORT_SYMBOL(flashcache_writeback_load);
+EXPORT_SYMBOL(flashcache_writeback_create);
+EXPORT_SYMBOL(flashcache_writeback_md_store);
+
+MODULE_DESCRIPTION(DM_NAME " Facebook flash cache target");
+MODULE_AUTHOR("Mohan - based on code by Ming");
+MODULE_LICENSE("GPL");
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache.h linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache.h
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache.h	2016-12-13 17:25:21.064077666 +0800
@@ -0,0 +1,605 @@
+/****************************************************************************
+ *  flashcache.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_H
+#define FLASHCACHE_H
+
+#define FLASHCACHE_VERSION		2
+
+#define DEV_PATHLEN	128
+
+#ifdef __KERNEL__
+
+/* Like ASSERT() but always compiled in */
+
+#define VERIFY(x) do { \
+	if (unlikely(!(x))) { \
+		dump_stack(); \
+		panic("VERIFY: assertion (%s) failed at %s (%d)\n", \
+		      #x,  __FILE__ , __LINE__);		    \
+	} \
+} while(0)
+
+#define DMC_DEBUG 0
+#define DMC_DEBUG_LITE 0
+
+#define DM_MSG_PREFIX "flashcache"
+#define DMC_PREFIX "flashcache: "
+
+#if DMC_DEBUG
+#define DPRINTK( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK( s, arg... )
+#endif
+
+/*
+ * Block checksums :
+ * Block checksums seem a good idea (especially for debugging, I found a couple 
+ * of bugs with this), but in practice there are a number of issues with this
+ * in production.
+ * 1) If a flash write fails, there is no guarantee that the failure was atomic.
+ * Some sectors may have been written to flash. If so, the checksum we have
+ * is wrong. We could re-read the flash block and recompute the checksum, but
+ * the read could fail too. 
+ * 2) On a node crash, we could have crashed between the flash data write and the
+ * flash metadata update (which updates the new checksum to flash metadata). When
+ * we reboot, the checksum we read from metadata is wrong. This is worked around
+ * by having the cache load recompute checksums after an unclean shutdown.
+ * 3) Checksums require 4 or 8 more bytes per block in terms of metadata overhead.
+ * Especially because the metadata is wired into memory.
+ * 4) Checksums force us to do a flash metadata IO on a block re-dirty. If we 
+ * didn't maintain checksums, we could avoid the metadata IO on a re-dirty.
+ * Therefore in production we disable block checksums.
+ */
+#if 0
+#define FLASHCACHE_DO_CHECKSUMS
+#endif
+
+#if DMC_DEBUG_LITE
+#define DPRINTK_LITE( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK_LITE( s, arg... )
+#endif
+
+/* Number of pages for I/O */
+#define FLASHCACHE_COPY_PAGES (1024)
+
+/* Default cache parameters */
+#define DEFAULT_CACHE_SIZE	65536
+#define DEFAULT_CACHE_ASSOC	512
+#define DEFAULT_BLOCK_SIZE	8	/* 4 KB */
+#define DEFAULT_MD_BLOCK_SIZE	8	/* 4 KB */
+#define FLASHCACHE_MAX_MD_BLOCK_SIZE	128	/* 64 KB */
+
+#define FLASHCACHE_FIFO		0
+#define FLASHCACHE_LRU		1
+
+/*
+ * The LRU pointers are maintained as set-relative offsets, instead of 
+ * pointers. This enables us to store the LRU pointers per cacheblock
+ * using 4 bytes instead of 16 bytes. The upshot of this is that we 
+ * are required to clamp the associativity at an 8K max.
+ */
+#define FLASHCACHE_MIN_ASSOC	 256
+#define FLASHCACHE_MAX_ASSOC	8192
+#define FLASHCACHE_LRU_NULL	0xFFFF
+
+struct cacheblock;
+
+struct cache_set {
+	u_int32_t		set_fifo_next;
+	u_int32_t		set_clean_next;
+	u_int16_t		clean_inprog;
+	u_int16_t		nr_dirty;
+	u_int16_t		lru_head, lru_tail;
+	u_int16_t		dirty_fallow;
+	unsigned long 		fallow_tstamp;
+	unsigned long 		fallow_next_cleaning;
+};
+
+struct flashcache_errors {
+	int	disk_read_errors;
+	int	disk_write_errors;
+	int	ssd_read_errors;
+	int	ssd_write_errors;
+	int	memory_alloc_errors;
+};
+
+struct flashcache_stats {
+	unsigned long reads;		/* Number of reads */
+	unsigned long writes;		/* Number of writes */
+	unsigned long read_hits;	/* Number of cache hits */
+	unsigned long write_hits;	/* Number of write hits (includes dirty write hits) */
+	unsigned long dirty_write_hits;	/* Number of "dirty" write hits */
+	unsigned long replace;		/* Number of cache replacements */
+	unsigned long wr_replace;
+	unsigned long wr_invalidates;	/* Number of write invalidations */
+	unsigned long rd_invalidates;	/* Number of read invalidations */
+	unsigned long pending_inval;	/* Invalidations due to concurrent ios on same block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	unsigned long checksum_store;
+	unsigned long checksum_valid;
+	unsigned long checksum_invalid;
+#endif
+	unsigned long enqueues;		/* enqueues on pending queue */
+	unsigned long cleanings;
+	unsigned long fallow_cleanings;
+	unsigned long noroom;		/* No room in set */
+	unsigned long md_write_dirty;	/* Metadata sector writes dirtying block */
+	unsigned long md_write_clean;	/* Metadata sector writes cleaning block */
+	unsigned long md_write_batch;	/* How many md updates did we batch ? */
+	unsigned long md_ssd_writes;	/* How many md ssd writes did we do ? */
+	unsigned long pid_drops;
+	unsigned long pid_adds;
+	unsigned long pid_dels;
+	unsigned long expiry;
+	unsigned long front_merge, back_merge;	/* Write Merging */
+	unsigned long uncached_reads, uncached_writes;
+	unsigned long uncached_sequential_reads, uncached_sequential_writes;
+	unsigned long disk_reads, disk_writes;
+	unsigned long ssd_reads, ssd_writes;
+	unsigned long uncached_io_requeue;
+	unsigned long skipclean;
+	unsigned long trim_blocks;
+	unsigned long clean_set_ios;
+};
+
+/* 
+ * Sequential block history structure - each one
+ * records a 'flow' of i/o.
+ */
+struct sequential_io {
+ 	sector_t 		most_recent_sector;
+	unsigned long		sequential_count;
+	/* We use LRU replacement when we need to record a new i/o 'flow' */
+	struct sequential_io 	*prev, *next;
+};
+#define SKIP_SEQUENTIAL_THRESHOLD 0			/* 0 = cache all, >0 = dont cache sequential i/o more than this (kb) */
+#define SEQUENTIAL_TRACKER_QUEUE_DEPTH	32		/* How many io 'flows' to track (random i/o will hog many).
+							 * This should be large enough so that we don't quickly 
+							 * evict sequential i/o when we see some random,
+							 * but small enough that searching through it isn't slow
+							 * (currently we do linear search, we could consider hashed */
+								
+	
+/*
+ * Cache context
+ */
+struct cache_c {
+	struct dm_target	*tgt;
+	
+	struct dm_dev 		*disk_dev;   /* Source device */
+	struct dm_dev 		*cache_dev; /* Cache device */
+
+	int 			on_ssd_version;
+	
+	spinlock_t		cache_spin_lock;
+
+	struct cacheblock	*cache;	/* Hash table for cache blocks */
+	struct cache_set	*cache_sets;
+	struct cache_md_block_head *md_blocks_buf;
+
+	unsigned int md_block_size;	/* Metadata block size in sectors */
+	
+	sector_t size;			/* Cache size */
+	unsigned int assoc;		/* Cache associativity */
+	unsigned int block_size;	/* Cache block size */
+	unsigned int block_shift;	/* Cache block size in bits */
+	unsigned int block_mask;	/* Cache block mask */
+	unsigned int assoc_shift;	/* Consecutive blocks size in bits */
+	unsigned int num_sets;		/* Number of cache sets */
+	
+	int	cache_mode;
+
+	wait_queue_head_t destroyq;	/* Wait queue for I/O completion */
+	/* XXX - Updates of nr_jobs should happen inside the lock. But doing it outside
+	   is OK since the filesystem is unmounted at this point */
+	atomic_t nr_jobs;		/* Number of I/O jobs */
+
+#define SLOW_REMOVE    1                                                                                    
+#define FAST_REMOVE    2
+	atomic_t remove_in_prog;
+
+	int	dirty_thresh_set;	/* Per set dirty threshold to start cleaning */
+	int	max_clean_ios_set;	/* Max cleaning IOs per set */
+	int	max_clean_ios_total;	/* Total max cleaning IOs */
+	int	clean_inprog;
+	int	sync_index;
+	int	nr_dirty;
+	unsigned long cached_blocks;	/* Number of cached blocks */
+	unsigned long pending_jobs_count;
+	int	md_blocks;		/* Numbers of metadata blocks, including header */
+
+	/* Stats */
+	struct flashcache_stats flashcache_stats;
+
+	/* Errors */
+	struct flashcache_errors flashcache_errors;
+
+#define IO_LATENCY_GRAN_USECS	250
+#define IO_LATENCY_MAX_US_TRACK	10000	/* 10 ms */
+#define IO_LATENCY_BUCKETS	(IO_LATENCY_MAX_US_TRACK / IO_LATENCY_GRAN_USECS)
+	unsigned long	latency_hist[IO_LATENCY_BUCKETS];
+	unsigned long	latency_hist_10ms;
+	
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	struct work_struct delayed_clean;
+#else
+	struct delayed_work delayed_clean;
+#endif
+
+	unsigned long pid_expire_check;
+
+	struct flashcache_cachectl_pid *blacklist_head, *blacklist_tail;
+	struct flashcache_cachectl_pid *whitelist_head, *whitelist_tail;
+	int num_blacklist_pids, num_whitelist_pids;
+	unsigned long blacklist_expire_check, whitelist_expire_check;
+
+#define PENDING_JOB_HASH_SIZE		32
+	struct pending_job *pending_job_hashbuckets[PENDING_JOB_HASH_SIZE];
+	
+	struct cache_c	*next_cache;
+
+	void *sysctl_handle;
+
+	// DM virtual device name, stored in superblock and restored on load
+	char dm_vdevname[DEV_PATHLEN];
+	// real device names are now stored as UUIDs
+	char cache_devname[DEV_PATHLEN];
+	char disk_devname[DEV_PATHLEN];
+
+	/* Per device sysctls */
+	int sysctl_io_latency_hist;
+	int sysctl_do_sync;
+	int sysctl_stop_sync;
+	int sysctl_dirty_thresh;
+	int sysctl_pid_do_expiry;
+	int sysctl_max_pids;
+	int sysctl_pid_expiry_secs;
+	int sysctl_reclaim_policy;
+	int sysctl_zerostats;
+	int sysctl_error_inject;
+	int sysctl_fast_remove;
+	int sysctl_cache_all;
+	int sysctl_fallow_clean_speed;
+	int sysctl_fallow_delay;
+	int sysctl_skip_seq_thresh_kb;
+
+	/* Sequential I/O spotter */
+	struct sequential_io	seq_recent_ios[SEQUENTIAL_TRACKER_QUEUE_DEPTH];
+	struct sequential_io	*seq_io_head;
+	struct sequential_io 	*seq_io_tail;
+};
+
+/* kcached/pending job states */
+#define READCACHE	1
+#define WRITECACHE	2
+#define READDISK	3
+#define WRITEDISK	4
+#define READFILL	5	/* Read Cache Miss Fill */
+#define INVALIDATE	6
+#define WRITEDISK_SYNC	7
+
+struct kcached_job {
+	struct list_head list;
+	struct cache_c *dmc;
+	struct bio *bio;	/* Original bio */
+	struct job_io_regions {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		struct io_region disk;
+		struct io_region cache;
+#else
+		struct dm_io_region disk;
+		struct dm_io_region cache;
+#endif
+	} job_io_regions;
+	int    index;
+	int    action;
+	int 	error;
+	struct flash_cacheblock *md_block;
+	struct bio_vec md_io_bvec;
+	struct timeval io_start_time;
+	struct kcached_job *next;
+};
+
+struct pending_job {
+	struct bio *bio;
+	int	action;	
+	int	index;
+	struct pending_job *prev, *next;
+};
+#endif /* __KERNEL__ */
+
+/* Cache Modes */
+enum {
+	FLASHCACHE_WRITE_BACK=1,
+	FLASHCACHE_WRITE_THROUGH=2,
+	FLASHCACHE_WRITE_AROUND=3,
+};
+
+/* States of a cache block */
+#define INVALID			0x0001
+#define VALID			0x0002	/* Valid */
+#define DISKREADINPROG		0x0004	/* Read from disk in progress */
+#define DISKWRITEINPROG		0x0008	/* Write to disk in progress */
+#define CACHEREADINPROG		0x0010	/* Read from cache in progress */
+#define CACHEWRITEINPROG	0x0020	/* Write to cache in progress */
+#define DIRTY			0x0040	/* Dirty, needs writeback to disk */
+/*
+ * Old and Dirty blocks are cleaned with a Clock like algorithm. The leading hand
+ * marks DIRTY_FALLOW_1. 900 seconds (default) later, the trailing hand comes along and
+ * marks DIRTY_FALLOW_2 if DIRTY_FALLOW_1 is already set. If the block was used in the 
+ * interim, (DIRTY_FALLOW_1|DIRTY_FALLOW_2) is cleared. Any block that has both 
+ * DIRTY_FALLOW_1 and DIRTY_FALLOW_2 marked is considered old and is eligible 
+ * for cleaning.
+ */
+#define DIRTY_FALLOW_1		0x0080	
+#define DIRTY_FALLOW_2		0x0100
+
+#define FALLOW_DOCLEAN		(DIRTY_FALLOW_1 | DIRTY_FALLOW_2)
+#define BLOCK_IO_INPROG	(DISKREADINPROG | DISKWRITEINPROG | CACHEREADINPROG | CACHEWRITEINPROG)
+
+/* Cache metadata is read by Flashcache utilities */
+#ifndef __KERNEL__
+typedef u_int64_t sector_t;
+#endif
+
+/* On Flash (cache metadata) Structures */
+#define CACHE_MD_STATE_DIRTY		0xdeadbeef
+#define CACHE_MD_STATE_CLEAN		0xfacecafe
+#define CACHE_MD_STATE_FASTCLEAN	0xcafefeed
+#define CACHE_MD_STATE_UNSTABLE		0xc8249756
+
+/* Cache block metadata structure */
+struct cacheblock {
+	u_int16_t	cache_state;
+	int16_t 	nr_queued;	/* jobs in pending queue */
+	u_int16_t	lru_prev, lru_next;
+	sector_t 	dbn;	/* Sector number of the cached block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	u_int64_t 	checksum;
+#endif
+};
+
+struct flash_superblock {
+	sector_t size;		/* Cache size */
+	u_int32_t block_size;	/* Cache block size */
+	u_int32_t assoc;	/* Cache associativity */
+	u_int32_t cache_sb_state;	/* Clean shutdown ? */
+	char cache_devname[DEV_PATHLEN]; /* Contains dm_vdev name as of v2 modifications */
+	sector_t cache_devsize;
+	char disk_devname[DEV_PATHLEN]; /* underlying block device name (use UUID paths!) */
+	sector_t disk_devsize;
+	u_int32_t cache_version;
+	u_int32_t md_block_size;
+};
+
+/* 
+ * We do metadata updates only when a block trasitions from DIRTY -> CLEAN
+ * or from CLEAN -> DIRTY. Consequently, on an unclean shutdown, we only
+ * pick up blocks that are marked (DIRTY | CLEAN), we clean these and stick
+ * them in the cache.
+ * On a clean shutdown, we will sync the state for every block, and we will
+ * load every block back into cache on a restart.
+ * 
+ * Note: When using larger flashcache metadata blocks, it is important to make 
+ * sure that a flash_cacheblock does not straddle 2 sectors. This avoids
+ * partial writes of a metadata slot on a powerfail/node crash. Aligning this
+ * a 16b or 32b struct avoids that issue.
+ * 
+ * Note: If a on-ssd flash_cacheblock does not fit exactly within a 512b sector,
+ * (ie. if there are any remainder runt bytes), logic in flashcache_conf.c which
+ * reads and writes flashcache metadata on create/load/remove will break.
+ * 
+ * If changing these, make sure they remain a ^2 size !
+ */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int64_t 	checksum;
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(32)));
+#else
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(16)));
+#endif
+
+#define MD_BLOCK_BYTES(DMC)		((DMC)->md_block_size * 512)
+#define MD_SECTORS_PER_BLOCK(DMC)	((DMC)->md_block_size)
+#define MD_SLOTS_PER_BLOCK(DMC)		(MD_BLOCK_BYTES(DMC) / (sizeof(struct flash_cacheblock)))
+#define INDEX_TO_MD_BLOCK(DMC, INDEX)	((INDEX) / MD_SLOTS_PER_BLOCK(DMC))
+#define INDEX_TO_MD_BLOCK_OFFSET(DMC, INDEX)	((INDEX) % MD_SLOTS_PER_BLOCK(DMC))
+
+#define METADATA_IO_BLOCKSIZE		(256*1024)
+#define METADATA_IO_NUM_BLOCKS(dmc)	(METADATA_IO_BLOCKSIZE / MD_BLOCK_BYTES(dmc))
+
+#define INDEX_TO_CACHE_ADDR(DMC, INDEX)	\
+	(((sector_t)(INDEX) << (DMC)->block_shift) + (DMC)->md_blocks * MD_SECTORS_PER_BLOCK((DMC)))
+
+#ifdef __KERNEL__
+
+/* Cache persistence */
+#define CACHE_RELOAD		1
+#define CACHE_CREATE		2
+#define CACHE_FORCECREATE	3
+
+/* 
+ * We have one of these for *every* cache metadata sector, to keep track
+ * of metadata ios in progress for blocks covered in this sector. Only
+ * one metadata IO per sector can be in progress at any given point in 
+ * time
+ */
+struct cache_md_block_head {
+	u_int32_t		nr_in_prog;
+	struct kcached_job	*queued_updates, *md_io_inprog;
+};
+
+#define MIN_JOBS 1024
+
+/* Default values for sysctls */
+#define DIRTY_THRESH_MIN	10
+#define DIRTY_THRESH_MAX	90
+#define DIRTY_THRESH_DEF	20
+
+#define MAX_CLEAN_IOS_SET	2
+#define MAX_CLEAN_IOS_TOTAL	4
+#define MAX_PIDS		100
+#define PID_EXPIRY_SECS		60
+#define FALLOW_DELAY		(60*15) /* 15 Mins default */
+#define FALLOW_SPEED_MIN	1
+#define FALLOW_SPEED_MAX	100
+#define FALLOW_CLEAN_SPEED	2
+
+/* DM async IO mempool sizing */
+#define FLASHCACHE_ASYNC_SIZE 1024
+
+enum {
+	FLASHCACHE_WHITELIST=0,
+	FLASHCACHE_BLACKLIST=1,
+};
+
+struct flashcache_cachectl_pid {
+	pid_t					pid;
+	struct flashcache_cachectl_pid		*next, *prev;
+	unsigned long				expiry;
+};
+
+struct dbn_index_pair {
+	sector_t	dbn;
+	int		index;
+};
+
+/* Error injection flags */
+#define READDISK_ERROR				0x00000001
+#define READCACHE_ERROR				0x00000002
+#define READFILL_ERROR				0x00000004
+#define WRITECACHE_ERROR			0x00000008
+#define WRITECACHE_MD_ERROR			0x00000010
+#define WRITEDISK_MD_ERROR			0x00000020
+#define KCOPYD_CALLBACK_ERROR			0x00000040
+#define DIRTY_WRITEBACK_JOB_ALLOC_FAIL		0x00000080
+#define READ_MISS_JOB_ALLOC_FAIL		0x00000100
+#define READ_HIT_JOB_ALLOC_FAIL			0x00000200
+#define READ_HIT_PENDING_JOB_ALLOC_FAIL		0x00000400
+#define INVAL_PENDING_JOB_ALLOC_FAIL		0x00000800
+#define WRITE_HIT_JOB_ALLOC_FAIL		0x00001000
+#define WRITE_HIT_PENDING_JOB_ALLOC_FAIL	0x00002000
+#define WRITE_MISS_JOB_ALLOC_FAIL		0x00004000
+#define WRITES_LIST_ALLOC_FAIL			0x00008000
+#define MD_ALLOC_SECTOR_ERROR			0x00010000
+
+/* Inject a 5s delay between syncing blocks and metadata */
+#define FLASHCACHE_SYNC_REMOVE_DELAY		5000
+
+int flashcache_map(struct dm_target *ti, struct bio *bio,
+		   union map_info *map_context);
+int flashcache_ctr(struct dm_target *ti, unsigned int argc,
+		   char **argv);
+void flashcache_dtr(struct dm_target *ti);
+
+int flashcache_status(struct dm_target *ti, status_type_t type,
+		      char *result, unsigned int maxlen);
+
+struct kcached_job *flashcache_alloc_cache_job(void);
+void flashcache_free_cache_job(struct kcached_job *job);
+struct pending_job *flashcache_alloc_pending_job(struct cache_c *dmc);
+void flashcache_free_pending_job(struct pending_job *job);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+u_int64_t flashcache_compute_checksum(struct bio *bio);
+void flashcache_store_checksum(struct kcached_job *job);
+int flashcache_validate_checksum(struct kcached_job *job);
+int flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block);
+#endif
+struct kcached_job *pop(struct list_head *jobs);
+void push(struct list_head *jobs, struct kcached_job *job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+void do_work(void *unused);
+#else
+void do_work(struct work_struct *unused);
+#endif
+struct kcached_job *new_kcached_job(struct cache_c *dmc, struct bio* bio,
+				    int index);
+void push_pending(struct kcached_job *job);
+void push_io(struct kcached_job *job);
+void push_md_io(struct kcached_job *job);
+void push_md_complete(struct kcached_job *job);
+void push_uncached_io_complete(struct kcached_job *job);
+int flashcache_pending_empty(void);
+int flashcache_io_empty(void);
+int flashcache_md_io_empty(void);
+int flashcache_md_complete_empty(void);
+void flashcache_md_write_done(struct kcached_job *job);
+void flashcache_do_pending(struct kcached_job *job);
+void flashcache_md_write(struct kcached_job *job);
+void flashcache_md_write_kickoff(struct kcached_job *job);
+void flashcache_do_io(struct kcached_job *job);
+void flashcache_uncached_io_complete(struct kcached_job *job);
+void flashcache_clean_set(struct cache_c *dmc, int set);
+void flashcache_sync_all(struct cache_c *dmc);
+void flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index);
+void flashcache_merge_writes(struct cache_c *dmc, 
+			     struct dbn_index_pair *writes_list, 
+			     int *nr_writes, int set);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct io_region *where, 
+			     int rw, void *data);
+#else
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where, 
+			     int rw, void *data);
+#endif
+void flashcache_update_sync_progress(struct cache_c *dmc);
+void flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+			    int index, int action, struct pending_job *job);
+struct pending_job *flashcache_deq_pending(struct cache_c *dmc, int index);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions, 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where, 
+#else
+			    struct io_region *where, 
+#endif
+			    int rw, 
+			    struct bio_vec *bvec, io_notify_fn fn, 
+			    void *context);
+#endif
+
+void flashcache_detect_fallow(struct cache_c *dmc, int index);
+void flashcache_clear_fallow(struct cache_c *dmc, int index);
+
+void flashcache_bio_endio(struct bio *bio, int error, 
+			  struct cache_c *dmc, struct timeval *io_start_time);
+
+/* procfs */
+void flashcache_module_procfs_init(void);
+void flashcache_module_procfs_releae(void);
+void flashcache_ctr_procfs(struct cache_c *dmc);
+void flashcache_dtr_procfs(struct cache_c *dmc);
+
+#endif /* __KERNEL__ */
+
+#endif
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_ioctl.c linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_ioctl.c
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_ioctl.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_ioctl.c	2016-12-13 17:25:21.066077451 +0800
@@ -0,0 +1,565 @@
+/****************************************************************************
+ *  flashcache_ioctl.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+static int flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid, 
+				      int which_list);
+static void flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid, 
+				      int which_list);
+
+static int
+flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid, 
+			   int which_list)
+{
+	struct flashcache_cachectl_pid *pid_list;
+	
+	pid_list = ((which_list == FLASHCACHE_WHITELIST) ? 
+		    dmc->whitelist_head : dmc->blacklist_head);
+	for ( ; pid_list != NULL ; pid_list = pid_list->next) {
+		if (pid_list->pid == pid)
+			return 1;
+	}
+	return 0;	
+}
+
+static void
+flashcache_drop_pids(struct cache_c *dmc, int which_list)
+{
+	if (which_list == FLASHCACHE_WHITELIST) {
+		while (dmc->num_whitelist_pids >= dmc->sysctl_max_pids) {
+			VERIFY(dmc->whitelist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->whitelist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}
+	} else {
+		while (dmc->num_blacklist_pids >= dmc->sysctl_max_pids) {
+			VERIFY(dmc->blacklist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->blacklist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}		
+	}
+}
+
+static void
+flashcache_add_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *new;
+ 	unsigned long flags;
+
+	new = kmalloc(sizeof(struct flashcache_cachectl_pid), GFP_KERNEL);
+	new->pid = pid;
+	new->next = NULL;
+	new->expiry = jiffies + dmc->sysctl_pid_expiry_secs * HZ;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (which_list == FLASHCACHE_WHITELIST) {
+		if (dmc->num_whitelist_pids > dmc->sysctl_max_pids)
+			flashcache_drop_pids(dmc, which_list);
+	} else {
+		if (dmc->num_blacklist_pids > dmc->sysctl_max_pids)
+			flashcache_drop_pids(dmc, which_list);		
+	}
+	if (flashcache_find_pid_locked(dmc, pid, which_list) == 0) {
+		struct flashcache_cachectl_pid **head, **tail;
+		
+		if (which_list == FLASHCACHE_WHITELIST) {
+			head = &dmc->whitelist_head;
+			tail = &dmc->whitelist_tail;
+		} else {
+			head = &dmc->blacklist_head;
+			tail = &dmc->blacklist_tail;
+		}
+		/* Add the new pid to the tail */
+		new->prev = *tail;
+		if (*head == NULL) {
+			VERIFY(*tail == NULL);
+			*head = new;
+		} else {
+			VERIFY(*tail != NULL);
+			(*tail)->next = new;
+		}
+		*tail = new;
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids++;
+		else
+			dmc->num_blacklist_pids++;
+		dmc->flashcache_stats.pid_adds++;
+		/* When adding the first entry to list, set expiry check timeout */
+		if (*head == new)
+			dmc->pid_expire_check = 
+				jiffies + ((dmc->sysctl_pid_expiry_secs + 1) * HZ);
+	} else
+		kfree(new);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	return;
+}
+
+static void
+flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *node;
+	struct flashcache_cachectl_pid **head, **tail;
+	
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *tail ; node != NULL ; node = node->prev) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (node->pid == pid) {
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				*tail = node->prev;
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			kfree(node);
+			dmc->flashcache_stats.pid_dels++;
+			if (which_list == FLASHCACHE_WHITELIST)
+				dmc->num_whitelist_pids--;
+			else
+				dmc->num_blacklist_pids--;
+			return;
+		}
+	}
+}
+
+static void
+flashcache_del_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	flashcache_del_pid_locked(dmc, pid, which_list);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+}
+
+/*
+ * This removes all "dead" pids. Pids that may have not cleaned up.
+ */
+void
+flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force)
+{
+	struct flashcache_cachectl_pid *node, **tail;
+	unsigned long flags;
+	
+	if (which_list == FLASHCACHE_WHITELIST)
+		tail = &dmc->whitelist_tail;
+	else
+		tail = &dmc->blacklist_tail;
+	rcu_read_lock();
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	node = *tail;
+	while (node != NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+		if (force == 0) {
+			struct task_struct *task;
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
+			task = find_task_by_pid_type(PIDTYPE_PID, node->pid);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+			task = find_task_by_vpid(node->pid);
+#else
+			ask = pid_task(find_vpid(node->pid), PIDTYPE_PID);
+#endif
+			/*
+			 * If that task was found, don't remove it !
+			 * This prevents a rogue "delete all" from removing
+			 * every thread from the list.
+			 */
+			if (task) {
+				node = node->prev;
+				continue;
+			}
+		}
+#endif
+		flashcache_del_pid_locked(dmc, node->pid, which_list);
+		node = *tail;
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	rcu_read_unlock();
+}
+
+static void
+flashcache_pid_expiry_list_locked(struct cache_c *dmc, int which_list)
+{
+	struct flashcache_cachectl_pid **head, **tail, *node;
+	
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *head ; node != NULL ; node = node->next) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (time_after(node->expiry, jiffies))
+			continue;
+		if (node->prev == NULL) {
+			*head = node->next;
+			if (node->next)
+				node->next->prev = NULL;
+		} else
+			node->prev->next = node->next;
+		if (node->next == NULL) {
+			*tail = node->prev;
+			if (node->prev)
+				node->prev->next = NULL;
+		} else
+			node->next->prev = node->prev;
+		kfree(node);
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids--;
+		else
+			dmc->num_blacklist_pids--;
+		dmc->flashcache_stats.expiry++;
+	}
+}
+
+void
+flashcache_pid_expiry_all_locked(struct cache_c *dmc)
+{
+	if (likely(time_before(jiffies, dmc->pid_expire_check)))
+		return;
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_WHITELIST);
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_BLACKLIST);
+	dmc->pid_expire_check = jiffies + (dmc->sysctl_pid_expiry_secs + 1) * HZ;
+}
+
+/*
+ * Is the IO cacheable, depending on global cacheability and the white/black
+ * lists ? This function is a bit confusing because we want to support inheritance
+ * of cacheability across pthreads (so we use the tgid). But when an entire thread
+ * group is added to the white/black list, we want to provide for exceptions for 
+ * individual threads as well.
+ * The Rules (in decreasing order of priority) :
+ * 1) Check the pid (thread id) against the list. 
+ * 2) Check the tgid against the list, then check for exceptions within the tgid.
+ * 3) Possibly don't cache sequential i/o.
+ */
+int
+flashcache_uncacheable(struct cache_c *dmc, struct bio *bio)
+{
+	int dontcache;
+	
+	if (dmc->sysctl_cache_all) {
+		/* If the tid has been blacklisted, we don't cache at all.
+		   This overrides everything else */
+		dontcache = flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_BLACKLIST);
+		if (dontcache)
+			goto out;
+		/* Is the tgid in the blacklist ? */
+		dontcache = flashcache_find_pid_locked(dmc, current->tgid, 
+						       FLASHCACHE_BLACKLIST);
+		/* 
+		 * If we found the tgid in the blacklist, is there a whitelist
+		 * exception entered for this thread ?
+		 */
+		if (dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_WHITELIST)) {
+				dontcache = 0;
+				goto out;
+			}
+		}
+
+		/* Finally, if we are neither in a whitelist or a blacklist,
+		 * do a final check to see if this is sequential i/o.  If
+		 * the relevant sysctl is set, we will skip it.
+		 */
+		dontcache = skip_sequential_io(dmc, bio);
+			
+	} else { /* cache nothing */
+		/* If the tid has been whitelisted, we cache 
+		   This overrides everything else */
+		dontcache = !flashcache_find_pid_locked(dmc, current->pid, 
+							FLASHCACHE_WHITELIST);
+		if (!dontcache)
+			goto out;
+		/* Is the tgid in the whitelist ? */
+		dontcache = !flashcache_find_pid_locked(dmc, current->tgid, 
+							FLASHCACHE_WHITELIST);
+		/* 
+		 * If we found the tgid in the whitelist, is there a black list 
+		 * exception entered for this thread ?
+		 */
+		if (!dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid, 
+						       FLASHCACHE_BLACKLIST))
+				dontcache = 1;
+		}
+		/* No sequential handling here.  If we add to the whitelist,
+		 * everything is cached, sequential or not.
+  		 */
+	}
+out:
+	return dontcache;
+}
+
+/* Below 2 functions manage the LRU cache of recent IO 'flows'.  
+ * A sequential IO will only take up one slot (we keep updating the 
+ * last sector seen) but random IO will quickly fill multiple slots.  
+ * We allocate the LRU cache from a small fixed sized buffer at startup. 
+ */
+void
+seq_io_remove_from_lru(struct cache_c *dmc, struct sequential_io *seqio)
+{
+	if (seqio->prev != NULL) 
+		seqio->prev->next = seqio->next;
+	else {
+		VERIFY(dmc->seq_io_head == seqio);
+		dmc->seq_io_head = seqio->next;
+	}
+	if (seqio->next != NULL)
+		seqio->next->prev = seqio->prev;
+	else {
+		VERIFY(dmc->seq_io_tail == seqio);
+		dmc->seq_io_tail = seqio->prev;
+	}
+}
+
+void
+seq_io_move_to_lruhead(struct cache_c *dmc, struct sequential_io *seqio)
+{
+	if (likely(seqio->prev != NULL || seqio->next != NULL))
+		seq_io_remove_from_lru(dmc, seqio);
+	/* Add it to LRU head */
+	if (dmc->seq_io_head != NULL)
+		dmc->seq_io_head->prev = seqio;
+	seqio->next = dmc->seq_io_head;
+	seqio->prev = NULL;
+	dmc->seq_io_head = seqio;
+}
+       
+
+/* Look for and maybe skip sequential i/o.  
+ *
+ * Since          performance(SSD) >> performance(HDD) for random i/o,
+ * but            performance(SSD) ~= performance(HDD) for sequential i/o,
+ * it may be optimal to save (presumably expensive) SSD cache space for random i/o only.
+ *
+ * We don't know whether a single request is part of a big sequential read/write.
+ * So all we can do is monitor a few requests, and try to spot if they are
+ * continuations of a recent 'flow' of i/o.  After several contiguous blocks we consider
+ * it sequential.
+ *
+ * You can tune the threshold with the sysctl skip_seq_thresh_kb (e.g. 64 = 64kb),
+ * or cache all i/o (without checking whether random or sequential) with skip_seq_thresh_kb = 0.
+ */
+int 
+skip_sequential_io(struct cache_c *dmc, struct bio *bio)
+{
+	struct sequential_io *seqio;
+	int sequential = 0;	/* Saw > 1 in a row? */
+	int skip       = 0;	/* Enough sequential to hit the threshold */
+
+	/* sysctl skip sequential threshold = 0 : disable, cache all sequential and random i/o.
+	 * This is the default. */	 
+	if (dmc->sysctl_skip_seq_thresh_kb == 0)  {
+		skip = 0;	/* Redundant, for emphasis */
+		goto out;
+	}
+
+	/* locking : We are already within cache_spin_lock so we don't
+	 * need to explicitly lock our data structures.
+ 	 */
+
+	/* Is it a continuation of recent i/o?  Try to find a match.  */
+	DPRINTK("skip_sequential_io: searching for %ld", bio->bi_sector);
+	/* search the list in LRU order so single sequential flow hits first slot */
+	for (seqio = dmc->seq_io_head; seqio != NULL && sequential == 0; seqio = seqio->next) { 
+
+		if (bio->bi_sector == seqio->most_recent_sector) {
+			/* Reread or write same sector again.  Ignore but move to head */
+			DPRINTK("skip_sequential_io: repeat");
+			sequential = 1;
+			if (dmc->seq_io_head != seqio)
+				seq_io_move_to_lruhead(dmc, seqio);
+		}
+		/* i/o to one block more than the previous i/o = sequential */	
+		else if (bio->bi_sector == seqio->most_recent_sector + dmc->block_size) {
+			DPRINTK("skip_sequential_io: sequential found");
+			/* Update stats.  */
+			seqio->most_recent_sector = bio->bi_sector;
+			seqio->sequential_count++;
+			sequential = 1;
+
+			/* And move to head, if not head already */
+			if (dmc->seq_io_head != seqio)
+				seq_io_move_to_lruhead(dmc, seqio);
+
+			/* Is it now sequential enough to be sure? (threshold expressed in kb) */
+			if (to_bytes(seqio->sequential_count * dmc->block_size) > dmc->sysctl_skip_seq_thresh_kb * 1024) {
+				DPRINTK("skip_sequential_io: Sequential i/o detected, seq count now %lu", 
+					seqio->sequential_count);
+				/* Sufficiently sequential */
+				skip = 1;
+			}
+		}
+	}
+	if (!sequential) {
+		/* Record the start of some new i/o, maybe we'll spot it as 
+		 * sequential soon.  */
+		DPRINTK("skip_sequential_io: concluded that its random i/o");
+
+		seqio = dmc->seq_io_tail;
+		seq_io_move_to_lruhead(dmc, seqio);
+
+		DPRINTK("skip_sequential_io: fill in data");
+
+		/* Fill in data */
+		seqio->most_recent_sector = bio->bi_sector;
+		seqio->sequential_count	  = 1;
+	}
+	DPRINTK("skip_sequential_io: complete.");
+out:
+	if (skip) {
+		if (bio_data_dir(bio) == READ)
+	        	dmc->flashcache_stats.uncached_sequential_reads++;
+		else 
+	        	dmc->flashcache_stats.uncached_sequential_writes++;
+	}
+
+	return skip;
+}
+
+
+/*
+ * Add/del pids whose IOs should be non-cacheable.
+ * We limit this number to 100 (arbitrary and sysctl'able).
+ * We also add an expiry to each entry (defaluts at 60 sec,
+ * arbitrary and sysctlable).
+ * This is needed because Linux lacks an "at_exit()" hook
+ * that modules can supply to do any cleanup on process 
+ * exit, for cases where the process dies after marking itself
+ * non-cacheable.
+ */
+int 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		 struct file *filp, unsigned int cmd,
+		 unsigned long arg)
+#else
+flashcache_ioctl(struct dm_target *ti, unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct block_device *bdev = dmc->disk_dev->bdev;
+	struct file fake_file = {};
+	struct dentry fake_dentry = {};
+	pid_t pid;
+
+	switch(cmd) {
+	case FLASHCACHEADDBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELALLBLACKLIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 0);
+		return 0;
+	case FLASHCACHEADDWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELALLWHITELIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 0);
+		return 0;
+	default:
+		fake_file.f_mode = dmc->disk_dev->mode;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+		fake_file.f_dentry = &fake_dentry;
+#else
+		fake_file.f_path.dentry = &fake_dentry;
+#endif
+		fake_dentry.d_inode = bdev->bd_inode;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+		return blkdev_driver_ioctl(bdev->bd_inode, &fake_file, bdev->bd_disk, cmd, arg);
+#else
+		return __blkdev_driver_ioctl(dmc->disk_dev->bdev, dmc->disk_dev->mode, cmd, arg);
+#endif
+	}
+
+}
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_ioctl.h linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_ioctl.h
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_ioctl.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_ioctl.h	2016-12-13 17:25:21.066077451 +0800
@@ -0,0 +1,70 @@
+/****************************************************************************
+ *  flashcache_ioctl.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_IOCTL_H
+#define FLASHCACHE_IOCTL_H
+
+#include <linux/types.h>
+
+#define FLASHCACHE_IOCTL 0xfe
+
+enum {
+	FLASHCACHEADDNCPID_CMD=200,
+	FLASHCACHEDELNCPID_CMD,
+	FLASHCACHEDELNCALL_CMD,
+	FLASHCACHEADDWHITELIST_CMD,
+	FLASHCACHEDELWHITELIST_CMD,
+	FLASHCACHEDELWHITELISTALL_CMD,
+};
+
+#define FLASHCACHEADDNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCALL	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCALL_CMD, pid_t)
+
+#define FLASHCACHEADDBLACKLIST		FLASHCACHEADDNCPID
+#define FLASHCACHEDELBLACKLIST		FLASHCACHEDELNCPID
+#define FLASHCACHEDELALLBLACKLIST	FLASHCACHEDELNCALL
+
+#define FLASHCACHEADDWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELALLWHITELIST	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELISTALL_CMD, pid_t)
+
+#ifdef __KERNEL__
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+int flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		     struct file *filp, unsigned int cmd,
+		     unsigned long arg);
+#else
+int flashcache_ioctl(struct dm_target *ti, unsigned int cmd,
+ 		     unsigned long arg);
+#endif
+void flashcache_pid_expiry_all_locked(struct cache_c *dmc);
+int flashcache_uncacheable(struct cache_c *dmc, struct bio *bio);
+void seq_io_remove_from_lru(struct cache_c *dmc, struct sequential_io *seqio);
+void seq_io_move_to_lruhead(struct cache_c *dmc, struct sequential_io *seqio);
+int skip_sequential_io(struct cache_c *dmc, struct bio *bio);
+void flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force);
+#endif /* __KERNEL__ */
+
+#endif
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_main.c linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_main.c
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_main.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_main.c	2016-12-13 17:25:21.068077248 +0800
@@ -0,0 +1,2051 @@
+/****************************************************************************
+ *  flashcache_main.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,21)
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#endif
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+#ifndef DM_MAPIO_SUBMITTED
+#define DM_MAPIO_SUBMITTED	0
+#endif
+
+/*
+ * TODO List :
+ * 1) Management of non cache pids : Needs improvement. Remove registration
+ * on process exits (with  a pseudo filesstem'ish approach perhaps) ?
+ * 2) Breaking up the cache spinlock : Right now contention on the spinlock
+ * is not a problem. Might need change in future.
+ * 3) Use the standard linked list manipulation macros instead rolling our own.
+ * 4) Fix a security hole : A malicious process with 'ro' access to a file can 
+ * potentially corrupt file data. This can be fixed by copying the data on a
+ * cache read miss.
+ */
+
+#define FLASHCACHE_SW_VERSION "flashcache-1.0"
+char *flashcache_sw_version = FLASHCACHE_SW_VERSION;
+
+static void flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+				 int index);
+static void flashcache_write(struct cache_c *dmc, struct bio* bio);
+static int flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio);
+static void flashcache_dirty_writeback(struct cache_c *dmc, int index);
+void flashcache_sync_blocks(struct cache_c *dmc);
+static void flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio);
+
+extern struct work_struct _kcached_wq;
+extern u_int64_t size_hist[];
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+extern struct dm_kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#else
+extern struct kcopyd_client *flashcache_kcp_client; /* Kcopyd client for writing back data */
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+extern struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions, 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where, 
+#else
+			    struct io_region *where, 
+#endif
+			    int rw, 
+			    struct bio_vec *bvec, io_notify_fn fn, 
+			    void *context)
+{
+	struct dm_io_request iorq;
+
+	iorq.bi_rw = rw;
+	iorq.mem.type = DM_IO_BVEC;
+	iorq.mem.ptr.bvec = bvec;
+	iorq.notify.fn = fn;
+	iorq.notify.context = context;
+	iorq.client = flashcache_io_client;
+	return dm_io(&iorq, num_regions, where, NULL);
+}
+#endif
+
+/* 
+ * A simple 2-hand clock like algorithm is used to identify dirty blocks 
+ * that lie fallow in the cache and thus are candidates for cleaning. 
+ * Note that we could have such fallow blocks in sets where the dirty blocks 
+ * is under the configured threshold.
+ * The hands are spaced fallow_delay seconds apart (one sweep runs every 
+ * fallow_delay seconds).  The interval is configurable via a sysctl. 
+ * Blocks are moved to DIRTY_FALLOW_1, if they are found to be in DIRTY_FALLOW_1
+ * for fallow_delay seconds or more, they are moved to DIRTY_FALLOW_1 | DIRTY_FALLOW_2, 
+ * at which point they are eligible for cleaning. Of course any intervening use
+ * of the block within the interval turns off these 2 bits.
+ * 
+ * Cleaning of these blocks happens from the flashcache_clean_set() function.
+ */
+void
+flashcache_detect_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	if ((cacheblk->cache_state & DIRTY) &&
+	    ((cacheblk->cache_state & BLOCK_IO_INPROG) == 0)) {
+		if ((cacheblk->cache_state & DIRTY_FALLOW_1) == 0)
+			cacheblk->cache_state |= DIRTY_FALLOW_1;
+		else if ((cacheblk->cache_state & DIRTY_FALLOW_2) == 0) {
+			dmc->cache_sets[index / dmc->assoc].dirty_fallow++;
+			cacheblk->cache_state |= DIRTY_FALLOW_2;
+		}
+	}
+}
+
+void
+flashcache_clear_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int set = index / dmc->assoc;
+	
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	if (cacheblk->cache_state & FALLOW_DOCLEAN) {
+		if (cacheblk->cache_state & DIRTY_FALLOW_2) {
+			VERIFY(dmc->cache_sets[set].dirty_fallow > 0);
+			dmc->cache_sets[set].dirty_fallow--;
+		}
+		cacheblk->cache_state &= ~FALLOW_DOCLEAN;
+	}
+}
+
+void 
+flashcache_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+	struct cache_c *dmc = job->dmc;
+	struct bio *bio;
+	unsigned long flags;
+	int index = job->index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	VERIFY(index != -1);		
+	bio = job->bio;
+	VERIFY(bio != NULL);
+	if (unlikely(error)) {
+		error = -EIO;
+		DMERR("flashcache_io_callback: io error %ld block %lu action %d", 
+		      error, job->job_io_regions.disk.sector, job->action);
+	}
+	job->error = error;
+	switch (job->action) {
+	case READDISK:
+		DPRINTK("flashcache_io_callback: READDISK  %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READDISK_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READDISK_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (likely(error == 0)) {
+			/* Kick off the write to the cache */
+			job->action = READFILL;
+			push_io(job);
+			schedule_work(&_kcached_wq);
+			return;
+		} else
+			dmc->flashcache_errors.disk_read_errors++;			
+		break;
+	case READCACHE:
+		DPRINTK("flashcache_io_callback: READCACHE %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READCACHE_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READCACHE_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & CACHEREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_read_errors++;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		if (likely(error == 0)) {
+			if (flashcache_validate_checksum(job)) {
+				DMERR("flashcache_io_callback: Checksum mismatch at disk offset %lu", 
+				      job->job_io_regions.disk.sector);
+				error = -EIO;
+			}
+		}
+#endif
+		break;		       
+	case READFILL:
+		DPRINTK("flashcache_io_callback: READFILL %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(dmc->sysctl_error_inject & READFILL_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~READFILL_ERROR;
+		}
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_write_errors++;
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		break;
+	case WRITECACHE:
+		DPRINTK("flashcache_io_callback: WRITECACHE %d",
+			index);
+		if (unlikely(dmc->sysctl_error_inject & WRITECACHE_ERROR)) {
+			job->error = error = -EIO;
+			dmc->sysctl_error_inject &= ~WRITECACHE_ERROR;
+		}
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		VERIFY(cacheblk->cache_state & CACHEWRITEINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (likely(error == 0)) {
+			if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				dmc->flashcache_stats.checksum_store++;
+				flashcache_store_checksum(job);
+				/* 
+				 * We need to update the metadata on a DIRTY->DIRTY as well 
+				 * since we save the checksums.
+				 */
+				flashcache_md_write(job);
+				return;
+#else
+				/* Only do cache metadata update on a non-DIRTY->DIRTY transition */
+				if ((cacheblk->cache_state & DIRTY) == 0) {
+					flashcache_md_write(job);
+					return;
+				}
+#endif
+			} else { /* cache_mode == WRITE_THROUGH */
+				/* Writs to both disk and cache completed */
+				VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				flashcache_store_checksum(job);
+				job->dmc->flashcache_stats.checksum_store++;
+#endif
+			}
+		} else {
+			dmc->flashcache_errors.ssd_write_errors++;
+			if (dmc->cache_mode == FLASHCACHE_WRITE_THROUGH)
+				/* 
+				 * We don't know if the IO failed because of a ssd write
+				 * error or a disk write error. Bump up both.
+				 * XXX - TO DO. We could check the error bits and allow
+				 * the IO to succeed as long as the disk write suceeded.
+				 * and invalidate the cache block.
+				 */
+				dmc->flashcache_errors.disk_write_errors++;
+		}
+		break;
+	}
+	flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+	/* 
+	 * The INPROG flag is still set. We cannot turn that off until all the pending requests
+	 * processed. We need to loop the pending requests back to a workqueue. We have the job,
+	 * add it to the pending req queue.
+	 */
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (unlikely(error || cacheblk->nr_queued > 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		push_pending(job);
+		schedule_work(&_kcached_wq);
+	} else {
+		cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_free_cache_job(job);
+		if (atomic_dec_and_test(&dmc->nr_jobs))
+			wake_up(&dmc->destroyq);
+	}
+}
+
+static void
+flashcache_free_pending_jobs(struct cache_c *dmc, struct cacheblock *cacheblk, 
+			     int error)
+{
+	struct pending_job *pending_job, *freelist = NULL;
+
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	freelist = flashcache_deq_pending(dmc, cacheblk - &dmc->cache[0]);
+	while (freelist != NULL) {
+		pending_job = freelist;
+		freelist = pending_job->next;
+		VERIFY(cacheblk->nr_queued > 0);
+		cacheblk->nr_queued--;
+		flashcache_bio_endio(pending_job->bio, error, dmc, NULL);
+		flashcache_free_pending_job(pending_job);
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+}
+
+/* 
+ * Common error handling for everything.
+ * 1) If the block isn't dirty, invalidate it.
+ * 2) Error all pending IOs that totally or partly overlap this block.
+ * 3) Free the job.
+ */
+static void
+flashcache_do_pending_error(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[job->index];
+
+	DMERR("flashcache_do_pending_error: error %d block %lu action %d", 
+	      job->error, job->job_io_regions.disk.sector, job->action);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(cacheblk->cache_state & VALID);
+	/* Invalidate block if possible */
+	if ((cacheblk->cache_state & DIRTY) == 0) {
+		dmc->cached_blocks--;
+		dmc->flashcache_stats.pending_inval++;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+	}
+	flashcache_free_pending_jobs(dmc, cacheblk, job->error);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void
+flashcache_do_pending_noerror(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+	struct pending_job *pending_job, *freelist;
+	int queued;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (cacheblk->cache_state & DIRTY) {
+		VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_BACK);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, index);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_dirty_writeback(dmc, index);
+		goto out;
+	}
+	DPRINTK("flashcache_do_pending: Index %d %lx",
+		index, cacheblk->cache_state);
+	VERIFY(cacheblk->cache_state & VALID);
+	dmc->cached_blocks--;
+	dmc->flashcache_stats.pending_inval++;
+	cacheblk->cache_state &= ~VALID;
+	cacheblk->cache_state |= INVALID;
+	while ((freelist = flashcache_deq_pending(dmc, index)) != NULL) {
+		while (freelist != NULL) {
+			VERIFY(!(cacheblk->cache_state & DIRTY));
+			pending_job = freelist;
+			freelist = pending_job->next;
+			VERIFY(cacheblk->nr_queued > 0);
+			cacheblk->nr_queued--;
+			if (pending_job->action == INVALIDATE) {
+				DPRINTK("flashcache_do_pending: INVALIDATE  %llu",
+					pending_job->bio->bi_sector);
+				VERIFY(pending_job->bio != NULL);
+				queued = flashcache_inval_blocks(dmc, pending_job->bio);
+				if (queued) {
+					if (unlikely(queued < 0)) {
+						/*
+						 * Memory allocation failure inside inval_blocks.
+						 * Fail this io.
+						 */
+						flashcache_bio_endio(pending_job->bio, -EIO, dmc, NULL);
+					}
+					flashcache_free_pending_job(pending_job);
+					continue;
+				}
+			}
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			DPRINTK("flashcache_do_pending: Sending down IO %llu",
+				pending_job->bio->bi_sector);
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, pending_job->bio);
+			flashcache_free_pending_job(pending_job);
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+out:
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+void
+flashcache_do_pending(struct kcached_job *job)
+{
+	if (job->error)
+		flashcache_do_pending_error(job);
+	else
+		flashcache_do_pending_noerror(job);
+}
+
+void
+flashcache_do_io(struct kcached_job *job)
+{
+	struct bio *bio = job->bio;
+	int r = 0;
+	
+	VERIFY(job->action == READFILL);
+	VERIFY(job->action == READFILL);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	flashcache_store_checksum(job);
+	job->dmc->flashcache_stats.checksum_store++;
+#endif
+	/* Write to cache device */
+	job->dmc->flashcache_stats.ssd_writes++;
+	r = dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, bio->bi_io_vec + bio->bi_idx,
+			     flashcache_io_callback, job);
+	VERIFY(r == 0);
+	/* In our case, dm_io_async_bvec() must always return 0 */
+}
+
+/*
+ * Map a block from the source device to a block in the cache device.
+ */
+static unsigned long 
+hash_block(struct cache_c *dmc, sector_t dbn)
+{
+	unsigned long set_number, value;
+
+	value = (unsigned long)
+		(dbn >> (dmc->block_shift + dmc->assoc_shift));
+	set_number = value % dmc->num_sets;
+	DPRINTK("Hash: %llu(%lu)->%lu", dbn, value, set_number);
+	return set_number;
+}
+
+static void
+find_valid_dbn(struct cache_c *dmc, sector_t dbn, 
+	       int start_index, int *valid, int *invalid)
+{
+	int i;
+	int end_index = start_index + dmc->assoc;
+
+	*valid = *invalid = -1;
+	for (i = start_index ; i < end_index ; i++) {
+		if (dbn == dmc->cache[i].dbn &&
+		    (dmc->cache[i].cache_state & VALID)) {
+			*valid = i;
+			if (dmc->sysctl_reclaim_policy == FLASHCACHE_LRU &&
+			    ((dmc->cache[i].cache_state & BLOCK_IO_INPROG) == 0))
+				flashcache_reclaim_lru_movetail(dmc, i);
+			/* 
+			 * If the block was DIRTY and earmarked for cleaning because it was old, make 
+			 * the block young again.
+			 */
+			flashcache_clear_fallow(dmc, i);
+			return;
+		}
+		if (*invalid == -1 && dmc->cache[i].cache_state == INVALID) {
+			VERIFY((dmc->cache[i].cache_state & FALLOW_DOCLEAN) == 0);
+			*invalid = i;
+		}
+	}
+	if (*valid == -1 && *invalid != -1)
+		if (dmc->sysctl_reclaim_policy == FLASHCACHE_LRU)
+			flashcache_reclaim_lru_movetail(dmc, *invalid);
+}
+
+/* Search for a slot that we can reclaim */
+static void
+find_reclaim_dbn(struct cache_c *dmc, int start_index, int *index)
+{
+	int set = start_index / dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+	
+	if (dmc->sysctl_reclaim_policy == FLASHCACHE_FIFO) {
+		int end_index = start_index + dmc->assoc;
+		int slots_searched = 0;
+		int i;
+
+		i = cache_set->set_fifo_next;
+		while (slots_searched < dmc->assoc) {
+			VERIFY(i >= start_index);
+			VERIFY(i < end_index);
+			if (dmc->cache[i].cache_state == VALID) {
+				*index = i;
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				break;
+			}
+			slots_searched++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		i++;
+		if (i == end_index)
+			i = start_index;
+		cache_set->set_fifo_next = i;
+	} else { /* reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if (cacheblk->cache_state == VALID) {
+				VERIFY((cacheblk - &dmc->cache[0]) == 
+				       (lru_rel_index + start_index));
+				*index = cacheblk - &dmc->cache[0];
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				flashcache_reclaim_lru_movetail(dmc, *index);
+				break;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+}
+
+/* 
+ * dbn is the starting sector, io_size is the number of sectors.
+ */
+static int 
+flashcache_lookup(struct cache_c *dmc, struct bio *bio, int *index)
+{
+	sector_t dbn = bio->bi_sector;
+#if DMC_DEBUG
+	int io_size = to_sector(bio->bi_size);
+#endif
+	unsigned long set_number = hash_block(dmc, dbn);
+	int invalid, oldest_clean = -1;
+	int start_index;
+
+	start_index = dmc->assoc * set_number;
+	DPRINTK("Cache lookup : dbn %llu(%lu), set = %d",
+		dbn, io_size, set_number);
+	find_valid_dbn(dmc, dbn, start_index, index, &invalid);
+	if (*index >= 0) {
+		DPRINTK("Cache lookup HIT: Block %llu(%lu): VALID index %d",
+			     dbn, io_size, *index);
+		/* We found the exact range of blocks we are looking for */
+		return VALID;
+	}
+	if (invalid == -1) {
+		/* We didn't find an invalid entry, search for oldest valid entry */
+		find_reclaim_dbn(dmc, start_index, &oldest_clean);
+	}
+	/* 
+	 * Cache miss :
+	 * We can't choose an entry marked INPROG, but choose the oldest
+	 * INVALID or the oldest VALID entry.
+	 */
+	*index = start_index + dmc->assoc;
+	if (invalid != -1) {
+		DPRINTK("Cache lookup MISS (INVALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, invalid, start_index);
+		*index = invalid;
+	} else if (oldest_clean != -1) {
+		DPRINTK("Cache lookup MISS (VALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, oldest_clean, start_index);
+		*index = oldest_clean;
+	} else {
+		DPRINTK_LITE("Cache read lookup MISS (NOROOM): dbn %llu(%lu), set = %d",
+			dbn, io_size, set_number);
+	}
+	if (*index < (start_index + dmc->assoc))
+		return INVALID;
+	else {
+		dmc->flashcache_stats.noroom++;
+		return -1;
+	}
+}
+
+/*
+ * Cache Metadata Update functions 
+ */
+void 
+flashcache_md_write_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_md_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static int
+flashcache_alloc_md_sector(struct kcached_job *job)
+{
+	struct page *page = NULL;
+	struct cache_c *dmc = job->dmc;	
+	
+	if (likely((dmc->sysctl_error_inject & MD_ALLOC_SECTOR_ERROR) == 0)) {
+		unsigned long addr;
+
+		/* Get physically consecutive pages */
+		addr = __get_free_pages(GFP_NOIO, get_order(MD_BLOCK_BYTES(job->dmc)));
+		if (addr)
+			page = virt_to_page(addr);
+	} else
+		dmc->sysctl_error_inject &= ~MD_ALLOC_SECTOR_ERROR;
+	job->md_io_bvec.bv_page = page;
+	if (unlikely(page == NULL)) {
+		job->dmc->flashcache_errors.memory_alloc_errors++;
+		return -ENOMEM;
+	}
+	job->md_io_bvec.bv_len = MD_BLOCK_BYTES(job->dmc);
+	job->md_io_bvec.bv_offset = 0;
+	job->md_block = (struct flash_cacheblock *)page_address(page);
+	return 0;
+}
+
+static void
+flashcache_free_md_sector(struct kcached_job *job)
+{
+	if (job->md_io_bvec.bv_page != NULL)
+		__free_pages(job->md_io_bvec.bv_page, get_order(MD_BLOCK_BYTES(job->dmc)));
+}
+
+void
+flashcache_md_write_kickoff(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;	
+	struct flash_cacheblock *md_block;
+	int md_block_ix;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i;
+	struct cache_md_block_head *md_block_head;
+	struct kcached_job *orig_job = job;
+	unsigned long flags;
+
+	if (flashcache_alloc_md_sector(job)) {
+		DMERR("flashcache: %d: Cache metadata write failed, cannot alloc page ! block %lu", 
+		      job->action, job->job_io_regions.disk.sector);
+		flashcache_md_write_callback(-EIO, job);
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/*
+	 * Transfer whatever is on the pending queue to the md_io_inprog queue.
+	 */
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	md_block_head->md_io_inprog = md_block_head->queued_updates;
+	md_block_head->queued_updates = NULL;
+	md_block = job->md_block;
+	md_block_ix = INDEX_TO_MD_BLOCK(dmc, job->index) * MD_SLOTS_PER_BLOCK(dmc);
+	/* First copy out the entire md block */
+	for (i = 0 ; 
+	     i < MD_SLOTS_PER_BLOCK(dmc) && md_block_ix < dmc->size ; 
+	     i++, md_block_ix++) {
+		md_block[i].dbn = dmc->cache[md_block_ix].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		md_block[i].checksum = dmc->cache[md_block_ix].checksum;
+#endif
+		md_block[i].cache_state = 
+			dmc->cache[md_block_ix].cache_state & (VALID | INVALID | DIRTY);
+	}
+	/* Then set/clear the DIRTY bit for the "current" index */
+	if (job->action == WRITECACHE) {
+		/* DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = 
+			(VALID | DIRTY);
+	} else { /* job->action == WRITEDISK* */
+		/* un-DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+	}
+
+	for (job = md_block_head->md_io_inprog ; 
+	     job != NULL ;
+	     job = job->next) {
+		dmc->flashcache_stats.md_write_batch++;
+		if (job->action == WRITECACHE) {
+			/* DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = 
+				(VALID | DIRTY);
+		} else { /* job->action == WRITEDISK* */
+			/* un-DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+		}
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	where.bdev = dmc->cache_dev->bdev;
+	where.count = MD_SECTORS_PER_BLOCK(dmc);
+	where.sector = (1 + INDEX_TO_MD_BLOCK(dmc, orig_job->index)) * MD_SECTORS_PER_BLOCK(dmc);
+	dmc->flashcache_stats.ssd_writes++;
+	dmc->flashcache_stats.md_ssd_writes++;
+	dm_io_async_bvec(1, &where, WRITE,
+			 &orig_job->md_io_bvec,
+			 flashcache_md_write_callback, orig_job);
+}
+
+void
+flashcache_md_write_done(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	int index;
+	unsigned long flags;
+	struct kcached_job *job_list;
+	int error = job->error;
+	struct kcached_job *next;
+	struct cacheblock *cacheblk;
+		
+	VERIFY(!in_interrupt());
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE || 
+	       job->action == WRITEDISK_SYNC);
+	flashcache_free_md_sector(job);
+	job->md_block = NULL;
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	job_list = job;
+	job->next = md_block_head->md_io_inprog;
+	md_block_head->md_io_inprog = NULL;
+	for (job = job_list ; job != NULL ; job = next) {
+		next = job->next;
+		job->error = error;
+		index = job->index;
+		cacheblk = &dmc->cache[index];
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (job->action == WRITECACHE) {
+			if (unlikely(dmc->sysctl_error_inject & WRITECACHE_MD_ERROR)) {
+				job->error = -EIO;
+				dmc->sysctl_error_inject &= ~WRITECACHE_MD_ERROR;
+			}
+			if (likely(job->error == 0)) {
+				if ((cacheblk->cache_state & DIRTY) == 0) {
+					dmc->cache_sets[index / dmc->assoc].nr_dirty++;
+					dmc->nr_dirty++;
+				}
+				dmc->flashcache_stats.md_write_dirty++;
+				cacheblk->cache_state |= DIRTY;
+			} else
+				dmc->flashcache_errors.ssd_write_errors++;
+			flashcache_bio_endio(job->bio, job->error, dmc, &job->io_start_time);
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: WRITE: Cache metadata write failed ! error %d block %lu", 
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+			}
+		} else {
+			int action = job->action;
+
+			if (unlikely(dmc->sysctl_error_inject & WRITEDISK_MD_ERROR)) {
+				job->error = -EIO;
+				dmc->sysctl_error_inject &= ~WRITEDISK_MD_ERROR;
+			}
+			/*
+			 * If we have an error on a WRITEDISK*, no choice but to preserve the 
+			 * dirty block in cache. Fail any IOs for this block that occurred while
+			 * the block was being cleaned.
+			 */
+			if (likely(job->error == 0)) {
+				dmc->flashcache_stats.md_write_clean++;
+				cacheblk->cache_state &= ~DIRTY;
+				VERIFY(dmc->cache_sets[index / dmc->assoc].nr_dirty > 0);
+				VERIFY(dmc->nr_dirty > 0);
+				dmc->cache_sets[index / dmc->assoc].nr_dirty--;
+				dmc->nr_dirty--;
+			} else 
+				dmc->flashcache_errors.ssd_write_errors++;
+			VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+			VERIFY(dmc->clean_inprog > 0);
+			dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+			dmc->clean_inprog--;
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: CLEAN: Cache metadata write failed ! error %d block %lu", 
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+			}
+			/* Kick off more cleanings */
+			if (action == WRITEDISK)
+				flashcache_clean_set(dmc, index / dmc->assoc);
+			else
+				flashcache_sync_blocks(dmc);
+			dmc->flashcache_stats.cleanings++;
+			if (action == WRITEDISK_SYNC)
+				flashcache_update_sync_progress(dmc);
+		}
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (md_block_head->queued_updates != NULL) {
+		/* peel off the first job from the pending queue and kick that off */
+		job = md_block_head->queued_updates;
+		md_block_head->queued_updates = job->next;
+		job->next = NULL;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		VERIFY(job->action == WRITEDISK || job->action == WRITECACHE ||
+		       job->action == WRITEDISK_SYNC);
+		flashcache_md_write_kickoff(job);
+	} else {
+		md_block_head->nr_in_prog = 0;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	}
+}
+
+/* 
+ * Kick off a cache metadata update (called from workqueue).
+ * Cache metadata update IOs to a given metadata sector are serialized using the 
+ * nr_in_prog bit in the md sector bufhead.
+ * If a metadata IO is already in progress, we queue up incoming metadata updates
+ * on the pending_jobs list of the md sector bufhead. When kicking off an IO, we
+ * cluster all these pending updates and do all of them as 1 flash write (that 
+ * logic is in md_write_kickoff), where it switches out the entire pending_jobs
+ * list and does all of those updates as 1 ssd write.
+ */
+void
+flashcache_md_write(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	unsigned long flags;
+	
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE || 
+	       job->action == WRITEDISK_SYNC);
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/* If a write is in progress for this metadata sector, queue this update up */
+	if (md_block_head->nr_in_prog != 0) {
+		struct kcached_job **nodepp;
+		
+		/* A MD update is already in progress, queue this one up for later */
+		nodepp = &md_block_head->queued_updates;
+		while (*nodepp != NULL)
+			nodepp = &((*nodepp)->next);
+		job->next = NULL;
+		*nodepp = job;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	} else {
+		md_block_head->nr_in_prog = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/*
+		 * Always push to a worker thread. If the driver has
+		 * a completion thread, we could end up deadlocking even
+		 * if the context would be safe enough to write from.
+		 * This could be executed from the context of an IO 
+		 * completion thread. Kicking off the write from that
+		 * context could result in the IO completion thread 
+		 * blocking (eg on memory allocation). That can easily
+		 * deadlock.
+		 */
+		push_md_io(job);
+		schedule_work(&_kcached_wq);
+	}
+}
+
+static void 
+flashcache_kcopyd_callback(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (unlikely(dmc->sysctl_error_inject & KCOPYD_CALLBACK_ERROR)) {
+		read_err = -EIO;
+		dmc->sysctl_error_inject &= ~KCOPYD_CALLBACK_ERROR;
+	}
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this block from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu", 
+		      -read_err, -write_err, job->job_io_regions.disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_clean_set(dmc, index / dmc->assoc); /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+	
+	DPRINTK("flashcache_dirty_writeback: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	if (unlikely(dmc->sysctl_error_inject & DIRTY_WRITEBACK_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~DIRTY_WRITEBACK_JOB_ALLOC_FAIL;
+	}
+	/*
+	 * If the device is being removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog))) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu", 
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for set cleaning) failed ! Can't allocate memory, block %lu", 
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback, 
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback, 
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+			       (dm_kcopyd_notify_fn) flashcache_kcopyd_callback, 
+			       (void *)job);
+#endif
+	}
+}
+
+/*
+ * This function encodes the background disk cleaning logic.
+ * Background disk cleaning is triggered for 2 reasons.
+ A) Dirty blocks are lying fallow in the set, making them good 
+    candidates for being cleaned.
+ B) This set has dirty blocks over the configured threshold 
+    for a set.
+ * (A) takes precedence over (B). Fallow dirty blocks are cleaned
+ * first.
+ * The cleaning of disk blocks is subject to the write limits per
+ * set and across the cache, which this function enforces.
+ *
+ * 1) Select the n blocks that we want to clean (choosing whatever policy), 
+ *    sort them.
+ * 2) Then sweep the entire set looking for other DIRTY blocks that can be 
+ *    tacked onto any of these blocks to form larger contigous writes. 
+ *    The idea here is that if you are going to do a write anyway, then we 
+ *    might as well opportunistically write out any contigous blocks for 
+ *    free.
+ */
+
+/* Are we under the limits for disk cleaning ? */
+static inline int
+flashcache_can_clean(struct cache_c *dmc, 
+		     struct cache_set *cache_set,
+		     int nr_writes)
+{
+	return ((cache_set->clean_inprog + nr_writes) < dmc->max_clean_ios_set &&
+		(nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total);
+}
+
+void
+flashcache_clean_set(struct cache_c *dmc, int set)
+{
+	unsigned long flags;
+	int threshold_clean = 0;
+	struct dbn_index_pair *writes_list;
+	int nr_writes = 0, i;
+	int start_index = set * dmc->assoc; 
+	int end_index = start_index + dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+	int do_delayed_clean = 0;
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	/* 
+	 * If a removal of this device is in progress, don't kick off 
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if (atomic_read(&dmc->remove_in_prog))
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (unlikely(dmc->sysctl_error_inject & WRITES_LIST_ALLOC_FAIL)) {
+		if (writes_list)
+			kfree(writes_list);
+		writes_list = NULL;
+		dmc->sysctl_error_inject &= ~WRITES_LIST_ALLOC_FAIL;
+	}
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/* 
+	 * Before we try to clean any blocks, check the last time the fallow block
+	 * detection was done. If it has been more than "fallow_delay" seconds, make 
+	 * a sweep through the set to detect (mark) fallow blocks.
+	 */
+	if (dmc->sysctl_fallow_delay && time_after(jiffies, cache_set->fallow_tstamp)) {
+		for (i = start_index ; i < end_index ; i++)
+			flashcache_detect_fallow(dmc, i);
+		cache_set->fallow_tstamp = jiffies + dmc->sysctl_fallow_delay * HZ;
+	}
+	/* If there are any dirty fallow blocks, clean them first */
+	for (i = start_index ; 
+	     (dmc->sysctl_fallow_delay > 0 &&
+	      cache_set->dirty_fallow > 0 &&
+	      time_after(jiffies, cache_set->fallow_next_cleaning) &&
+	      i < end_index) ; 
+	     i++) {
+		cacheblk = &dmc->cache[i];
+		if (!(cacheblk->cache_state & DIRTY_FALLOW_2))
+			continue;
+		if (!flashcache_can_clean(dmc, cache_set, nr_writes)) {
+			/*
+			 * There are fallow blocks that need cleaning, but we 
+			 * can't clean them this pass, schedule delayed cleaning 
+			 * later.
+			 */
+			do_delayed_clean = 1;
+			goto out;
+		}
+		VERIFY(cacheblk->cache_state & DIRTY);
+		VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == 0);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, i);
+		writes_list[nr_writes].dbn = cacheblk->dbn;
+		writes_list[nr_writes].index = i;
+		dmc->flashcache_stats.fallow_cleanings++;
+		nr_writes++;
+	}
+	if (nr_writes > 0)
+		cache_set->fallow_next_cleaning = jiffies + HZ / dmc->sysctl_fallow_clean_speed;
+	if (cache_set->nr_dirty < dmc->dirty_thresh_set ||
+	    !flashcache_can_clean(dmc, cache_set, nr_writes))
+		goto out;
+	/*
+	 * We picked up all the dirty fallow blocks we can. We can still clean more to 
+	 * remain under the dirty threshold. Clean some more blocks.
+	 */
+	threshold_clean = cache_set->nr_dirty - dmc->dirty_thresh_set;
+	if (dmc->sysctl_reclaim_policy == FLASHCACHE_FIFO) {
+		int scanned;
+		
+		scanned = 0;
+		i = cache_set->set_clean_next;
+		DPRINTK("flashcache_clean_set: Set %d", set);
+		while (scanned < dmc->assoc &&
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[i];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {	
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = i;
+				nr_writes++;
+			}
+			scanned++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		cache_set->set_clean_next = i;
+	} else { /* reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL && 
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = cacheblk - &dmc->cache[0];
+				nr_writes++;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+out:
+	if (nr_writes > 0) {
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		dmc->flashcache_stats.clean_set_ios += nr_writes;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback(dmc, writes_list[i].index);
+	} else {
+		if (cache_set->nr_dirty > dmc->dirty_thresh_set)
+			do_delayed_clean = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (do_delayed_clean)
+			schedule_delayed_work(&dmc->delayed_clean, 1*HZ);
+	}
+	kfree(writes_list);
+}
+
+static void
+flashcache_read_hit(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+
+	cacheblk = &dmc->cache[index];
+	/* If block is busy, queue IO pending completion of in-progress IO */
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		struct kcached_job *job;
+			
+		cacheblk->cache_state |= CACHEREADINPROG;
+		dmc->flashcache_stats.read_hits++;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+			bio->bi_sector, bio->bi_size, index, "CACHE HIT");
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(dmc->sysctl_error_inject & READ_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			dmc->sysctl_error_inject &= ~READ_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/* 
+			 * We have a read hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any 
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Read (hit) failed ! Can't allocate memory for cache IO, block %lu", 
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			job->action = READCACHE; /* Fetch data from cache */
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_reads++;
+			dm_io_async_bvec(1, &job->job_io_regions.cache, READ,
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(dmc->sysctl_error_inject & READ_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			dmc->sysctl_error_inject &= ~READ_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (pjob == NULL)
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, READCACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+		     int index)
+{
+	struct kcached_job *job;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(dmc->sysctl_error_inject & READ_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~READ_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/* 
+		 * We have a read miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any 
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Read (miss) failed ! Can't allocate memory for cache IO, block %lu", 
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		job->action = READDISK; /* Fetch data from the source device */
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.disk_reads++;
+		dm_io_async_bvec(1, &job->job_io_regions.disk, READ,
+				 bio->bi_io_vec + bio->bi_idx,
+				 flashcache_io_callback, job);
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_read(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+	
+	DPRINTK("Got a %s for %llu (%u bytes)",
+	        (bio_rw(bio) == READ ? "READ":"READA"), 
+		bio->bi_sector, bio->bi_size);
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	/* Cache Read Hit case */
+	if (res > 0) {
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & VALID) && 
+		    (cacheblk->dbn == bio->bi_sector)) {
+			flashcache_read_hit(dmc, bio, index);
+			return;
+		}
+	}
+	/*
+	 * In all cases except for a cache hit (and VALID), test for potential 
+	 * invalidations that we need to do.
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+
+	if (res == -1 || flashcache_uncacheable(dmc, bio)) {
+		/* No room , non-cacheable or sequential i/o means not wanted in cache */
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu):%s",
+			bio->bi_sector, bio->bi_size, "CACHE MISS & NO ROOM");
+		if (res == -1)
+			flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+		/* Start uncached IO */
+		flashcache_start_uncached_io(dmc, bio);
+		return;
+	}
+	/* 
+	 * (res == INVALID) Cache Miss 
+	 * And we found cache blocks to replace
+	 * Claim the cache blocks before giving up the spinlock
+	 */
+	if (dmc->cache[index].cache_state & VALID)
+		dmc->flashcache_stats.replace++;
+	else
+		dmc->cached_blocks++;
+	dmc->cache[index].cache_state = VALID | DISKREADINPROG;
+	dmc->cache[index].dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+
+	DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+		bio->bi_sector, bio->bi_size, index, "CACHE MISS & REPLACE");
+	flashcache_read_miss(dmc, bio, index);
+}
+
+/*
+ * Invalidate any colliding blocks if they are !BUSY and !DIRTY. If the colliding
+ * block is DIRTY, we need to kick off a write. In both cases, we need to wait 
+ * until the underlying IO is finished, and then proceed with the invalidation.
+ */
+static int
+flashcache_inval_block_set(struct cache_c *dmc, int set, struct bio *bio, int rw,
+			   struct pending_job *pjob)
+{
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_index, end_index, i;
+	struct cacheblock *cacheblk;
+	
+	start_index = dmc->assoc * set;
+	end_index = start_index + dmc->assoc;
+	for (i = start_index ; i < end_index ; i++) {
+		sector_t start_dbn = dmc->cache[i].dbn;
+		sector_t end_dbn = start_dbn + dmc->block_size;
+		
+		cacheblk = &dmc->cache[i];
+		if (cacheblk->cache_state & INVALID)
+			continue;
+		if ((io_start >= start_dbn && io_start < end_dbn) ||
+		    (io_end >= start_dbn && io_end < end_dbn)) {
+			/* We have a match */
+			if (rw == WRITE)
+				dmc->flashcache_stats.wr_invalidates++;
+			else
+				dmc->flashcache_stats.rd_invalidates++;
+			if (!(cacheblk->cache_state & (BLOCK_IO_INPROG | DIRTY)) &&
+			    (cacheblk->nr_queued == 0)) {
+				dmc->cached_blocks--;			
+				DPRINTK("Cache invalidate (!BUSY): Block %llu %lx",
+					start_dbn, cacheblk->cache_state);
+				cacheblk->cache_state = INVALID;
+				continue;
+			}
+			/*
+			 * The conflicting block has either IO in progress or is 
+			 * Dirty. In all cases, we need to add ourselves to the 
+			 * pending queue. Then if the block is dirty, we kick off
+			 * an IO to clean the block. 
+			 * Note that if the block is dirty and IO is in progress
+			 * on it, the do_pending handler will clean the block
+			 * and then process the pending queue.
+			 */
+			flashcache_enq_pending(dmc, bio, i, INVALIDATE, pjob);
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				/* 
+				 * Kick off block write.
+				 * We can't kick off the write under the spinlock.
+				 * Instead, we mark the slot DISKWRITEINPROG, drop 
+				 * the spinlock and kick off the write. A block marked
+				 * DISKWRITEINPROG cannot change underneath us. 
+				 * to enqueue ourselves onto it's pending queue.
+				 *
+				 * XXX - The dropping of the lock here can be avoided if
+				 * we punt the cleaning of the block to the worker thread,
+				 * at the cost of a context switch.
+				 */
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				spin_unlock_irq(&dmc->cache_spin_lock);
+				flashcache_dirty_writeback(dmc, i); /* Must inc nr_jobs */
+				spin_lock_irq(&dmc->cache_spin_lock);
+			}
+			return 1;
+		}
+	}
+	return 0;
+}
+
+/* 
+ * Since md will break up IO into blocksize pieces, we only really need to check 
+ * the start set and the end set for overlaps.
+ */
+static int
+flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio)
+{	
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_set, end_set;
+	int queued;
+	struct pending_job *pjob1, *pjob2;
+
+	pjob1 = flashcache_alloc_pending_job(dmc);
+	if (unlikely(dmc->sysctl_error_inject & INVAL_PENDING_JOB_ALLOC_FAIL)) {
+		if (pjob1) {
+			flashcache_free_pending_job(pjob1);
+			pjob1 = NULL;
+		}
+		dmc->sysctl_error_inject &= ~INVAL_PENDING_JOB_ALLOC_FAIL;
+	}
+	if (pjob1 == NULL) {
+		queued = -ENOMEM;
+		goto out;
+	}
+	pjob2 = flashcache_alloc_pending_job(dmc);
+	if (pjob2 == NULL) {
+		flashcache_free_pending_job(pjob1);
+		queued = -ENOMEM;
+		goto out;
+	}
+	start_set = hash_block(dmc, io_start);
+	end_set = hash_block(dmc, io_end);
+	queued = flashcache_inval_block_set(dmc, start_set, bio, 
+					    bio_data_dir(bio), pjob1);
+	if (queued) {
+		flashcache_free_pending_job(pjob2);
+		goto out;
+	} else
+		flashcache_free_pending_job(pjob1);		
+	if (start_set != end_set) {
+		queued = flashcache_inval_block_set(dmc, end_set, 
+						    bio, bio_data_dir(bio), pjob2);
+		if (!queued)
+			flashcache_free_pending_job(pjob2);
+	} else
+		flashcache_free_pending_job(pjob2);		
+out:
+	return queued;
+}
+
+static void
+flashcache_write_miss(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct kcached_job *job;
+	int queued;
+
+	cacheblk = &dmc->cache[index];
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+	if (cacheblk->cache_state & VALID)
+		dmc->flashcache_stats.wr_replace++;
+	else
+		dmc->cached_blocks++;
+	cacheblk->cache_state = VALID | CACHEWRITEINPROG;
+	cacheblk->dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(dmc->sysctl_error_inject & WRITE_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		dmc->sysctl_error_inject &= ~WRITE_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/* 
+		 * We have a write miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any 
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Write (miss) failed ! Can't allocate memory for cache IO, block %lu", 
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_writes++;
+		job->action = WRITECACHE; 
+		if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+			/* Write data to the cache */		
+			dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, 
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		} else {
+			VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+			/* Write data to both disk and cache */
+			dm_io_async_bvec(2, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+					 (struct io_region *)&job->job_io_regions, 
+#else
+					 (struct dm_io_region *)&job->job_io_regions, 
+#endif
+					 WRITE, 
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		}
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_write_hit(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+	struct kcached_job *job;
+
+	cacheblk = &dmc->cache[index];
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		if (cacheblk->cache_state & DIRTY)
+			dmc->flashcache_stats.dirty_write_hits++;
+		dmc->flashcache_stats.write_hits++;
+		cacheblk->cache_state |= CACHEWRITEINPROG;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(dmc->sysctl_error_inject & WRITE_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			dmc->sysctl_error_inject &= ~WRITE_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/* 
+			 * We have a write hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any 
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Write (hit) failed ! Can't allocate memory for cache IO, block %lu", 
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			DPRINTK("Queue job for %llu", bio->bi_sector);
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_writes++;
+			job->action = WRITECACHE;
+			if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+				/* Write data to the cache */
+				dm_io_async_bvec(1, &job->job_io_regions.cache, WRITE, 
+						 bio->bi_io_vec + bio->bi_idx,
+						 flashcache_io_callback, job);
+				flashcache_clean_set(dmc, index / dmc->assoc);
+			} else {
+				VERIFY(dmc->cache_mode == FLASHCACHE_WRITE_THROUGH);
+				/* Write data to both disk and cache */
+				dmc->flashcache_stats.disk_writes++;
+				dm_io_async_bvec(2, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+						 (struct io_region *)&job->job_io_regions, 
+#else
+						 (struct dm_io_region *)&job->job_io_regions, 
+#endif
+						 WRITE, 
+						 bio->bi_io_vec + bio->bi_idx,
+						 flashcache_io_callback, job);				
+			}
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(dmc->sysctl_error_inject & WRITE_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			dmc->sysctl_error_inject &= ~WRITE_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(pjob == NULL))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, WRITECACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_write(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+	
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	if (res != -1) {
+		/* Cache Hit */
+		cacheblk = &dmc->cache[index];		
+		if ((cacheblk->cache_state & VALID) && 
+		    (cacheblk->dbn == bio->bi_sector)) {
+			/* Cache Hit */
+			flashcache_write_hit(dmc, bio, index);
+		} else {
+			/* Cache Miss, found block to recycle */
+			flashcache_write_miss(dmc, bio, index);
+		}
+		return;
+	}
+	/*
+	 * No room in the set. We cannot write to the cache and have to 
+	 * send the request to disk. Before we do that, we must check 
+	 * for potential invalidations !
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	/* Start uncached IO */
+	flashcache_start_uncached_io(dmc, bio);
+	flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,32)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+#define bio_barrier(bio)        ((bio)->bi_rw & (1 << BIO_RW_BARRIER))
+#else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+#define bio_barrier(bio)        ((bio)->bi_rw & REQ_HARDBARRIER)
+#else
+#define bio_barrier(bio)        ((bio)->bi_rw & REQ_FLUSH)
+#endif
+#endif
+#endif
+
+/*
+ * Decide the mapping and perform necessary cache operations for a bio request.
+ */
+int 
+flashcache_map(struct dm_target *ti, struct bio *bio,
+	       union map_info *map_context)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	int sectors = to_sector(bio->bi_size);
+	int queued;
+	
+	if (sectors <= 32)
+		size_hist[sectors]++;
+
+	if (bio_barrier(bio))
+		return -EOPNOTSUPP;
+
+	VERIFY(to_sector(bio->bi_size) <= dmc->block_size);
+
+	if (bio_data_dir(bio) == READ)
+		dmc->flashcache_stats.reads++;
+	else
+		dmc->flashcache_stats.writes++;
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	if (unlikely(dmc->sysctl_pid_do_expiry && 
+		     (dmc->whitelist_head || dmc->blacklist_head)))
+		flashcache_pid_expiry_all_locked(dmc);
+	if ((to_sector(bio->bi_size) != dmc->block_size) ||
+	    (bio_data_dir(bio) == WRITE && 
+	     (dmc->cache_mode == FLASHCACHE_WRITE_AROUND || flashcache_uncacheable(dmc, bio)))) {
+		queued = flashcache_inval_blocks(dmc, bio);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		if (queued) {
+			if (unlikely(queued < 0))
+				flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		} else {
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, bio);
+		}
+	} else {
+		spin_unlock_irq(&dmc->cache_spin_lock);		
+		if (bio_data_dir(bio) == READ)
+			flashcache_read(dmc, bio);
+		else
+			flashcache_write(dmc, bio);
+	}
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* Block sync support functions */
+static void 
+flashcache_kcopyd_callback_sync(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback_sync: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this cache from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu", 
+		      -read_err, -write_err, job->job_io_regions.disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;			
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_sync_blocks(dmc);  /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback_sync(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+	
+	VERIFY((cacheblk->cache_state & FALLOW_DOCLEAN) == 0);
+	DPRINTK("flashcache_dirty_writeback_sync: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	/*
+	 * If the device is being (fast) removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog) == FAST_REMOVE)) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu", 
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for sync) failed ! Can't allocate memory, block %lu", 
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK_SYNC;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback_sync,
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback_sync, 
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(flashcache_kcp_client, &job->job_io_regions.cache, 1, &job->job_io_regions.disk, 0, 
+			       (dm_kcopyd_notify_fn)flashcache_kcopyd_callback_sync, 
+			       (void *)job);
+#endif
+	}
+}
+
+/* 
+ * Sync all dirty blocks. We pick off dirty blocks, sort them, merge them with 
+ * any contigous blocks we can within the set and fire off the writes.
+ */
+void
+flashcache_sync_blocks(struct cache_c *dmc)
+{
+	unsigned long flags;
+	int index;
+	struct dbn_index_pair *writes_list;
+	int nr_writes;
+	int i, set;
+	struct cacheblock *cacheblk;
+
+	/* 
+	 * If a (fast) removal of this device is in progress, don't kick off 
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback_sync() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if ((atomic_read(&dmc->remove_in_prog) == FAST_REMOVE) || 
+	    dmc->sysctl_stop_sync)
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	nr_writes = 0;
+	set = -1;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);	
+	index = dmc->sync_index;
+	while (index < dmc->size && 
+	       (nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total) {
+		VERIFY(nr_writes <= dmc->assoc);
+		if (((index % dmc->assoc) == 0) && (nr_writes > 0)) {
+			/*
+			 * Crossing a set, sort/merge all the IOs collected so
+			 * far and issue the writes.
+			 */
+			VERIFY(set != -1);
+			flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			for (i = 0 ; i < nr_writes ; i++)
+				flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+			nr_writes = 0;
+			set = -1;
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			cacheblk->cache_state |= DISKWRITEINPROG;
+			flashcache_clear_fallow(dmc, index);
+			writes_list[nr_writes].dbn = cacheblk->dbn;
+			writes_list[nr_writes].index = index;
+			set = index / dmc->assoc;
+			nr_writes++;
+		}
+		index++;
+	}
+	dmc->sync_index = index;
+	if (nr_writes > 0) {
+		VERIFY(set != -1);
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+	} else
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	kfree(writes_list);
+}
+
+void
+flashcache_sync_all(struct cache_c *dmc)
+{
+	unsigned long flags;
+
+	if (dmc->cache_mode != FLASHCACHE_WRITE_BACK)
+		return;
+	dmc->sysctl_stop_sync = 0;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	dmc->sync_index = 0;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);	
+	flashcache_sync_blocks(dmc);
+}
+
+/*
+ * We handle uncached IOs ourselves to deal with the problem of out of ordered
+ * IOs corrupting the cache. Consider the case where we get 2 concurent IOs
+ * for the same block Write-Read (or a Write-Write). Consider the case where
+ * the first Write is uncacheable and the second IO is cacheable. If the 
+ * 2 IOs are out-of-ordered below flashcache, then we will cache inconsistent
+ * data in flashcache (persistently).
+ * 
+ * We do invalidations before launching uncacheable IOs to disk. But in case
+ * of out of ordering the invalidations before launching the IOs does not help.
+ * We need to invalidate after the IO completes.
+ * 
+ * Doing invalidations after the completion of an uncacheable IO will cause 
+ * any overlapping dirty blocks in the cache to be written out and the IO 
+ * relaunched. If the overlapping blocks are busy, the IO is relaunched to 
+ * disk also (post invalidation). In these 2 cases, we will end up sending
+ * 2 disk IOs for the block. But this is a rare case.
+ * 
+ * When 2 IOs for the same block are sent down (by un co-operating processes)
+ * the storage stack is allowed to re-order the IOs at will. So the applications
+ * cannot expect any ordering at all.
+ * 
+ * What we try to avoid here is inconsistencies between disk and the ssd cache.
+ */
+void 
+flashcache_uncached_io_complete(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	int queued;
+	int error = job->error;
+
+	if (unlikely(error)) {
+		DMERR("flashcache uncached disk IO error: io error %d block %lu R/w %s", 
+		      error, job->job_io_regions.disk.sector, 
+		      (bio_data_dir(job->bio) == WRITE) ? "WRITE" : "READ");
+		if (bio_data_dir(job->bio) == WRITE)
+			dmc->flashcache_errors.disk_write_errors++;
+		else
+			dmc->flashcache_errors.disk_read_errors++;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	queued = flashcache_inval_blocks(dmc, job->bio);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(job->bio, -EIO, dmc, NULL);
+		/* 
+		 * The IO will be re-executed.
+		 * The do_pending logic will re-launch the 
+		 * disk IO post-invalidation calling start_uncached_io.
+		 * This should be a rare occurrence.
+		 */
+		dmc->flashcache_stats.uncached_io_requeue++;
+	} else {
+		flashcache_bio_endio(job->bio, error, dmc, &job->io_start_time);
+	}
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void 
+flashcache_uncached_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+
+	VERIFY(job->index == -1);
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_uncached_io_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static void
+flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio)
+{
+	int is_write = (bio_data_dir(bio) == WRITE);
+	struct kcached_job *job;
+	
+	if (is_write) {
+		dmc->flashcache_stats.uncached_writes++;
+		dmc->flashcache_stats.disk_writes++;
+	} else {
+		dmc->flashcache_stats.uncached_reads++;
+		dmc->flashcache_stats.disk_reads++;
+	}
+	job = new_kcached_job(dmc, bio, -1);
+	if (unlikely(job == NULL)) {
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	atomic_inc(&dmc->nr_jobs);
+	dm_io_async_bvec(1, &job->job_io_regions.disk,
+			 ((is_write) ? WRITE : READ), 
+			 bio->bi_io_vec + bio->bi_idx,
+			 flashcache_uncached_io_callback, job);
+}
+
+EXPORT_SYMBOL(flashcache_io_callback);
+EXPORT_SYMBOL(flashcache_do_pending_error);
+EXPORT_SYMBOL(flashcache_do_pending_noerror);
+EXPORT_SYMBOL(flashcache_do_pending);
+EXPORT_SYMBOL(flashcache_do_io);
+EXPORT_SYMBOL(flashcache_map);
+EXPORT_SYMBOL(flashcache_write);
+EXPORT_SYMBOL(flashcache_inval_blocks);
+EXPORT_SYMBOL(flashcache_inval_block_set);
+EXPORT_SYMBOL(flashcache_read);
+EXPORT_SYMBOL(flashcache_read_miss);
+EXPORT_SYMBOL(flashcache_clean_set);
+EXPORT_SYMBOL(flashcache_dirty_writeback);
+EXPORT_SYMBOL(flashcache_kcopyd_callback);
+EXPORT_SYMBOL(flashcache_lookup);
+EXPORT_SYMBOL(flashcache_alloc_md_sector);
+EXPORT_SYMBOL(flashcache_free_md_sector);
+EXPORT_SYMBOL(flashcache_md_write_callback);
+EXPORT_SYMBOL(flashcache_md_write_kickoff);
+EXPORT_SYMBOL(flashcache_md_write_done);
+EXPORT_SYMBOL(flashcache_md_write);
+
+
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_procfs.c linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_procfs.c
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_procfs.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_procfs.c	2016-12-13 17:25:21.069077149 +0800
@@ -0,0 +1,1100 @@
+/****************************************************************************
+ *  flashcache_conf.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+static int fallow_clean_speed_min = FALLOW_SPEED_MIN;
+static int fallow_clean_speed_max = FALLOW_SPEED_MAX;
+
+extern u_int64_t size_hist[];
+
+static char *flashcache_cons_procfs_cachename(struct cache_c *dmc, char *path_component);
+static char *flashcache_cons_sysctl_devname(struct cache_c *dmc);
+
+#define FLASHCACHE_PROC_ROOTDIR_NAME	"flashcache"
+
+static int
+flashcache_io_latency_init(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			   struct file *file,
+#endif
+			   void __user *buffer,
+			   size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_io_latency_hist) {
+			int i;
+				
+			for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+				dmc->latency_hist[i] = 0;
+			dmc->latency_hist_10ms = 0;
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_sync_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+		       struct file *file, 
+#endif
+		       void __user *buffer, 
+		       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_do_sync) {
+			dmc->sysctl_stop_sync = 0;
+			cancel_delayed_work(&dmc->delayed_clean);
+			flush_scheduled_work();
+			flashcache_sync_all(dmc);
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_zerostats_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			    struct file *file, 
+#endif
+			    void __user *buffer, 
+			    size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_zerostats) {
+			int i;
+
+			memset(&dmc->flashcache_stats, 0, sizeof(struct flashcache_stats));
+			for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+				dmc->latency_hist[i] = 0;
+			dmc->latency_hist_10ms = 0;
+		}
+	}
+	return 0;
+}
+
+static int 
+flashcache_fallow_clean_speed_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+				     struct file *file, 
+#endif
+				     void __user *buffer, 
+				     size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_fallow_clean_speed < fallow_clean_speed_min)
+			dmc->sysctl_fallow_clean_speed = fallow_clean_speed_min;
+
+		if (dmc->sysctl_fallow_clean_speed > fallow_clean_speed_max)
+			dmc->sysctl_fallow_clean_speed = fallow_clean_speed_max;
+	}
+	return 0;
+}
+
+static int
+flashcache_dirty_thresh_sysctl(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			       struct file *file, 
+#endif
+			       void __user *buffer, 
+			       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc = (struct cache_c *)table->extra1;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+        proc_dointvec(table, write, file, buffer, length, ppos);
+#else
+        proc_dointvec(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (dmc->sysctl_dirty_thresh > DIRTY_THRESH_MAX)
+			dmc->sysctl_dirty_thresh = DIRTY_THRESH_MAX;
+
+		if (dmc->sysctl_dirty_thresh < DIRTY_THRESH_MIN)
+			dmc->sysctl_dirty_thresh = DIRTY_THRESH_MIN;
+
+		dmc->dirty_thresh_set = 
+			(dmc->assoc * dmc->sysctl_dirty_thresh) / 100;
+	}
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+#define CTL_UNNUMBERED			-2
+#endif
+
+/*
+ * Each ctl_table array needs to be 1 more than the actual number of
+ * entries - zero padded at the end ! Therefore the NUM_*_SYSCTLS
+ * is 1 more than then number of sysctls.
+ */
+#define FLASHCACHE_NUM_WRITEBACK_SYSCTLS	17
+
+static struct flashcache_writeback_sysctl_table {
+	struct ctl_table_header *sysctl_header;
+	ctl_table		vars[FLASHCACHE_NUM_WRITEBACK_SYSCTLS];
+	ctl_table		dev[2];
+	ctl_table		dir[2];
+	ctl_table		root[2];
+} flashcache_writeback_sysctl = {
+	.vars = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "io_latency_hist",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_io_latency_init,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_sync",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_sync_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "stop_sync",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "dirty_thresh_pct",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_dirty_thresh_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_clean_ios_total",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_clean_ios_set",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_pid_expiry",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_pids",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "pid_expiry_secs",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "reclaim_policy",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "zero_stats",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_zerostats_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+#ifdef notdef
+		/* 
+		 * Disable this for all except devel builds 
+		 * If you enable this, you must bump FLASHCACHE_NUM_WRITEBACK_SYSCTLS
+		 * by 1 !
+		 */
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "error_inject",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+#endif
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fast_remove",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "cache_all",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fallow_clean_speed",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_fallow_clean_speed_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "fallow_delay",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "skip_seq_thresh_kb",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+	},
+	.dev = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "flashcache-dev",
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writeback_sysctl.vars,
+		},
+	},
+	.dir = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= FLASHCACHE_PROC_ROOTDIR_NAME,
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writeback_sysctl.dev,
+		},
+	},
+	.root = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_DEV,
+#endif
+			.procname	= "dev",
+			.maxlen		= 0,
+			.mode		= 0555,
+			.child		= flashcache_writeback_sysctl.dir,
+		},
+	},
+};
+
+/*
+ * Each ctl_table array needs to be 1 more than the actual number of
+ * entries - zero padded at the end ! Therefore the NUM_*_SYSCTLS
+ * is 1 more than then number of sysctls.
+ */
+#define FLASHCACHE_NUM_WRITETHROUGH_SYSCTLS	9
+
+static struct flashcache_writethrough_sysctl_table {
+	struct ctl_table_header *sysctl_header;
+	ctl_table		vars[FLASHCACHE_NUM_WRITETHROUGH_SYSCTLS];
+	ctl_table		dev[2];
+	ctl_table		dir[2];
+	ctl_table		root[2];
+} flashcache_writethrough_sysctl = {
+	.vars = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "io_latency_hist",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_io_latency_init,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "do_pid_expiry",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "max_pids",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "pid_expiry_secs",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "reclaim_policy",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "zero_stats",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &flashcache_zerostats_sysctl,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.strategy	= &sysctl_intvec,
+#endif
+		},
+#ifdef notdef
+		/* 
+		 * Disable this for all except devel builds 
+		 * If you enable this, you must bump FLASHCACHE_NUM_WRITEBACK_SYSCTLS
+		 * by 1 !
+		 */
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "error_inject",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+#endif
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "cache_all",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "skip_seq_thresh_kb",
+			.maxlen		= sizeof(int),
+			.mode		= 0644,
+			.proc_handler	= &proc_dointvec,
+		},
+	},
+	.dev = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= "flashcache-dev",
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writethrough_sysctl.vars,
+		},
+	},
+	.dir = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_UNNUMBERED,
+#endif
+			.procname	= FLASHCACHE_PROC_ROOTDIR_NAME,
+			.maxlen		= 0,
+			.mode		= S_IRUGO|S_IXUGO,
+			.child		= flashcache_writethrough_sysctl.dev,
+		},
+	},
+	.root = {
+		{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+			.ctl_name	= CTL_DEV,
+#endif
+			.procname	= "dev",
+			.maxlen		= 0,
+			.mode		= 0555,
+			.child		= flashcache_writethrough_sysctl.dir,
+		},
+	},
+};
+
+int *
+flashcache_find_sysctl_data(struct cache_c *dmc, ctl_table *vars)
+{
+	if (strcmp(vars->procname, "io_latency_hist") == 0)
+		return &dmc->sysctl_io_latency_hist;
+	else if (strcmp(vars->procname, "do_sync") == 0) 
+		return &dmc->sysctl_do_sync;
+	else if (strcmp(vars->procname, "stop_sync") == 0) 
+		return &dmc->sysctl_stop_sync;
+	else if (strcmp(vars->procname, "dirty_thresh_pct") == 0) 
+		return &dmc->sysctl_dirty_thresh;
+	else if (strcmp(vars->procname, "max_clean_ios_total") == 0) 
+		return &dmc->max_clean_ios_total;
+	else if (strcmp(vars->procname, "max_clean_ios_set") == 0) 
+		return &dmc->max_clean_ios_set;
+	else if (strcmp(vars->procname, "do_pid_expiry") == 0) 
+		return &dmc->sysctl_pid_do_expiry;
+	else if (strcmp(vars->procname, "max_pids") == 0) 
+		return &dmc->sysctl_max_pids;
+	else if (strcmp(vars->procname, "pid_expiry_secs") == 0) 
+		return &dmc->sysctl_pid_expiry_secs;
+	else if (strcmp(vars->procname, "reclaim_policy") == 0) 
+		return &dmc->sysctl_reclaim_policy;
+	else if (strcmp(vars->procname, "zero_stats") == 0) 
+		return &dmc->sysctl_zerostats;
+	else if (strcmp(vars->procname, "error_inject") == 0) 
+		return &dmc->sysctl_error_inject;
+	else if (strcmp(vars->procname, "fast_remove") == 0) 
+		return &dmc->sysctl_fast_remove;
+	else if (strcmp(vars->procname, "cache_all") == 0) 
+		return &dmc->sysctl_cache_all;
+	else if (strcmp(vars->procname, "fallow_clean_speed") == 0) 
+		return &dmc->sysctl_fallow_clean_speed;
+	else if (strcmp(vars->procname, "fallow_delay") == 0) 
+		return &dmc->sysctl_fallow_delay;
+	else if (strcmp(vars->procname, "skip_seq_thresh_kb") == 0) 
+		return &dmc->sysctl_skip_seq_thresh_kb;
+	VERIFY(0);
+	return NULL;
+}
+
+static void
+flashcache_writeback_sysctl_register(struct cache_c *dmc)
+{
+	int i;
+	struct flashcache_writeback_sysctl_table *t;
+	
+	t = kmemdup(&flashcache_writeback_sysctl, sizeof(*t), GFP_KERNEL);
+	if (t == NULL)
+		return;
+	for (i = 0 ; i < ARRAY_SIZE(t->vars) - 1 ; i++) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+		t->vars[i].de = NULL;
+#endif
+		t->vars[i].data = flashcache_find_sysctl_data(dmc, &t->vars[i]);
+		t->vars[i].extra1 = dmc;
+	}
+	
+	t->dev[0].procname = flashcache_cons_sysctl_devname(dmc);
+	t->dev[0].child = t->vars;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dev[0].de = NULL;
+#endif
+	t->dir[0].child = t->dev;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dir[0].de = NULL;
+#endif
+	t->root[0].child = t->dir;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->root[0].de = NULL;
+#endif
+	
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->sysctl_header = register_sysctl_table(t->root, 0);
+#else
+	t->sysctl_header = register_sysctl_table(t->root);
+#endif
+	if (t->sysctl_header == NULL)
+		goto out;
+	
+	dmc->sysctl_handle = t;
+	return;
+
+out:
+	kfree(t->dev[0].procname);
+	kfree(t);
+}
+
+static void
+flashcache_writeback_sysctl_unregister(struct cache_c *dmc)
+{
+	struct flashcache_writeback_sysctl_table *t;
+
+	t = dmc->sysctl_handle;
+	if (t != NULL) {
+		dmc->sysctl_handle = NULL;
+		unregister_sysctl_table(t->sysctl_header);
+		kfree(t->dev[0].procname);
+		kfree(t);		
+	}
+}
+
+static void
+flashcache_writethrough_sysctl_register(struct cache_c *dmc)
+{
+	int i;
+	struct flashcache_writethrough_sysctl_table *t;
+	
+	t = kmemdup(&flashcache_writethrough_sysctl, sizeof(*t), GFP_KERNEL);
+	if (t == NULL)
+		return;
+	for (i = 0 ; i < ARRAY_SIZE(t->vars) - 1 ; i++) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+		t->vars[i].de = NULL;
+#endif
+		t->vars[i].data = flashcache_find_sysctl_data(dmc, &t->vars[i]);
+		t->vars[i].extra1 = dmc;
+	}
+	
+	t->dev[0].procname = flashcache_cons_sysctl_devname(dmc);
+	t->dev[0].child = t->vars;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dev[0].de = NULL;
+#endif
+	t->dir[0].child = t->dev;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->dir[0].de = NULL;
+#endif
+	t->root[0].child = t->dir;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->root[0].de = NULL;
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	t->sysctl_header = register_sysctl_table(t->root, 0);
+#else
+	t->sysctl_header = register_sysctl_table(t->root);
+#endif
+	if (t->sysctl_header == NULL)
+		goto out;
+	
+	dmc->sysctl_handle = t;
+	return;
+
+out:
+	kfree(t->dev[0].procname);
+	kfree(t);
+}
+
+static void
+flashcache_writethrough_sysctl_unregister(struct cache_c *dmc)
+{
+	struct flashcache_writethrough_sysctl_table *t;
+
+	t = dmc->sysctl_handle;
+	if (t != NULL) {
+		dmc->sysctl_handle = NULL;
+		unregister_sysctl_table(t->sysctl_header);
+		kfree(t->dev[0].procname);
+		kfree(t);		
+	}
+}
+
+
+static int 
+flashcache_stats_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+	struct flashcache_stats *stats;
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+
+	stats = &dmc->flashcache_stats;
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	seq_printf(seq, "reads=%lu writes=%lu \n", 
+		   stats->reads, stats->writes);
+	seq_printf(seq, "read_hits=%lu read_hit_percent=%d ", 
+		   stats->read_hits, read_hit_pct);
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK || dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		seq_printf(seq, "write_hits=%lu write_hit_percent=%d ", 
+		   	   stats->write_hits, write_hit_pct);
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+		seq_printf(seq, "dirty_write_hits=%lu dirty_write_hit_percent=%d ",
+			   stats->dirty_write_hits, dirty_write_hit_pct);
+	}
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK || dmc->cache_mode == FLASHCACHE_WRITE_THROUGH) {
+		seq_printf(seq, "replacement=%lu write_replacement=%lu ",
+			   stats->replace, stats->wr_replace);
+		seq_printf(seq,  "write_invalidates=%lu read_invalidates=%lu ",
+			   stats->wr_invalidates, stats->rd_invalidates);
+	} else {	/* WRITE_AROUND */
+		seq_printf(seq, "replacement=%lu ",
+			   stats->replace);
+		seq_printf(seq, "read_invalidates=%lu ",
+			   stats->rd_invalidates);
+	}
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	seq_printf(seq,  "checksum_store=%ld checksum_valid=%ld checksum_invalid=%ld ",
+		stats->checksum_store, stats->checksum_valid, stats->checksum_invalid);
+#endif
+	seq_printf(seq,  "pending_enqueues=%lu pending_inval=%lu ",
+		   stats->enqueues, stats->pending_inval);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) { 
+		seq_printf(seq, "metadata_dirties=%lu metadata_cleans=%lu ",
+			   stats->md_write_dirty, stats->md_write_clean);
+		seq_printf(seq, "metadata_batch=%lu metadata_ssd_writes=%lu ",
+			   stats->md_write_batch, stats->md_ssd_writes);
+		seq_printf(seq, "cleanings=%lu fallow_cleanings=%lu ",
+			   stats->cleanings, stats->fallow_cleanings);
+	}
+	seq_printf(seq, "no_room=%lu ",
+		   stats->noroom);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK) {
+ 		seq_printf(seq, "front_merge=%lu back_merge=%lu ",
+			   stats->front_merge, stats->back_merge);
+	}
+	seq_printf(seq,  "disk_reads=%lu disk_writes=%lu ssd_reads=%lu ssd_writes=%lu ",
+		   stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes);
+	seq_printf(seq,  "uncached_reads=%lu uncached_writes=%lu uncached_IO_requeue=%lu ",
+		   stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue);
+	seq_printf(seq,  "uncached_sequential_reads=%lu uncached_sequential_writes=%lu ",
+		   stats->uncached_sequential_reads, stats->uncached_sequential_writes);
+	seq_printf(seq, "pid_adds=%lu pid_dels=%lu pid_drops=%lu pid_expiry=%lu\n",
+		   stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+	return 0;
+}
+
+static int 
+flashcache_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_stats_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_stats_operations = {
+	.open		= flashcache_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_errors_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+
+	seq_printf(seq, "disk_read_errors=%d disk_write_errors=%d ",
+		   dmc->flashcache_errors.disk_read_errors, 
+		   dmc->flashcache_errors.disk_write_errors);
+	seq_printf(seq, "ssd_read_errors=%d ssd_write_errors=%d ",
+		   dmc->flashcache_errors.ssd_read_errors, 
+		   dmc->flashcache_errors.ssd_write_errors);
+	seq_printf(seq, "memory_alloc_errors=%d\n", 
+		   dmc->flashcache_errors.memory_alloc_errors);
+	return 0;
+}
+
+static int 
+flashcache_errors_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_errors_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_errors_operations = {
+	.open		= flashcache_errors_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_iosize_hist_show(struct seq_file *seq, void *v)
+{
+	int i;
+	
+	for (i = 1 ; i <= 32 ; i++) {
+		seq_printf(seq, "%d:%llu ", i*512, size_hist[i]);
+	}
+	seq_printf(seq, "\n");
+	return 0;
+}
+
+static int 
+flashcache_iosize_hist_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_iosize_hist_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_iosize_hist_operations = {
+	.open		= flashcache_iosize_hist_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int 
+flashcache_pidlists_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc = seq->private;
+	struct flashcache_cachectl_pid *pid_list;
+ 	unsigned long flags;
+	
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	seq_printf(seq, "Blacklist: ");
+	pid_list = dmc->blacklist_head;
+	while (pid_list != NULL) {
+		seq_printf(seq, "%u ", pid_list->pid);
+		pid_list = pid_list->next;
+	}
+	seq_printf(seq, "\n");
+	seq_printf(seq, "Whitelist: ");
+	pid_list = dmc->whitelist_head;
+	while (pid_list != NULL) {
+		seq_printf(seq, "%u ", pid_list->pid);
+		pid_list = pid_list->next;
+	}
+	seq_printf(seq, "\n");
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	return 0;
+}
+
+static int 
+flashcache_pidlists_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_pidlists_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_pidlists_operations = {
+	.open		= flashcache_pidlists_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+extern char *flashcache_sw_version;
+
+static int 
+flashcache_version_show(struct seq_file *seq, void *v)
+{
+	seq_printf(seq, "Flashcache Version : %s\n", flashcache_sw_version);
+#ifdef COMMIT_REV
+	seq_printf(seq, "git commit: %s\n", COMMIT_REV);
+#endif
+	return 0;
+}
+
+static int 
+flashcache_version_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_version_show, PDE(inode)->data);
+}
+
+static struct file_operations flashcache_version_operations = {
+	.open		= flashcache_version_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+void
+flashcache_module_procfs_init(void)
+{
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *entry;
+
+	if (proc_mkdir("flashcache", NULL)) {
+		entry = create_proc_entry("flashcache/flashcache_version", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_version_operations;
+	}
+#endif /* CONFIG_PROC_FS */
+}
+
+void
+flashcache_module_procfs_releae(void)
+{
+#ifdef CONFIG_PROC_FS
+	(void)remove_proc_entry("flashcache/flashcache_version", NULL);
+	(void)remove_proc_entry("flashcache", NULL);
+#endif /* CONFIG_PROC_FS */
+}
+
+static char *
+flashcache_cons_sysctl_devname(struct cache_c *dmc)
+{
+	char *pathname;
+	
+	pathname = kzalloc(strlen(dmc->cache_devname) + strlen(dmc->disk_devname) + 2,
+			   GFP_KERNEL);
+	strcpy(pathname, strrchr(dmc->cache_devname, '/') + 1);
+	strcat(pathname, "+");
+	strcat(pathname, strrchr(dmc->disk_devname, '/') + 1);
+	return pathname;
+}
+
+static char *
+flashcache_cons_procfs_cachename(struct cache_c *dmc, char *path_component)
+{
+	char *pathname;
+	char *s;
+	
+	pathname = kzalloc(strlen(dmc->cache_devname) + strlen(dmc->disk_devname) + 4 + 
+			   strlen(FLASHCACHE_PROC_ROOTDIR_NAME) + 
+			   strlen(path_component), 
+			   GFP_KERNEL);
+	strcpy(pathname, FLASHCACHE_PROC_ROOTDIR_NAME);
+	strcat(pathname, "/");
+	s = strrchr(dmc->cache_devname, '/');
+	if (s) 
+		s++;
+	else
+		s = dmc->cache_devname;
+	strcat(pathname, s);
+	strcat(pathname, "+");
+	s = strrchr(dmc->disk_devname, '/');
+	if (s) 
+		s++;
+	else
+		s = dmc->disk_devname;
+	strcat(pathname, s);
+	if (strcmp(path_component, "") != 0) {
+		strcat(pathname, "/");
+		strcat(pathname, path_component);
+	}
+	return pathname;
+}
+
+void 
+flashcache_ctr_procfs(struct cache_c *dmc)
+{
+	char *s;
+	struct proc_dir_entry *entry;
+
+	s =  flashcache_cons_procfs_cachename(dmc, "");
+	entry = proc_mkdir(s, NULL);
+	kfree(s);
+	if (entry == NULL)
+		return;
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_stats");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_stats_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_errors");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_errors_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_iosize_hist");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_iosize_hist_operations;
+		entry->data = dmc;
+	}
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_pidlists");
+	entry = create_proc_entry(s, 0, NULL);
+	if (entry) {
+		entry->proc_fops =  &flashcache_pidlists_operations;
+		entry->data = dmc;			
+	}
+	kfree(s);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		flashcache_writeback_sysctl_register(dmc);
+	else
+		flashcache_writethrough_sysctl_register(dmc);
+}
+
+void 
+flashcache_dtr_procfs(struct cache_c *dmc)
+{
+	char *s;
+	
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_stats");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_errors");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_iosize_hist");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "flashcache_pidlists");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	s = flashcache_cons_procfs_cachename(dmc, "");
+	remove_proc_entry(s, NULL);
+	kfree(s);
+
+	if (dmc->cache_mode == FLASHCACHE_WRITE_BACK)
+		flashcache_writeback_sysctl_unregister(dmc);
+	else
+		flashcache_writethrough_sysctl_unregister(dmc);
+
+}
+
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_subr.c linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_subr.c
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/flashcache_subr.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/flashcache_subr.c	2016-12-13 17:25:21.068077248 +0800
@@ -0,0 +1,785 @@
+/****************************************************************************
+ *  flashcache_subr.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ * 
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/sort.h>
+#include <linux/time.h>
+#include <asm/kmap_types.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+
+static DEFINE_SPINLOCK(_job_lock);
+
+extern mempool_t *_job_pool;
+extern mempool_t *_pending_job_pool;
+
+extern atomic_t nr_cache_jobs;
+extern atomic_t nr_pending_jobs;
+
+LIST_HEAD(_pending_jobs);
+LIST_HEAD(_io_jobs);
+LIST_HEAD(_md_io_jobs);
+LIST_HEAD(_md_complete_jobs);
+LIST_HEAD(_uncached_io_complete_jobs);
+
+int
+flashcache_pending_empty(void)
+{
+	return list_empty(&_pending_jobs);
+}
+
+int
+flashcache_io_empty(void)
+{
+	return list_empty(&_io_jobs);
+}
+
+int
+flashcache_md_io_empty(void)
+{
+	return list_empty(&_md_io_jobs);
+}
+
+int
+flashcache_md_complete_empty(void)
+{
+	return list_empty(&_md_complete_jobs);
+}
+
+int
+flashcache_uncached_io_complete_empty(void)
+{
+	return list_empty(&_uncached_io_complete_jobs);
+}
+
+struct kcached_job *
+flashcache_alloc_cache_job(void)
+{
+	struct kcached_job *job;
+
+	job = mempool_alloc(_job_pool, GFP_NOIO);
+	if (likely(job))
+		atomic_inc(&nr_cache_jobs);
+	return job;
+}
+
+void
+flashcache_free_cache_job(struct kcached_job *job)
+{
+	mempool_free(job, _job_pool);
+	atomic_dec(&nr_cache_jobs);
+}
+
+struct pending_job *
+flashcache_alloc_pending_job(struct cache_c *dmc)
+{
+	struct pending_job *job;
+
+	job = mempool_alloc(_pending_job_pool, GFP_ATOMIC);
+	if (likely(job))
+		atomic_inc(&nr_pending_jobs);
+	else
+		dmc->flashcache_errors.memory_alloc_errors++;
+	return job;
+}
+
+void
+flashcache_free_pending_job(struct pending_job *job)
+{
+	mempool_free(job, _pending_job_pool);
+	atomic_dec(&nr_pending_jobs);
+}
+
+#define FLASHCACHE_PENDING_JOB_HASH(INDEX)		((INDEX) % PENDING_JOB_HASH_SIZE)
+
+void 
+flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+		       int index, int action, struct pending_job *job)
+{
+	struct pending_job **head;
+	
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	DPRINTK("flashcache_enq_pending: Queue to pending Q Index %d %llu",
+		index, bio->bi_sector);
+	VERIFY(job != NULL);
+	job->action = action;
+	job->index = index;
+	job->bio = bio;
+	job->prev = NULL;
+	job->next = *head;
+	if (*head)
+		(*head)->prev = job;
+	*head = job;
+	dmc->cache[index].nr_queued++;
+	dmc->flashcache_stats.enqueues++;
+	dmc->pending_jobs_count++;
+}
+
+/*
+ * Deq and move all pending jobs that match the index for this slot to list returned
+ */
+struct pending_job *
+flashcache_deq_pending(struct cache_c *dmc, int index)
+{
+	struct pending_job *node, *next, *movelist = NULL;
+	int moved = 0;
+	struct pending_job **head;
+	
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	for (node = *head ; node != NULL ; node = next) {
+		next = node->next;
+		if (node->index == index) {
+			/* 
+			 * Remove pending job from the global list of 
+			 * jobs and move it to the private list for freeing 
+			 */
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			node->prev = NULL;
+			node->next = movelist;
+			movelist = node;
+			moved++;
+		}
+	}
+	VERIFY(dmc->pending_jobs_count >= moved);
+	dmc->pending_jobs_count -= moved;
+	return movelist;
+}
+
+#ifdef FLASHCACHE_DO_CHECKSUMS
+int
+flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int error;
+	u_int64_t sum = 0, *idx;
+	int cnt;
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+	where.count = dmc->block_size;
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, block);
+	if (error)
+		return error;
+	cnt = dmc->block_size * 512;
+	idx = (u_int64_t *)block;
+	while (cnt > 0) {
+		sum += *idx++;
+		cnt -= sizeof(u_int64_t);		
+	}
+	dmc->cache[index].checksum = sum;
+	return 0;
+}
+
+u_int64_t
+flashcache_compute_checksum(struct bio *bio)
+{
+	int i;	
+	u_int64_t sum = 0, *idx;
+	int cnt;
+	int kmap_type;
+	void *kvaddr;
+
+	if (in_interrupt())
+		kmap_type = KM_SOFTIRQ0;
+	else
+		kmap_type = KM_USER0;
+	for (i = bio->bi_idx ; i < bio->bi_vcnt ; i++) {
+		kvaddr = kmap_atomic(bio->bi_io_vec[i].bv_page, kmap_type);
+		idx = (u_int64_t *)
+			((char *)kvaddr + bio->bi_io_vec[i].bv_offset);
+		cnt = bio->bi_io_vec[i].bv_len;
+		while (cnt > 0) {
+			sum += *idx++;
+			cnt -= sizeof(u_int64_t);
+		}
+		kunmap_atomic(kvaddr, kmap_type);
+	}
+	return sum;
+}
+
+void
+flashcache_store_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	unsigned long flags;
+	
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	job->dmc->cache[job->index].checksum = sum;
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+}
+
+int
+flashcache_validate_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	int retval;
+	unsigned long flags;
+	
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	if (likely(job->dmc->cache[job->index].checksum == sum)) {
+		job->dmc->flashcache_stats.checksum_valid++;		
+		retval = 0;
+	} else {
+		job->dmc->flashcache_stats.checksum_invalid++;
+		retval = 1;
+	}
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+	return retval;
+}
+#endif
+
+/*
+ * Functions to push and pop a job onto the head of a given job list.
+ */
+struct kcached_job *
+pop(struct list_head *jobs)
+{
+	struct kcached_job *job = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	if (!list_empty(jobs)) {
+		job = list_entry(jobs->next, struct kcached_job, list);
+		list_del(&job->list);
+	}
+	spin_unlock_irqrestore(&_job_lock, flags);
+	return job;
+}
+
+void 
+push(struct list_head *jobs, struct kcached_job *job)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	list_add_tail(&job->list, jobs);
+	spin_unlock_irqrestore(&_job_lock, flags);
+}
+
+void
+push_pending(struct kcached_job *job)
+{
+	push(&_pending_jobs, job);	
+}
+
+void
+push_io(struct kcached_job *job)
+{
+	push(&_io_jobs, job);	
+}
+
+void
+push_uncached_io_complete(struct kcached_job *job)
+{
+	push(&_uncached_io_complete_jobs, job);	
+}
+
+void
+push_md_io(struct kcached_job *job)
+{
+	push(&_md_io_jobs, job);	
+}
+
+void
+push_md_complete(struct kcached_job *job)
+{
+	push(&_md_complete_jobs, job);	
+}
+
+static void
+process_jobs(struct list_head *jobs,
+	     void (*fn) (struct kcached_job *))
+{
+	struct kcached_job *job;
+
+	while ((job = pop(jobs)))
+		(void)fn(job);
+}
+
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+do_work(void *unused)
+#else
+do_work(struct work_struct *unused)
+#endif
+{
+	process_jobs(&_md_complete_jobs, flashcache_md_write_done);
+	process_jobs(&_pending_jobs, flashcache_do_pending);
+	process_jobs(&_md_io_jobs, flashcache_md_write_kickoff);
+	process_jobs(&_io_jobs, flashcache_do_io);
+	process_jobs(&_uncached_io_complete_jobs, flashcache_uncached_io_complete);
+}
+
+struct kcached_job *
+new_kcached_job(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct kcached_job *job;
+
+	job = flashcache_alloc_cache_job();
+	if (unlikely(job == NULL)) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return NULL;
+	}
+	job->dmc = dmc;
+	job->index = index;
+	job->job_io_regions.cache.bdev = dmc->cache_dev->bdev;
+	if (index != -1) {
+		job->job_io_regions.cache.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+		job->job_io_regions.cache.count = dmc->block_size;	
+	}
+	job->error = 0;	
+	job->bio = bio;
+	job->job_io_regions.disk.bdev = dmc->disk_dev->bdev;
+	if (index != -1) {
+		job->job_io_regions.disk.sector = dmc->cache[index].dbn;
+		job->job_io_regions.disk.count = dmc->block_size;
+	} else {
+		job->job_io_regions.disk.sector = bio->bi_sector;
+		job->job_io_regions.disk.count = to_sector(bio->bi_size);
+	}
+	job->next = NULL;
+	job->md_block = NULL;
+	if (dmc->sysctl_io_latency_hist)
+		do_gettimeofday(&job->io_start_time);
+	else {
+		job->io_start_time.tv_sec = 0;
+		job->io_start_time.tv_usec = 0;
+	}
+	return job;
+}
+
+static void
+flashcache_record_latency(struct cache_c *dmc, struct timeval *start_tv)
+{
+	struct timeval latency;
+	int64_t us;
+	
+	do_gettimeofday(&latency);
+	latency.tv_sec -= start_tv->tv_sec;
+	latency.tv_usec -= start_tv->tv_usec;	
+	us = latency.tv_sec * USEC_PER_SEC + latency.tv_usec;
+	us /= IO_LATENCY_GRAN_USECS;	/* histogram 250us gran, scale 10ms total */
+	if (us < IO_LATENCY_BUCKETS)
+		/* < 10ms latency, track it */
+		dmc->latency_hist[us]++;
+	else
+		/* else count it in 10ms+ bucket */
+		dmc->latency_hist_10ms++;
+}
+
+void
+flashcache_bio_endio(struct bio *bio, int error, 
+		     struct cache_c *dmc, struct timeval *start_time)
+{
+	if (unlikely(dmc->sysctl_io_latency_hist && 
+		     start_time != NULL && 
+		     start_time->tv_sec != 0))
+		flashcache_record_latency(dmc, start_time);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24)
+	bio_endio(bio, bio->bi_size, error);
+#else
+	bio_endio(bio, error);
+#endif	
+}
+
+void
+flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index)
+{
+	int set = index / dmc->assoc;
+	int start_index = set * dmc->assoc;
+	int my_index = index - start_index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	/* Remove from LRU */
+	if (likely((cacheblk->lru_prev != FLASHCACHE_LRU_NULL) ||
+		   (cacheblk->lru_next != FLASHCACHE_LRU_NULL))) {
+		if (cacheblk->lru_prev != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_prev + start_index].lru_next = 
+				cacheblk->lru_next;
+		else
+			dmc->cache_sets[set].lru_head = cacheblk->lru_next;
+		if (cacheblk->lru_next != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_next + start_index].lru_prev = 
+				cacheblk->lru_prev;
+		else
+			dmc->cache_sets[set].lru_tail = cacheblk->lru_prev;
+	}
+	/* And add it to LRU Tail */
+	cacheblk->lru_next = FLASHCACHE_LRU_NULL;
+	cacheblk->lru_prev = dmc->cache_sets[set].lru_tail;
+	if (dmc->cache_sets[set].lru_tail == FLASHCACHE_LRU_NULL)
+		dmc->cache_sets[set].lru_head = my_index;
+	else
+		dmc->cache[dmc->cache_sets[set].lru_tail + start_index].lru_next = 
+			my_index;
+	dmc->cache_sets[set].lru_tail = my_index;
+}
+
+static int 
+cmp_dbn(const void *a, const void *b)
+{
+	if (((struct dbn_index_pair *)a)->dbn < ((struct dbn_index_pair *)b)->dbn)
+		return -1;
+	else
+		return 1;
+}
+
+static void
+swap_dbn_index_pair(void *a, void *b, int size)
+{
+	struct dbn_index_pair temp;
+	
+	temp = *(struct dbn_index_pair *)a;
+	*(struct dbn_index_pair *)a = *(struct dbn_index_pair *)b;
+	*(struct dbn_index_pair *)b = temp;
+}
+
+/* 
+ * We have a list of blocks to write out to disk.
+ * 1) Sort the blocks by dbn.
+ * 2) (sysctl'able) See if there are any other blocks in the same set
+ * that are contig to any of the blocks in step 1. If so, include them
+ * in our "to write" set, maintaining sorted order.
+ * Has to be called under the cache spinlock !
+ */
+void
+flashcache_merge_writes(struct cache_c *dmc, struct dbn_index_pair *writes_list, 
+			int *nr_writes, int set)
+{	
+	int start_index = set * dmc->assoc;
+	int end_index = start_index + dmc->assoc;
+	int old_writes = *nr_writes;
+	int new_inserts = 0;
+	struct dbn_index_pair *set_dirty_list = NULL;
+	int ix, nr_set_dirty;
+	struct cacheblock *cacheblk;
+			
+	if (unlikely(*nr_writes == 0))
+		return;
+	sort(writes_list, *nr_writes, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+
+	set_dirty_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_ATOMIC);
+	if (set_dirty_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		goto out;
+	}
+	nr_set_dirty = 0;
+	for (ix = start_index ; ix < end_index ; ix++) {
+		cacheblk = &dmc->cache[ix];
+		/*
+		 * Any DIRTY block in "writes_list" will be marked as 
+		 * DISKWRITEINPROG already, so we'll skip over those here.
+		 */
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			set_dirty_list[nr_set_dirty].dbn = cacheblk->dbn;
+			set_dirty_list[nr_set_dirty].index = ix;
+			nr_set_dirty++;
+		}
+	}
+	if (nr_set_dirty == 0)
+		goto out;
+	sort(set_dirty_list, nr_set_dirty, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+	for (ix = 0 ; ix < nr_set_dirty ; ix++) {
+		int back_merge, k;
+		int i;
+
+		cacheblk = &dmc->cache[set_dirty_list[ix].index];
+		back_merge = -1;
+		VERIFY((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY);
+		for (i = 0 ; i < *nr_writes ; i++) {
+			int insert;
+			int j = 0;
+				
+			insert = 0;
+			if (cacheblk->dbn + dmc->block_size == writes_list[i].dbn) {
+				/* cacheblk to be inserted above i */
+				insert = 1;
+				j = i;
+				back_merge = j;
+			}
+			if (cacheblk->dbn - dmc->block_size == writes_list[i].dbn ) {
+				/* cacheblk to be inserted after i */
+				insert = 1;
+				j = i + 1;
+			}
+			VERIFY(j < dmc->assoc);
+			if (insert) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[ix].index);
+				/* 
+				 * Shift down everthing from j to ((*nr_writes) - 1) to
+				 * make room for the new entry. And add the new entry.
+				 */
+				for (k = (*nr_writes) - 1 ; k >= j ; k--)
+					writes_list[k + 1] = writes_list[k];
+				writes_list[j].dbn = cacheblk->dbn;
+				writes_list[j].index = cacheblk - &dmc->cache[0];
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				if (back_merge == -1)
+					dmc->flashcache_stats.front_merge++;
+				else
+					dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				break;
+			}
+		}
+		/*
+		 * If we did a back merge, we need to walk back in the set's dirty list
+		 * to see if we can pick off any more contig blocks. Forward merges don't
+		 * need this special treatment since we are walking the 2 lists in that 
+		 * direction. It would be nice to roll this logic into the above.
+		 */
+		if (back_merge != -1) {
+			for (k = ix - 1 ; k >= 0 ; k--) {
+				int n;
+
+				if (set_dirty_list[k].dbn + dmc->block_size != 
+				    writes_list[back_merge].dbn)
+					break;
+				dmc->cache[set_dirty_list[k].index].cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[k].index);
+				for (n = (*nr_writes) - 1 ; n >= back_merge ; n--)
+					writes_list[n + 1] = writes_list[n];
+				writes_list[back_merge].dbn = set_dirty_list[k].dbn;
+				writes_list[back_merge].index = set_dirty_list[k].index;
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);				
+			}
+		}
+	}
+	VERIFY((*nr_writes) == (old_writes + new_inserts));
+out:
+	if (set_dirty_list)
+		kfree(set_dirty_list);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+extern struct dm_io_client *flashcache_io_client; /* Client memory pool*/
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int 
+flashcache_dm_io_async_vm(struct cache_c *dmc, unsigned int num_regions, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			  struct io_region *where, 
+#else
+			  struct dm_io_region *where, 
+#endif
+			  int rw,
+			  void *data, io_notify_fn fn, void *context)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = fn,
+		.notify.context = context,
+		.client = flashcache_io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,29)
+/*
+ * Wrappers for doing DM sync IO, using DM async IO.
+ * It is a shame we need do this, but DM sync IO is interruptible :(
+ * And we want uninterruptible disk IO :)
+ * 
+ * This is fixed in 2.6.30, where sync DM IO is uninterruptible.
+ */
+#define FLASHCACHE_DM_IO_SYNC_INPROG	0x01
+
+static DECLARE_WAIT_QUEUE_HEAD(flashcache_dm_io_sync_waitqueue);
+static DEFINE_SPINLOCK(flashcache_dm_io_sync_spinlock);
+
+struct flashcache_dm_io_sync_state {
+	int			error;
+	int			flags;
+};
+
+static void
+flashcache_dm_io_sync_vm_callback(unsigned long error, void *context)
+{
+	struct flashcache_dm_io_sync_state *state = 
+		(struct flashcache_dm_io_sync_state *)context;
+	unsigned long flags;
+	
+	spin_lock_irqsave(&flashcache_dm_io_sync_spinlock, flags);
+	state->flags &= ~FLASHCACHE_DM_IO_SYNC_INPROG;
+	state->error = error;
+	wake_up(&flashcache_dm_io_sync_waitqueue);
+	spin_unlock_irqrestore(&flashcache_dm_io_sync_spinlock, flags);
+}
+
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc, 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			 struct io_region *where, 
+#else
+			  struct dm_io_region *where, 
+#endif
+			 int rw, void *data)
+{
+        DEFINE_WAIT(wait);
+	struct flashcache_dm_io_sync_state state;
+
+	state.error = -EINTR;
+	state.flags = FLASHCACHE_DM_IO_SYNC_INPROG;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	dm_io_async_vm(1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#else
+	flashcache_dm_io_async_vm(dmc, 1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#endif
+	spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	while (state.flags & FLASHCACHE_DM_IO_SYNC_INPROG) {
+		prepare_to_wait(&flashcache_dm_io_sync_waitqueue, &wait, 
+				TASK_UNINTERRUPTIBLE);
+		spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+		schedule();
+		spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	}
+	finish_wait(&flashcache_dm_io_sync_waitqueue, &wait);
+	spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+	return state.error;
+}
+#else
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where, int rw, void *data)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = NULL,
+		.client = flashcache_io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+void
+flashcache_update_sync_progress(struct cache_c *dmc)
+{
+	u_int64_t dirty_pct;
+	
+	if (dmc->flashcache_stats.cleanings % 1000)
+		return;
+	if (!dmc->nr_dirty || !dmc->size || !printk_ratelimit())
+		return;
+	dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+	printk(KERN_INFO "Flashcache: Cleaning %d Dirty blocks, Dirty Blocks pct %llu%%", 
+	       dmc->nr_dirty, dirty_pct);
+	printk(KERN_INFO "\r");
+}
+
+EXPORT_SYMBOL(flashcache_alloc_cache_job);
+EXPORT_SYMBOL(flashcache_free_cache_job);
+EXPORT_SYMBOL(flashcache_alloc_pending_job);
+EXPORT_SYMBOL(flashcache_free_pending_job);
+EXPORT_SYMBOL(pop);
+EXPORT_SYMBOL(push);
+EXPORT_SYMBOL(push_pending);
+EXPORT_SYMBOL(push_io);
+EXPORT_SYMBOL(push_md_io);
+EXPORT_SYMBOL(push_md_complete);
+EXPORT_SYMBOL(process_jobs);
+EXPORT_SYMBOL(do_work);
+EXPORT_SYMBOL(new_kcached_job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm_callback);
+#endif
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm);
+EXPORT_SYMBOL(flashcache_reclaim_lru_movetail);
+EXPORT_SYMBOL(flashcache_merge_writes);
+EXPORT_SYMBOL(flashcache_enq_pending);
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Kconfig linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Kconfig
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Kconfig	2016-12-13 17:22:00.426074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Kconfig	2016-12-13 17:25:21.062077898 +0800
@@ -357,6 +357,12 @@
 	  A target that discards writes, and returns all zeroes for
 	  reads.  Useful in some recovery situations.
 
+config DM_FLASHCACHE
+	tristate "Block level disk caching target (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	  A write back block caching target.
+
 config DM_MULTIPATH
 	tristate "Multipath target"
 	depends on BLK_DEV_DM
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Kconfig.orig linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Kconfig.orig
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Kconfig.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Kconfig.orig	2016-12-13 17:23:54.911074270 +0800
@@ -0,0 +1,434 @@
+#
+# Block device driver configuration
+#
+
+menuconfig MD
+	bool "Multiple devices driver support (RAID and LVM)"
+	depends on BLOCK
+	help
+	  Support multiple physical spindles through a single logical device.
+	  Required for RAID and logical volume management.
+
+if MD
+
+config BLK_DEV_MD
+	tristate "RAID support"
+	---help---
+	  This driver lets you combine several hard disk partitions into one
+	  logical block device. This can be used to simply append one
+	  partition to another one or to combine several redundant hard disks
+	  into a RAID1/4/5 device so as to provide protection against hard
+	  disk failures. This is called "Software RAID" since the combining of
+	  the partitions is done by the kernel. "Hardware RAID" means that the
+	  combining is done by a dedicated controller; if you have such a
+	  controller, you do not need to say Y here.
+
+	  More information about Software RAID on Linux is contained in the
+	  Software RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also learn
+	  where to get the supporting user space utilities raidtools.
+
+	  If unsure, say N.
+
+config MD_AUTODETECT
+	bool "Autodetect RAID arrays during kernel boot"
+	depends on BLK_DEV_MD=y
+	default y
+	---help---
+	  If you say Y here, then the kernel will try to autodetect raid
+	  arrays as part of its boot process. 
+
+	  If you don't use raid and say Y, this autodetection can cause 
+	  a several-second delay in the boot time due to various
+	  synchronisation steps that are part of this step.
+
+	  If unsure, say Y.
+
+config MD_LINEAR
+	tristate "Linear (append) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  If you say Y here, then your multiple devices driver will be able to
+	  use the so-called linear mode, i.e. it will combine the hard disk
+	  partitions by simply appending one to the other.
+
+	  To compile this as a module, choose M here: the module
+	  will be called linear.
+
+	  If unsure, say Y.
+
+config MD_RAID0
+	tristate "RAID-0 (striping) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  If you say Y here, then your multiple devices driver will be able to
+	  use the so-called raid0 mode, i.e. it will combine the hard disk
+	  partitions into one logical device in such a fashion as to fill them
+	  up evenly, one chunk here and one chunk there. This will increase
+	  the throughput rate if the partitions reside on distinct disks.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  To compile this as a module, choose M here: the module
+	  will be called raid0.
+
+	  If unsure, say Y.
+
+config MD_RAID1
+	tristate "RAID-1 (mirroring) mode"
+	depends on BLK_DEV_MD
+	---help---
+	  A RAID-1 set consists of several disk drives which are exact copies
+	  of each other.  In the event of a mirror failure, the RAID driver
+	  will continue to use the operational mirrors in the set, providing
+	  an error free MD (multiple device) to the higher levels of the
+	  kernel.  In a set with N drives, the available space is the capacity
+	  of a single drive, and the set protects against a failure of (N - 1)
+	  drives.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>.  There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  If you want to use such a RAID-1 set, say Y.  To compile this code
+	  as a module, choose M here: the module will be called raid1.
+
+	  If unsure, say Y.
+
+config MD_RAID10
+	tristate "RAID-10 (mirrored striping) mode (EXPERIMENTAL)"
+	depends on BLK_DEV_MD && EXPERIMENTAL
+	---help---
+	  RAID-10 provides a combination of striping (RAID-0) and
+	  mirroring (RAID-1) with easier configuration and more flexible
+	  layout.
+	  Unlike RAID-0, but like RAID-1, RAID-10 requires all devices to
+	  be the same size (or at least, only as much as the smallest device
+	  will be used).
+	  RAID-10 provides a variety of layouts that provide different levels
+	  of redundancy and performance.
+
+	  RAID-10 requires mdadm-1.7.0 or later, available at:
+
+	  ftp://ftp.kernel.org/pub/linux/utils/raid/mdadm/
+
+	  If unsure, say Y.
+
+config MD_RAID456
+	tristate "RAID-4/RAID-5/RAID-6 mode"
+	depends on BLK_DEV_MD
+	select RAID6_PQ
+	select ASYNC_MEMCPY
+	select ASYNC_XOR
+	select ASYNC_PQ
+	select ASYNC_RAID6_RECOV
+	---help---
+	  A RAID-5 set of N drives with a capacity of C MB per drive provides
+	  the capacity of C * (N - 1) MB, and protects against a failure
+	  of a single drive. For a given sector (row) number, (N - 1) drives
+	  contain data sectors, and one drive contains the parity protection.
+	  For a RAID-4 set, the parity blocks are present on a single drive,
+	  while a RAID-5 set distributes the parity across the drives in one
+	  of the available parity distribution methods.
+
+	  A RAID-6 set of N drives with a capacity of C MB per drive
+	  provides the capacity of C * (N - 2) MB, and protects
+	  against a failure of any two drives. For a given sector
+	  (row) number, (N - 2) drives contain data sectors, and two
+	  drives contains two independent redundancy syndromes.  Like
+	  RAID-5, RAID-6 distributes the syndromes across the drives
+	  in one of the available parity distribution methods.
+
+	  Information about Software RAID on Linux is contained in the
+	  Software-RAID mini-HOWTO, available from
+	  <http://www.tldp.org/docs.html#howto>. There you will also
+	  learn where to get the supporting user space utilities raidtools.
+
+	  If you want to use such a RAID-4/RAID-5/RAID-6 set, say Y.  To
+	  compile this code as a module, choose M here: the module
+	  will be called raid456.
+
+	  If unsure, say Y.
+
+config MD_MULTIPATH
+	tristate "Multipath I/O support"
+	depends on BLK_DEV_MD
+	help
+	  Multipath-IO is the ability of certain devices to address the same
+	  physical disk over multiple 'IO paths'. The code ensures that such
+	  paths can be defined and handled at runtime, and ensures that a
+	  transparent failover to the backup path(s) happens if a IO errors
+	  arrives on the primary path.
+
+	  If unsure, say N.
+
+config MD_FAULTY
+	tristate "Faulty test module for MD"
+	depends on BLK_DEV_MD
+	help
+	  The "faulty" module allows for a block device that occasionally returns
+	  read or write errors.  It is useful for testing.
+
+	  In unsure, say N.
+
+config BLK_DEV_DM_BUILTIN
+	boolean
+
+config BLK_DEV_DM
+	tristate "Device mapper support"
+	select BLK_DEV_DM_BUILTIN
+	---help---
+	  Device-mapper is a low level volume manager.  It works by allowing
+	  people to specify mappings for ranges of logical sectors.  Various
+	  mapping types are available, in addition people may write their own
+	  modules containing custom mappings if they wish.
+
+	  Higher level volume managers such as LVM2 use this driver.
+
+	  To compile this as a module, choose M here: the module will be
+	  called dm-mod.
+
+	  If unsure, say N.
+
+config DM_DEBUG
+	boolean "Device mapper debugging support"
+	depends on BLK_DEV_DM
+	---help---
+	  Enable this for messages that may help debug device-mapper problems.
+
+	  If unsure, say N.
+
+config DM_BUFIO
+       tristate
+       depends on BLK_DEV_DM && EXPERIMENTAL
+       ---help---
+	 This interface allows you to do buffered I/O on a device and acts
+	 as a cache, holding recently-read blocks in memory and performing
+	 delayed writes.
+
+config DM_BIO_PRISON
+       tristate
+       depends on BLK_DEV_DM && EXPERIMENTAL
+       ---help---
+	 Some bio locking schemes used by other device-mapper targets
+	 including thin provisioning.
+
+source "drivers/md/persistent-data/Kconfig"
+
+config DM_CRYPT
+	tristate "Crypt target support"
+	depends on BLK_DEV_DM
+	select CRYPTO
+	select CRYPTO_CBC
+	---help---
+	  This device-mapper target allows you to create a device that
+	  transparently encrypts the data on it. You'll need to activate
+	  the ciphers you're going to use in the cryptoapi configuration.
+
+	  For further information on dm-crypt and userspace tools see:
+	  <https://gitlab.com/cryptsetup/cryptsetup/wikis/DMCrypt>
+
+	  To compile this code as a module, choose M here: the module will
+	  be called dm-crypt.
+
+	  If unsure, say N.
+
+config DM_SNAPSHOT
+       tristate "Snapshot target"
+       depends on BLK_DEV_DM
+       select DM_BUFIO
+       ---help---
+         Allow volume managers to take writable snapshots of a device.
+
+config DM_THIN_PROVISIONING
+       tristate "Thin provisioning target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM && EXPERIMENTAL
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         Provides thin provisioning and snapshots that share a data store.
+
+config DM_CACHE
+       tristate "Cache target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         dm-cache attempts to improve performance of a block device by
+         moving frequently used data to a smaller, higher performance
+         device.  Different 'policy' plugins can be used to change the
+         algorithms used to select which blocks are promoted, demoted,
+         cleaned etc.  It supports writeback and writethrough modes.
+
+config DM_CACHE_MQ
+       tristate "MQ Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A cache policy that uses a multiqueue ordered by recent hit
+         count to select which blocks should be promoted and demoted.
+         This is meant to be a general purpose policy.  It prioritises
+         reads over writes.
+
+config DM_CACHE_SMQ
+       tristate "Stochastic MQ Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A cache policy that uses a multiqueue ordered by recent hits
+         to select which blocks should be promoted and demoted.
+         This is meant to be a general purpose policy.  It prioritises
+         reads over writes.  This SMQ policy (vs MQ) offers the promise
+         of less memory utilization, improved performance and increased
+         adaptability in the face of changing workloads.
+
+config DM_CACHE_CLEANER
+       tristate "Cleaner Cache Policy (EXPERIMENTAL)"
+       depends on DM_CACHE
+       default y
+       ---help---
+         A simple cache policy that writes back all data to the
+         origin.  Used when decommissioning a dm-cache.
+
+config DM_ERA
+       tristate "Era target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         dm-era tracks which parts of a block device are written to
+         over time.  Useful for maintaining cache coherency when using
+         vendor snapshots.
+
+config DM_MIRROR
+       tristate "Mirror target"
+       depends on BLK_DEV_DM
+       ---help---
+         Allow volume managers to mirror logical volumes, also
+         needed for live data migration tools such as 'pvmove'.
+
+config DM_RAID
+       tristate "RAID 1/4/5/6/10 target"
+       depends on BLK_DEV_DM
+       select MD_RAID1
+       select MD_RAID10
+       select MD_RAID456
+       select BLK_DEV_MD
+       ---help---
+	 A dm target that supports RAID1, RAID10, RAID4, RAID5 and RAID6 mappings
+
+	 A RAID-5 set of N drives with a capacity of C MB per drive provides
+	 the capacity of C * (N - 1) MB, and protects against a failure
+	 of a single drive. For a given sector (row) number, (N - 1) drives
+	 contain data sectors, and one drive contains the parity protection.
+	 For a RAID-4 set, the parity blocks are present on a single drive,
+	 while a RAID-5 set distributes the parity across the drives in one
+	 of the available parity distribution methods.
+
+	 A RAID-6 set of N drives with a capacity of C MB per drive
+	 provides the capacity of C * (N - 2) MB, and protects
+	 against a failure of any two drives. For a given sector
+	 (row) number, (N - 2) drives contain data sectors, and two
+	 drives contains two independent redundancy syndromes.  Like
+	 RAID-5, RAID-6 distributes the syndromes across the drives
+	 in one of the available parity distribution methods.
+
+config DM_LOG_USERSPACE
+	tristate "Mirror userspace logging (EXPERIMENTAL)"
+	depends on DM_MIRROR && EXPERIMENTAL && NET
+	select CONNECTOR
+	---help---
+	  The userspace logging module provides a mechanism for
+	  relaying the dm-dirty-log API to userspace.  Log designs
+	  which are more suited to userspace implementation (e.g.
+	  shared storage logs) or experimental logs can be implemented
+	  by leveraging this framework.
+
+config DM_ZERO
+	tristate "Zero target"
+	depends on BLK_DEV_DM
+	---help---
+	  A target that discards writes, and returns all zeroes for
+	  reads.  Useful in some recovery situations.
+
+config DM_MULTIPATH
+	tristate "Multipath target"
+	depends on BLK_DEV_DM
+	# nasty syntax but means make DM_MULTIPATH independent
+	# of SCSI_DH if the latter isn't defined but if
+	# it is, DM_MULTIPATH must depend on it.  We get a build
+	# error if SCSI_DH=m and DM_MULTIPATH=y
+	depends on SCSI_DH || !SCSI_DH
+	---help---
+	  Allow volume managers to support multipath hardware.
+
+config DM_MULTIPATH_QL
+	tristate "I/O Path Selector based on the number of in-flight I/Os"
+	depends on DM_MULTIPATH
+	---help---
+	  This path selector is a dynamic load balancer which selects
+	  the path with the least number of in-flight I/Os.
+
+	  If unsure, say N.
+
+config DM_MULTIPATH_ST
+	tristate "I/O Path Selector based on the service time"
+	depends on DM_MULTIPATH
+	---help---
+	  This path selector is a dynamic load balancer which selects
+	  the path expected to complete the incoming I/O in the shortest
+	  time.
+
+	  If unsure, say N.
+
+config DM_DELAY
+	tristate "I/O delaying target (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	A target that delays reads and/or writes and can send
+	them to different devices.  Useful for testing.
+
+	If unsure, say N.
+
+config DM_RAID45
+	tristate "RAID 4/5 target (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	select ASYNC_XOR
+	---help---
+	A target that supports RAID4 and RAID5 mappings.
+
+	If unsure, say N.
+
+config DM_UEVENT
+	bool "DM uevents (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	Generate udev events for DM events.
+
+config DM_FLAKEY
+       tristate "Flakey target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM && EXPERIMENTAL
+       ---help---
+         A target that intermittently fails I/O for debugging purposes.
+
+config DM_SWITCH
+	tristate "Switch target support (EXPERIMENTAL)"
+	depends on BLK_DEV_DM
+	---help---
+	  This device-mapper target creates a device that supports an arbitrary
+	  mapping of fixed-size regions of I/O across a fixed set of paths.
+	  The path used for any specific region can be switched dynamically
+	  by sending the target a message.
+
+	  To compile this code as a module, choose M here: the module will
+	  be called dm-switch.
+
+	  If unsure, say N.
+
+endif # MD
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Makefile linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Makefile
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Makefile	2016-12-13 17:22:00.427074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Makefile	2016-12-13 17:26:00.677073279 +0800
@@ -9,6 +9,8 @@
 dm-mirror-y	+= dm-raid1.o
 dm-log-userspace-y \
 		+= dm-log-userspace-base.o dm-log-userspace-transfer.o
+flashcache-y    +=  flashcache_conf.o flashcache_main.o flashcache_subr.o \
+                   flashcache_ioctl.o flashcache_procfs.o
 dm-thin-pool-y	+= dm-thin.o dm-thin-metadata.o
 dm-cache-y	+= dm-cache-target.o dm-cache-metadata.o dm-cache-policy.o
 dm-cache-mq-y   += dm-cache-policy-mq.o
@@ -47,6 +49,7 @@
 obj-$(CONFIG_DM_MIRROR)		+= dm-mirror.o dm-log.o dm-region-hash.o
 obj-$(CONFIG_DM_LOG_USERSPACE)	+= dm-log-userspace.o
 obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
+obj-$(CONFIG_DM_FLASHCACHE)	+= flashcache.o
 obj-$(CONFIG_DM_RAID)		+= dm-raid.o
 obj-$(CONFIG_DM_THIN_PROVISIONING)	+= dm-thin-pool.o
 obj-$(CONFIG_DM_CACHE)		+= dm-cache.o
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Makefile.orig linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Makefile.orig
--- linux-2.6.32-642.11.1.el6.x86_64/drivers/md/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/drivers/md/Makefile.orig	2016-12-13 17:23:54.913074270 +0800
@@ -0,0 +1,63 @@
+## Makefile for the kernel software RAID and LVM drivers.
+#
+
+dm-mod-y	+= dm.o dm-table.o dm-target.o dm-linear.o dm-stripe.o \
+		   dm-ioctl.o dm-io.o dm-kcopyd.o dm-sysfs.o dm-stats.o
+dm-multipath-y	+= dm-path-selector.o dm-mpath.o
+dm-snapshot-y	+= dm-snap.o dm-exception-store.o dm-snap-transient.o \
+		    dm-snap-persistent.o
+dm-mirror-y	+= dm-raid1.o
+dm-log-userspace-y \
+		+= dm-log-userspace-base.o dm-log-userspace-transfer.o
+dm-thin-pool-y	+= dm-thin.o dm-thin-metadata.o
+dm-cache-y	+= dm-cache-target.o dm-cache-metadata.o dm-cache-policy.o
+dm-cache-mq-y   += dm-cache-policy-mq.o
+dm-cache-smq-y   += dm-cache-policy-smq.o
+dm-cache-cleaner-y += dm-cache-policy-cleaner.o
+dm-era-y	+= dm-era-target.o
+md-mod-y	+= md.o bitmap.o
+raid456-y	+= raid5.o
+
+# Note: link order is important.  All raid personalities
+# and must come before md.o, as they each initialise 
+# themselves, and md.o may use the personalities when it 
+# auto-initialised.
+
+obj-$(CONFIG_MD_LINEAR)		+= linear.o
+obj-$(CONFIG_MD_RAID0)		+= raid0.o
+obj-$(CONFIG_MD_RAID1)		+= raid1.o
+obj-$(CONFIG_MD_RAID10)		+= raid10.o
+obj-$(CONFIG_MD_RAID456)	+= raid456.o
+obj-$(CONFIG_MD_MULTIPATH)	+= multipath.o
+obj-$(CONFIG_MD_FAULTY)		+= faulty.o
+obj-$(CONFIG_BLK_DEV_MD)	+= md-mod.o
+obj-$(CONFIG_BLK_DEV_DM)	+= dm-mod.o
+obj-$(CONFIG_BLK_DEV_DM_BUILTIN) += dm-builtin.o
+obj-$(CONFIG_DM_BUFIO)		+= dm-bufio.o
+obj-$(CONFIG_DM_BIO_PRISON)	+= dm-bio-prison.o
+obj-$(CONFIG_DM_CRYPT)		+= dm-crypt.o
+obj-$(CONFIG_DM_DELAY)		+= dm-delay.o
+obj-$(CONFIG_DM_FLAKEY)		+= dm-flakey.o
+obj-$(CONFIG_DM_MULTIPATH)	+= dm-multipath.o dm-round-robin.o
+obj-$(CONFIG_DM_MULTIPATH_QL)	+= dm-queue-length.o
+obj-$(CONFIG_DM_MULTIPATH_ST)	+= dm-service-time.o
+obj-$(CONFIG_DM_SWITCH)		+= dm-switch.o
+obj-$(CONFIG_DM_SNAPSHOT)	+= dm-snapshot.o
+obj-$(CONFIG_DM_PERSISTENT_DATA)	+= persistent-data/
+obj-$(CONFIG_DM_MIRROR)		+= dm-mirror.o dm-log.o dm-region-hash.o
+obj-$(CONFIG_DM_LOG_USERSPACE)	+= dm-log-userspace.o
+obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
+obj-$(CONFIG_DM_RAID)		+= dm-raid.o
+obj-$(CONFIG_DM_THIN_PROVISIONING)	+= dm-thin-pool.o
+obj-$(CONFIG_DM_CACHE)		+= dm-cache.o
+obj-$(CONFIG_DM_CACHE_MQ)	+= dm-cache-mq.o
+obj-$(CONFIG_DM_CACHE_SMQ)	+= dm-cache-smq.o
+obj-$(CONFIG_DM_CACHE_CLEANER)	+= dm-cache-cleaner.o
+obj-$(CONFIG_DM_ERA)		+= dm-era.o
+obj-$(CONFIG_DM_RAID45)		+= dm-raid45.o dm-log.o dm-memcache.o \
+				   dm-region-hash.o
+
+ifeq ($(CONFIG_DM_UEVENT),y)
+dm-mod-objs			+= dm-uevent.o
+endif
+
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/linux/hookers.h linux-2.6.32-642.11.1.el6.toa.x86_64/include/linux/hookers.h
--- linux-2.6.32-642.11.1.el6.x86_64/include/linux/hookers.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/linux/hookers.h	2016-12-13 17:25:10.422084237 +0800
@@ -0,0 +1,152 @@
+/*
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ *
+ *	Changes:
+ *		Li Yu :		Starting up.
+ */
+
+#ifndef _LINUX_HOOKER_H_
+#define _LINUX_HOOKER_H_
+
+#include <linux/types.h>
+
+struct hooked_place;
+
+/*
+ * This API allows us replace and restore the function pointer in any order.
+ *
+ * This is designed to satisfy hooker stack usage pattern. e.g.
+ *
+ *	In our TCP implemention, icsk_af_ops->syn_recv_sock is called
+ *	when thea three way handshake has completed, we need to hook it
+ *      sometimes, e.g. to compute some statistics data on fly, even to
+ *      add a private TCP option.
+ *
+ *  By hooking this function, we can attain the goal without any kernel
+ *  change or just some small changes, and hope that this can help to
+ *  reduce the cost of maintaining custom kernel release too. Of course,
+ *  this can't replace that design necessary extendible framework, but I
+ *  think that hooking is a good and cheep choice of starting all.
+ *
+ *	Assume that we have two hooks, we expect that the hooking could
+ *      produce below behavior:
+ *
+ *	First, install two hookers:
+ *
+ *          install(&syn_recv_sock, hook1)
+ *          install(&syn_recv_sock, hook2)
+ *
+ *	Now, we expect the invoking order is:
+ *
+ *	     orig_syn_recv_sock() , hook2() , hook1()
+ *
+ *	Then, remove a hooker:
+ *
+ *          uninstall(&syn_recv_sock, hook1)
+ *
+ *      Then, the invoking order should be:
+ *
+ *	   orig_syn_recv_sock(), hook2()
+ *
+ *	Last, remove all rest hookers:
+ *
+ *          uninstall(&syn_recv_sock, hook2)
+ *
+ *      The result just is:
+ *
+ *	    orig_syn_recv_sock()
+ *
+ *      See, it is function pointer stack here. however, if we just simplely
+ *	used address of hooker1 in "top" hooker function (hooker2),
+ *	we will get an invalid memory access exception when prior hookers
+ *      (hooker1) is uninstalled first. Under second simple design, we just
+ *      support the some fixed predefined hooking addresses, and manage hookers
+ *      by a simple linked list.
+ *
+ *
+ * Usage:
+ *
+ *	1. Install a hooker on address which you are interesting in.
+ *	   Assume that the kernel has a callback table as below:
+ *
+ *		struct icsk_ops {
+ *			...
+			 *int (*foo)(int a, char b);
+ *			...
+ *		};
+ *
+ *		struct icsk_ops icsk_ops = {
+ *			...
+ *			.foo = real_foo,
+ *			...
+ *		};
+ *
+ *	   Then we should hook &icsk_ops.foo by such way:
+ *
+ *		static int foo_hooker(int a, char b, int *p_ret)
+ *		{
+ *			int ret = *p_ret;
+ *
+ *			//do something that may overwrite return value.
+ *			//p_ret saves the result value of original function
+ *			//or other hookers.
+ *
+ *			//You should not have any assume for invoking order
+ *			//of hookers.
+ *
+ *			return ret;
+ *		}
+ *
+ *		struct hooker h = {
+ *			.func = foo_hooker,
+ *		};
+ *
+ *		hooker_install(&icsk_ops.foo ,&h);
+ *
+ *		The hooker and original function has same function signature, if
+ *		the original function has not return value, IOW, it's like
+ *
+ *			void foo(int a, char b) { ... }
+ *
+ *	2. Uninstall hooker is easy, just:
+ *
+ *		hooker_uninstall(&h);
+ *
+ */
+
+struct hooker {
+	struct hooked_place *hplace;
+	void *func;	/* the installed hooker function pointer */
+	struct list_head chain;
+};
+
+/*
+ * Install the hooker function at specified address.
+ * This function may sleep.
+ *
+ * Parameters:
+ *	place - the address that saves function pointer
+ *	hooker - the hooker to install, the caller must fill
+ *		 its func member first
+ *
+ * Return:
+ *	    0  - All OK, please note that hooker func may be called before
+ *		 this return
+ *	  < 0 -  any error, e.g. out of memory, existing same installed hooker
+ */
+extern int hooker_install(void *place, struct hooker *hooker);
+
+/*
+ * Remove the installed hooker function that saved in hooker->func.
+ * This function may sleep.
+ *
+ * Parameters:
+ *	place - the address that saves function pointer
+ *	hooker - the installed hooker struct
+ */
+extern void hooker_uninstall(struct hooker *hooker);
+
+#endif
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/inet_common.h linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/inet_common.h
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/inet_common.h	2016-12-13 17:21:59.238074679 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/inet_common.h	2016-12-13 17:25:15.710067502 +0800
@@ -1,7 +1,7 @@
 #ifndef _INET_COMMON_H
 #define _INET_COMMON_H
 
-extern const struct proto_ops		inet_stream_ops;
+extern struct proto_ops		inet_stream_ops;
 extern const struct proto_ops		inet_dgram_ops;
 
 /*
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/ipv6.h linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/ipv6.h
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/ipv6.h	2016-12-13 17:21:59.228075013 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/ipv6.h	2016-12-13 17:25:15.711067609 +0800
@@ -680,7 +680,7 @@
 /*
  * reassembly.c
  */
-extern const struct proto_ops inet6_stream_ops;
+extern struct proto_ops inet6_stream_ops;
 extern const struct proto_ops inet6_dgram_ops;
 
 struct group_source_req;
@@ -730,5 +730,11 @@
 extern void ipv6_static_sysctl_unregister(void);
 #endif
 
+/* public func in tcp_ipv6.c */
+extern struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+					struct request_sock *req,
+					struct dst_entry *dst);
+extern struct inet_connection_sock_af_ops ipv6_specific;
+
 #endif /* __KERNEL__ */
 #endif /* _NET_IPV6_H */
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/ipv6.h.orig linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/ipv6.h.orig
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/ipv6.h.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/ipv6.h.orig	2016-12-13 17:23:53.740074309 +0800
@@ -0,0 +1,734 @@
+/*
+ *	Linux INET6 implementation
+ *
+ *	Authors:
+ *	Pedro Roque		<roque@di.fc.ul.pt>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _NET_IPV6_H
+#define _NET_IPV6_H
+
+#include <linux/ipv6.h>
+#include <linux/hardirq.h>
+#include <net/if_inet6.h>
+#include <net/ndisc.h>
+#include <net/flow.h>
+#include <net/snmp.h>
+
+#define SIN6_LEN_RFC2133	24
+
+#define IPV6_MAXPLEN		65535
+
+/*
+ *	NextHeader field of IPv6 header
+ */
+
+#define NEXTHDR_HOP		0	/* Hop-by-hop option header. */
+#define NEXTHDR_TCP		6	/* TCP segment. */
+#define NEXTHDR_UDP		17	/* UDP message. */
+#define NEXTHDR_IPV6		41	/* IPv6 in IPv6 */
+#define NEXTHDR_ROUTING		43	/* Routing header. */
+#define NEXTHDR_FRAGMENT	44	/* Fragmentation/reassembly header. */
+#define NEXTHDR_ESP		50	/* Encapsulating security payload. */
+#define NEXTHDR_AUTH		51	/* Authentication header. */
+#define NEXTHDR_ICMP		58	/* ICMP for IPv6. */
+#define NEXTHDR_NONE		59	/* No next header */
+#define NEXTHDR_DEST		60	/* Destination options header. */
+#define NEXTHDR_SCTP		132	/* SCTP message. */
+#define NEXTHDR_MOBILITY	135	/* Mobility header. */
+
+#define NEXTHDR_MAX		255
+
+
+
+#define IPV6_DEFAULT_HOPLIMIT   64
+#define IPV6_DEFAULT_MCASTHOPS	1
+
+/*
+ *	Addr type
+ *	
+ *	type	-	unicast | multicast
+ *	scope	-	local	| site	    | global
+ *	v4	-	compat
+ *	v4mapped
+ *	any
+ *	loopback
+ */
+
+#define IPV6_ADDR_ANY		0x0000U
+
+#define IPV6_ADDR_UNICAST      	0x0001U	
+#define IPV6_ADDR_MULTICAST    	0x0002U	
+
+#define IPV6_ADDR_LOOPBACK	0x0010U
+#define IPV6_ADDR_LINKLOCAL	0x0020U
+#define IPV6_ADDR_SITELOCAL	0x0040U
+
+#define IPV6_ADDR_COMPATv4	0x0080U
+
+#define IPV6_ADDR_SCOPE_MASK	0x00f0U
+
+#define IPV6_ADDR_MAPPED	0x1000U
+
+/*
+ *	Addr scopes
+ */
+#ifdef __KERNEL__
+#define IPV6_ADDR_MC_SCOPE(a)	\
+	((a)->s6_addr[1] & 0x0f)	/* nonstandard */
+#define __IPV6_ADDR_SCOPE_INVALID	-1
+#endif
+#define IPV6_ADDR_SCOPE_NODELOCAL	0x01
+#define IPV6_ADDR_SCOPE_LINKLOCAL	0x02
+#define IPV6_ADDR_SCOPE_SITELOCAL	0x05
+#define IPV6_ADDR_SCOPE_ORGLOCAL	0x08
+#define IPV6_ADDR_SCOPE_GLOBAL		0x0e
+
+/*
+ *	Addr flags
+ */
+#ifdef __KERNEL__
+#define IPV6_ADDR_MC_FLAG_TRANSIENT(a)	\
+	((a)->s6_addr[1] & 0x10)
+#define IPV6_ADDR_MC_FLAG_PREFIX(a)	\
+	((a)->s6_addr[1] & 0x20)
+#define IPV6_ADDR_MC_FLAG_RENDEZVOUS(a)	\
+	((a)->s6_addr[1] & 0x40)
+#endif
+
+/*
+ *	fragmentation header
+ */
+
+struct frag_hdr {
+	__u8	nexthdr;
+	__u8	reserved;
+	__be16	frag_off;
+	__be32	identification;
+};
+
+#define	IP6_MF	0x0001
+
+#ifdef __KERNEL__
+
+#include <net/sock.h>
+
+/* sysctls */
+extern int sysctl_mld_max_msf;
+extern struct ctl_path net_ipv6_ctl_path[];
+
+#define _DEVINC(net, statname, modifier, idev, field)			\
+({									\
+	struct inet6_dev *_idev = (idev);				\
+	if (likely(_idev != NULL))					\
+		SNMP_INC_STATS##modifier((_idev)->stats.statname, (field)); \
+	SNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\
+})
+
+#define _DEVADD(net, statname, modifier, idev, field, val)		\
+({									\
+	struct inet6_dev *_idev = (idev);				\
+	if (likely(_idev != NULL))					\
+		SNMP_ADD_STATS##modifier((_idev)->stats.statname, (field), (val)); \
+	SNMP_ADD_STATS##modifier((net)->mib.statname##_statistics, (field), (val));\
+})
+
+#define _DEVUPD(net, statname, modifier, idev, field, val)		\
+({									\
+	struct inet6_dev *_idev = (idev);				\
+	if (likely(_idev != NULL))					\
+		SNMP_UPD_PO_STATS##modifier((_idev)->stats.statname, field, (val)); \
+	SNMP_UPD_PO_STATS##modifier((net)->mib.statname##_statistics, field, (val));\
+})
+
+/* MIBs */
+
+#define IP6_INC_STATS(net, idev,field)		\
+		_DEVINC(net, ipv6, , idev, field)
+#define IP6_INC_STATS_BH(net, idev,field)	\
+		_DEVINC(net, ipv6, _BH, idev, field)
+#define IP6_ADD_STATS(net, idev,field,val)	\
+		_DEVADD(net, ipv6, , idev, field, val)
+#define IP6_ADD_STATS_BH(net, idev,field,val)	\
+		_DEVADD(net, ipv6, _BH, idev, field, val)
+#define IP6_UPD_PO_STATS(net, idev,field,val)   \
+		_DEVUPD(net, ipv6, , idev, field, val)
+#define IP6_UPD_PO_STATS_BH(net, idev,field,val)   \
+		_DEVUPD(net, ipv6, _BH, idev, field, val)
+#define ICMP6_INC_STATS(net, idev, field)	\
+		_DEVINC(net, icmpv6, , idev, field)
+#define ICMP6_INC_STATS_BH(net, idev, field)	\
+		_DEVINC(net, icmpv6, _BH, idev, field)
+
+#define ICMP6MSGOUT_INC_STATS(net, idev, field)		\
+	_DEVINC(net, icmpv6msg, , idev, field +256)
+#define ICMP6MSGOUT_INC_STATS_BH(net, idev, field)	\
+	_DEVINC(net, icmpv6msg, _BH, idev, field +256)
+#define ICMP6MSGIN_INC_STATS_BH(net, idev, field)	\
+	_DEVINC(net, icmpv6msg, _BH, idev, field)
+
+struct ip6_ra_chain
+{
+	struct ip6_ra_chain	*next;
+	struct sock		*sk;
+	int			sel;
+	void			(*destructor)(struct sock *);
+};
+
+extern struct ip6_ra_chain	*ip6_ra_chain;
+extern rwlock_t ip6_ra_lock;
+
+/*
+   This structure is prepared by protocol, when parsing
+   ancillary data and passed to IPv6.
+ */
+
+struct ipv6_txoptions
+{
+	atomic_t		refcnt;
+	/* Length of this structure */
+	int			tot_len;
+
+	/* length of extension headers   */
+
+	__u16			opt_flen;	/* after fragment hdr */
+	__u16			opt_nflen;	/* before fragment hdr */
+
+	struct ipv6_opt_hdr	*hopopt;
+	struct ipv6_opt_hdr	*dst0opt;
+	struct ipv6_rt_hdr	*srcrt;	/* Routing Header */
+	struct ipv6_opt_hdr	*dst1opt;
+	struct rcu_head		rcu;
+	/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */
+};
+
+struct ip6_flowlabel
+{
+	struct ip6_flowlabel	*next;
+	__be32			label;
+	atomic_t		users;
+	struct in6_addr		dst;
+	struct ipv6_txoptions	*opt;
+	unsigned long		linger;
+	u8			share;
+	u32			owner;
+	unsigned long		lastuse;
+	unsigned long		expires;
+	struct net		*fl_net;
+};
+
+#define IPV6_FLOWINFO_MASK	cpu_to_be32(0x0FFFFFFF)
+#define IPV6_FLOWLABEL_MASK	cpu_to_be32(0x000FFFFF)
+
+struct ipv6_fl_socklist
+{
+	struct ipv6_fl_socklist	*next;
+	struct ip6_flowlabel	*fl;
+};
+
+static inline struct ipv6_txoptions *txopt_get(const struct ipv6_pinfo *np)
+{
+	struct ipv6_txoptions *opt;
+
+	rcu_read_lock();
+	opt = rcu_dereference(np->opt);
+	if (opt && !atomic_inc_not_zero(&opt->refcnt))
+		opt = NULL;
+	rcu_read_unlock();
+	return opt;
+}
+
+static inline void txopt_put(struct ipv6_txoptions *opt)
+{
+	if (opt && atomic_dec_and_test(&opt->refcnt))
+		kfree_rcu(opt, rcu);
+}
+
+extern struct ip6_flowlabel	*fl6_sock_lookup(struct sock *sk, __be32 label);
+extern struct ipv6_txoptions	*fl6_merge_options(struct ipv6_txoptions * opt_space,
+						   struct ip6_flowlabel * fl,
+						   struct ipv6_txoptions * fopt);
+extern void			fl6_free_socklist(struct sock *sk);
+extern int			ipv6_flowlabel_opt(struct sock *sk, char __user *optval, int optlen);
+extern int			ip6_flowlabel_init(void);
+extern void			ip6_flowlabel_cleanup(void);
+
+static inline void fl6_sock_release(struct ip6_flowlabel *fl)
+{
+	if (fl)
+		atomic_dec(&fl->users);
+}
+
+extern int 			ip6_ra_control(struct sock *sk, int sel);
+
+extern int			ipv6_parse_hopopts(struct sk_buff *skb);
+
+extern struct ipv6_txoptions *  ipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt);
+extern struct ipv6_txoptions *	ipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,
+						   int newtype,
+						   struct ipv6_opt_hdr __user *newopt,
+						   int newoptlen);
+struct ipv6_txoptions *ipv6_fixup_options(struct ipv6_txoptions *opt_space,
+					  struct ipv6_txoptions *opt);
+
+extern int ipv6_opt_accepted(struct sock *sk, struct sk_buff *skb);
+
+static inline bool ipv6_accept_ra(struct inet6_dev *idev)
+{
+	/* If forwarding is enabled, RA are not accepted unless the special
+	 * hybrid mode (accept_ra=2) is enabled.
+	 */
+	return idev->cnf.forwarding ? idev->cnf.accept_ra == 2 :
+	    idev->cnf.accept_ra;
+}
+
+#if IS_ENABLED(CONFIG_IPV6)
+static inline int ip6_frag_nqueues(struct net *net)
+{
+	struct netns_frags_priv *nf_priv = netns_frags_priv(&net->ipv6.frags);
+	return nf_priv->nqueues;
+}
+
+static inline int ip6_frag_mem(struct net *net)
+{
+	return sum_frag_mem_limit(&net->ipv6.frags);
+}
+#endif
+
+#define IPV6_FRAG_HIGH_THRESH	(4 * 1024*1024)	/* 4194304 */
+#define IPV6_FRAG_LOW_THRESH	(3 * 1024*1024)	/* 3145728 */
+#define IPV6_FRAG_TIMEOUT	(60 * HZ)	/* 60 seconds */
+
+extern int __ipv6_addr_type(const struct in6_addr *addr);
+static inline int ipv6_addr_type(const struct in6_addr *addr)
+{
+	return __ipv6_addr_type(addr) & 0xffff;
+}
+
+static inline int ipv6_addr_scope(const struct in6_addr *addr)
+{
+	return __ipv6_addr_type(addr) & IPV6_ADDR_SCOPE_MASK;
+}
+
+static inline int __ipv6_addr_src_scope(int type)
+{
+	return (type == IPV6_ADDR_ANY ? __IPV6_ADDR_SCOPE_INVALID : (type >> 16));
+}
+
+static inline int ipv6_addr_src_scope(const struct in6_addr *addr)
+{
+	return __ipv6_addr_src_scope(__ipv6_addr_type(addr));
+}
+
+static inline int ipv6_addr_cmp(const struct in6_addr *a1, const struct in6_addr *a2)
+{
+	return memcmp(a1, a2, sizeof(struct in6_addr));
+}
+
+static inline int
+ipv6_masked_addr_cmp(const struct in6_addr *a1, const struct in6_addr *m,
+		     const struct in6_addr *a2)
+{
+	return (!!(((a1->s6_addr32[0] ^ a2->s6_addr32[0]) & m->s6_addr32[0]) |
+		   ((a1->s6_addr32[1] ^ a2->s6_addr32[1]) & m->s6_addr32[1]) |
+		   ((a1->s6_addr32[2] ^ a2->s6_addr32[2]) & m->s6_addr32[2]) |
+		   ((a1->s6_addr32[3] ^ a2->s6_addr32[3]) & m->s6_addr32[3])));
+}
+
+static inline void ipv6_addr_copy(struct in6_addr *a1, const struct in6_addr *a2)
+{
+	memcpy(a1, a2, sizeof(struct in6_addr));
+}
+
+static inline void ipv6_addr_prefix(struct in6_addr *pfx, 
+				    const struct in6_addr *addr,
+				    int plen)
+{
+	/* caller must guarantee 0 <= plen <= 128 */
+	int o = plen >> 3,
+	    b = plen & 0x7;
+
+	memset(pfx->s6_addr, 0, sizeof(pfx->s6_addr));
+	memcpy(pfx->s6_addr, addr, o);
+	if (b != 0)
+		pfx->s6_addr[o] = addr->s6_addr[o] & (0xff00 >> b);
+}
+
+static inline void ipv6_addr_set(struct in6_addr *addr, 
+				     __be32 w1, __be32 w2,
+				     __be32 w3, __be32 w4)
+{
+	addr->s6_addr32[0] = w1;
+	addr->s6_addr32[1] = w2;
+	addr->s6_addr32[2] = w3;
+	addr->s6_addr32[3] = w4;
+}
+
+static inline int ipv6_addr_equal(const struct in6_addr *a1,
+				  const struct in6_addr *a2)
+{
+	return (((a1->s6_addr32[0] ^ a2->s6_addr32[0]) |
+		 (a1->s6_addr32[1] ^ a2->s6_addr32[1]) |
+		 (a1->s6_addr32[2] ^ a2->s6_addr32[2]) |
+		 (a1->s6_addr32[3] ^ a2->s6_addr32[3])) == 0);
+}
+
+static inline int __ipv6_prefix_equal(const __be32 *a1, const __be32 *a2,
+				      unsigned int prefixlen)
+{
+	unsigned pdw, pbi;
+
+	/* check complete u32 in prefix */
+	pdw = prefixlen >> 5;
+	if (pdw && memcmp(a1, a2, pdw << 2))
+		return 0;
+
+	/* check incomplete u32 in prefix */
+	pbi = prefixlen & 0x1f;
+	if (pbi && ((a1[pdw] ^ a2[pdw]) & htonl((0xffffffff) << (32 - pbi))))
+		return 0;
+
+	return 1;
+}
+
+static inline int ipv6_prefix_equal(const struct in6_addr *a1,
+				    const struct in6_addr *a2,
+				    unsigned int prefixlen)
+{
+	return __ipv6_prefix_equal(a1->s6_addr32, a2->s6_addr32,
+				   prefixlen);
+}
+
+struct inet_frag_queue;
+
+enum ip6_defrag_users {
+	IP6_DEFRAG_LOCAL_DELIVER,
+	IP6_DEFRAG_CONNTRACK_IN,
+	IP6_DEFRAG_CONNTRACK_OUT,
+	IP6_DEFRAG_CONNTRACK_BRIDGE_IN,
+};
+
+struct ip6_create_arg {
+	__be32 id;
+	u32 user;
+	struct in6_addr *src;
+	struct in6_addr *dst;
+};
+
+void ip6_frag_init(struct inet_frag_queue *q, void *a);
+bool ip6_frag_match(struct inet_frag_queue *q, void *a);
+
+static inline int ipv6_addr_any(const struct in6_addr *a)
+{
+	return ((a->s6_addr32[0] | a->s6_addr32[1] | 
+		 a->s6_addr32[2] | a->s6_addr32[3] ) == 0); 
+}
+
+static inline int ipv6_addr_loopback(const struct in6_addr *a)
+{
+	return ((a->s6_addr32[0] | a->s6_addr32[1] |
+		 a->s6_addr32[2] | (a->s6_addr32[3] ^ htonl(1))) == 0);
+}
+
+
+/*
+ *	Equivalent of ipv4 struct ip
+ */
+struct frag_queue {
+	struct inet_frag_queue	q;
+
+	__be32			id;		/* fragment id		*/
+	u32			user;
+	struct in6_addr		saddr;
+	struct in6_addr		daddr;
+
+	int			iif;
+	unsigned int		csum;
+	__u16			nhoffset;
+};
+
+void ip6_expire_frag_queue(struct net *net, struct frag_queue *fq,
+			   struct inet_frags *frags);
+
+static inline int ipv6_addr_v4mapped(const struct in6_addr *a)
+{
+	return ((a->s6_addr32[0] | a->s6_addr32[1] |
+		 (a->s6_addr32[2] ^ htonl(0x0000ffff))) == 0);
+}
+
+/*
+ * Check for a RFC 4843 ORCHID address
+ * (Overlay Routable Cryptographic Hash Identifiers)
+ */
+static inline int ipv6_addr_orchid(const struct in6_addr *a)
+{
+	return ((a->s6_addr32[0] & htonl(0xfffffff0))
+		== htonl(0x20010010));
+}
+
+static inline void ipv6_addr_set_v4mapped(const __be32 addr,
+					  struct in6_addr *v4mapped)
+{
+	ipv6_addr_set(v4mapped,
+			0, 0,
+			htonl(0x0000FFFF),
+			addr);
+}
+
+/*
+ * find the first different bit between two addresses
+ * length of address must be a multiple of 32bits
+ */
+static inline int __ipv6_addr_diff(const void *token1, const void *token2, int addrlen)
+{
+	const __be32 *a1 = token1, *a2 = token2;
+	int i;
+
+	addrlen >>= 2;
+
+	for (i = 0; i < addrlen; i++) {
+		__be32 xb = a1[i] ^ a2[i];
+		if (xb)
+			return i * 32 + 32 - fls(ntohl(xb));
+	}
+
+	/*
+	 *	we should *never* get to this point since that 
+	 *	would mean the addrs are equal
+	 *
+	 *	However, we do get to it 8) And exacly, when
+	 *	addresses are equal 8)
+	 *
+	 *	ip route add 1111::/128 via ...
+	 *	ip route add 1111::/64 via ...
+	 *	and we are here.
+	 *
+	 *	Ideally, this function should stop comparison
+	 *	at prefix length. It does not, but it is still OK,
+	 *	if returned value is greater than prefix length.
+	 *					--ANK (980803)
+	 */
+	return (addrlen << 5);
+}
+
+static inline int ipv6_addr_diff(const struct in6_addr *a1, const struct in6_addr *a2)
+{
+	return __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));
+}
+
+extern void ipv6_select_ident(struct frag_hdr *fhdr, struct in6_addr *addr);
+
+static inline __be32 ip6_flowlabel(const struct ipv6hdr *hdr)
+{
+	return *(__be32 *)hdr & IPV6_FLOWLABEL_MASK;
+}
+
+/*
+ *	Prototypes exported by ipv6
+ */
+
+/*
+ *	rcv function (called from netdevice level)
+ */
+
+extern int			ipv6_rcv(struct sk_buff *skb, 
+					 struct net_device *dev, 
+					 struct packet_type *pt,
+					 struct net_device *orig_dev);
+
+extern int			ip6_rcv_finish(struct sk_buff *skb);
+
+/*
+ *	upper-layer output functions
+ */
+extern int			ip6_xmit(struct sock *sk,
+					 struct sk_buff *skb,
+					 struct flowi *fl,
+					 struct ipv6_txoptions *opt,
+					 int ipfragok);
+
+extern int			ip6_nd_hdr(struct sock *sk,
+					   struct sk_buff *skb,
+					   struct net_device *dev,
+					   const struct in6_addr *saddr,
+					   const struct in6_addr *daddr,
+					   int proto, int len);
+
+extern int			ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr);
+
+extern int			ip6_append_data(struct sock *sk,
+						int getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb),
+		    				void *from,
+						int length,
+						int transhdrlen,
+		      				int hlimit,
+		      				int tclass,
+						struct ipv6_txoptions *opt,
+						struct flowi *fl,
+						struct rt6_info *rt,
+						unsigned int flags);
+
+extern int			ip6_push_pending_frames(struct sock *sk);
+
+extern void			ip6_flush_pending_frames(struct sock *sk);
+
+extern int			ip6_dst_lookup(struct sock *sk,
+					       struct dst_entry **dst,
+					       struct flowi *fl);
+extern int			ip6_dst_blackhole(struct sock *sk,
+						  struct dst_entry **dst,
+						  struct flowi *fl);
+extern int			ip6_sk_dst_lookup(struct sock *sk,
+						  struct dst_entry **dst,
+						  struct flowi *fl);
+
+/*
+ *	skb processing functions
+ */
+
+extern int			ip6_output(struct sk_buff *skb);
+extern int			ip6_forward(struct sk_buff *skb);
+extern int			ip6_input(struct sk_buff *skb);
+extern int			ip6_mc_input(struct sk_buff *skb);
+
+extern int			__ip6_local_out(struct sk_buff *skb);
+extern int			ip6_local_out(struct sk_buff *skb);
+
+/*
+ *	Extension header (options) processing
+ */
+
+extern void 			ipv6_push_nfrag_opts(struct sk_buff *skb,
+						     struct ipv6_txoptions *opt,
+						     u8 *proto,
+						     struct in6_addr **daddr_p);
+extern void			ipv6_push_frag_opts(struct sk_buff *skb,
+						    struct ipv6_txoptions *opt,
+						    u8 *proto);
+
+extern int			ipv6_skip_exthdr(const struct sk_buff *, int start,
+					         u8 *nexthdrp);
+extern int			ipv6_skip_exthdr_fragoff(
+					const struct sk_buff *, int start,
+					u8 *nexthdrp, __be16 *frag_offp);
+
+extern int 			ipv6_ext_hdr(u8 nexthdr);
+
+enum {
+	IP6_FH_F_FRAG		= (1 << 0),
+	IP6_FH_F_AUTH		= (1 << 1),
+	IP6_FH_F_SKIP_RH	= (1 << 2),
+};
+
+/* find specified header and get offset to it */
+extern int ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset,
+			 int target, unsigned short *fragoff, int *fragflg);
+
+extern int ipv6_find_tlv(struct sk_buff *skb, int offset, int type);
+
+extern struct in6_addr *fl6_update_dst(struct flowi *fl,
+				       const struct ipv6_txoptions *opt,
+				       struct in6_addr *orig);
+
+/*
+ *	socket options (ipv6_sockglue.c)
+ */
+
+extern int			ipv6_setsockopt(struct sock *sk, int level, 
+						int optname,
+						char __user *optval, 
+						unsigned int optlen);
+extern int			ipv6_getsockopt(struct sock *sk, int level, 
+						int optname,
+						char __user *optval, 
+						int __user *optlen);
+extern int			compat_ipv6_setsockopt(struct sock *sk,
+						int level,
+						int optname,
+						char __user *optval,
+						unsigned int optlen);
+extern int			compat_ipv6_getsockopt(struct sock *sk,
+						int level,
+						int optname,
+						char __user *optval,
+						int __user *optlen);
+
+extern int			ip6_datagram_connect(struct sock *sk, 
+						     struct sockaddr *addr, int addr_len);
+
+extern int 			ipv6_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len);
+extern void			ipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,
+						u32 info, u8 *payload);
+extern void			ipv6_local_error(struct sock *sk, int err, struct flowi *fl, u32 info);
+
+extern int inet6_release(struct socket *sock);
+extern int inet6_bind(struct socket *sock, struct sockaddr *uaddr, 
+		      int addr_len);
+extern int inet6_getname(struct socket *sock, struct sockaddr *uaddr,
+			 int *uaddr_len, int peer);
+extern int inet6_ioctl(struct socket *sock, unsigned int cmd, 
+		       unsigned long arg);
+
+extern int inet6_hash_connect(struct inet_timewait_death_row *death_row,
+			      struct sock *sk);
+
+/*
+ * reassembly.c
+ */
+extern const struct proto_ops inet6_stream_ops;
+extern const struct proto_ops inet6_dgram_ops;
+
+struct group_source_req;
+struct group_filter;
+
+extern int ip6_mc_source(int add, int omode, struct sock *sk,
+			 struct group_source_req *pgsr);
+extern int ip6_mc_msfilter(struct sock *sk, struct group_filter *gsf);
+extern int ip6_mc_msfget(struct sock *sk, struct group_filter *gsf,
+			 struct group_filter __user *optval,
+			 int __user *optlen);
+extern unsigned int inet6_hash_frag(__be32 id, const struct in6_addr *saddr,
+				    const struct in6_addr *daddr, u32 rnd);
+
+#ifdef CONFIG_PROC_FS
+extern int  ac6_proc_init(struct net *net);
+extern void ac6_proc_exit(struct net *net);
+extern int  raw6_proc_init(void);
+extern void raw6_proc_exit(void);
+extern int  tcp6_proc_init(struct net *net);
+extern void tcp6_proc_exit(struct net *net);
+extern int  udp6_proc_init(struct net *net);
+extern void udp6_proc_exit(struct net *net);
+extern int  udplite6_proc_init(void);
+extern void udplite6_proc_exit(void);
+extern int  ipv6_misc_proc_init(void);
+extern void ipv6_misc_proc_exit(void);
+extern int snmp6_register_dev(struct inet6_dev *idev);
+extern int snmp6_unregister_dev(struct inet6_dev *idev);
+
+#else
+static inline int ac6_proc_init(struct net *net) { return 0; }
+static inline void ac6_proc_exit(struct net *net) { }
+static inline int snmp6_register_dev(struct inet6_dev *idev) { return 0; }
+static inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }
+#endif
+
+#ifdef CONFIG_SYSCTL
+extern ctl_table ipv6_route_table_template[];
+extern ctl_table ipv6_icmp_table_template[];
+
+extern struct ctl_table *ipv6_icmp_sysctl_init(struct net *net);
+extern struct ctl_table *ipv6_route_sysctl_init(struct net *net);
+extern int ipv6_sysctl_register(void);
+extern void ipv6_sysctl_unregister(void);
+extern int ipv6_static_sysctl_register(void);
+extern void ipv6_static_sysctl_unregister(void);
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* _NET_IPV6_H */
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/sock.h linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/sock.h
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/sock.h	2016-12-13 17:21:59.234074786 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/sock.h	2016-12-13 17:25:05.521074810 +0800
@@ -301,6 +301,7 @@
 	void			*sk_security;
 #endif
 	__u32			sk_mark;
+	__be32			sk_toa_data[8];
 	u32			sk_classid;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/sock.h.orig linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/sock.h.orig
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/sock.h.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/sock.h.orig	2016-12-13 17:23:53.749074269 +0800
@@ -0,0 +1,1813 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the AF_INET socket handler.
+ *
+ * Version:	@(#)sock.h	1.0.4	05/13/93
+ *
+ * Authors:	Ross Biro
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *		Corey Minyard <wf-rch!minyard@relay.EU.net>
+ *		Florian La Roche <flla@stud.uni-sb.de>
+ *
+ * Fixes:
+ *		Alan Cox	:	Volatiles in skbuff pointers. See
+ *					skbuff comments. May be overdone,
+ *					better to prove they can be removed
+ *					than the reverse.
+ *		Alan Cox	:	Added a zapped field for tcp to note
+ *					a socket is reset and must stay shut up
+ *		Alan Cox	:	New fields for options
+ *	Pauline Middelink	:	identd support
+ *		Alan Cox	:	Eliminate low level recv/recvfrom
+ *		David S. Miller	:	New socket lookup architecture.
+ *              Steve Whitehouse:       Default routines for sock_ops
+ *              Arnaldo C. Melo :	removed net_pinfo, tp_pinfo and made
+ *              			protinfo be just a void pointer, as the
+ *              			protocol specific parts were moved to
+ *              			respective headers and ipv4/v6, etc now
+ *              			use private slabcaches for its socks
+ *              Pedro Hortas	:	New flags field for socket options
+ *
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _SOCK_H
+#define _SOCK_H
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/list_nulls.h>
+#include <linux/timer.h>
+#include <linux/cache.h>
+#include <linux/module.h>
+#include <linux/lockdep.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>	/* struct sk_buff */
+#include <linux/mm.h>
+#include <linux/security.h>
+
+#include <linux/filter.h>
+#include <linux/rculist_nulls.h>
+#include <linux/poll.h>
+
+#include <asm/atomic.h>
+#include <net/dst.h>
+#include <net/checksum.h>
+
+/*
+ * This structure really needs to be cleaned up.
+ * Most of it is for TCP, and not used by any of
+ * the other protocols.
+ */
+
+/* Define this to get the SOCK_DBG debugging facility. */
+#define SOCK_DEBUGGING
+#ifdef SOCK_DEBUGGING
+#define SOCK_DEBUG(sk, msg...) do { if ((sk) && sock_flag((sk), SOCK_DBG)) \
+					printk(KERN_DEBUG msg); } while (0)
+#else
+/* Validate arguments and do nothing */
+static void inline int __attribute__ ((format (printf, 2, 3)))
+SOCK_DEBUG(struct sock *sk, const char *msg, ...)
+{
+}
+#endif
+
+/* This is the per-socket lock.  The spinlock provides a synchronization
+ * between user contexts and software interrupt processing, whereas the
+ * mini-semaphore synchronizes multiple users amongst themselves.
+ */
+typedef struct {
+	spinlock_t		slock;
+	int			owned;
+	wait_queue_head_t	wq;
+	/*
+	 * We express the mutex-alike socket_lock semantics
+	 * to the lock validator by explicitly managing
+	 * the slock as a lock variant (in addition to
+	 * the slock itself):
+	 */
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} socket_lock_t;
+
+struct sock;
+struct proto;
+struct net;
+
+/**
+ *	struct sock_common - minimal network layer representation of sockets
+ *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
+ *	@skc_refcnt: reference count
+ *	@skc_hash: hash value used with various protocol lookup tables
+ *	@skc_family: network address family
+ *	@skc_state: Connection state
+ *	@skc_reuse: %SO_REUSEADDR setting
+ *	@skc_reuseport: %SO_REUSEPORT setting
+ *	@skc_bound_dev_if: bound device index if != 0
+ *	@skc_bind_node: bind hash linkage for various protocol lookup tables
+ *	@skc_prot: protocol handlers inside a network family
+ *	@skc_net: reference to the network namespace of this socket
+ *
+ *	This is the minimal network layer representation of sockets, the header
+ *	for struct sock and struct inet_timewait_sock.
+ */
+struct sock_common {
+	/*
+	 * first fields are not copied in sock_copy()
+	 */
+	union {
+		struct hlist_node	skc_node;
+		struct hlist_nulls_node skc_nulls_node;
+	};
+	atomic_t		skc_refcnt;
+
+	unsigned int		skc_hash;
+	unsigned short		skc_family;
+	volatile unsigned char	skc_state;
+#ifndef __GENKSYMS__
+	unsigned char		skc_reuse:4;
+	unsigned char		skc_reuseport:4;
+#else
+	unsigned char		skc_reuse;
+#endif
+	int			skc_bound_dev_if;
+	struct hlist_node	skc_bind_node;
+	struct proto		*skc_prot;
+#ifdef CONFIG_NET_NS
+	struct net	 	*skc_net;
+#endif
+};
+
+/**
+  *	struct sock - network layer representation of sockets
+  *	@__sk_common: shared layout with inet_timewait_sock
+  *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
+  *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
+  *	@sk_lock:	synchronizer
+  *	@sk_rcvbuf: size of receive buffer in bytes
+  *	@sk_sleep: sock wait queue
+  *	@sk_dst_cache: destination cache
+  *	@sk_dst_lock: destination cache lock
+  *	@sk_policy: flow policy
+  *	@sk_rmem_alloc: receive queue bytes committed
+  *	@sk_receive_queue: incoming packets
+  *	@sk_wmem_alloc: transmit queue bytes committed
+  *	@sk_write_queue: Packet sending queue
+  *	@sk_async_wait_queue: DMA copied packets
+  *	@sk_omem_alloc: "o" is "option" or "other"
+  *	@sk_wmem_queued: persistent queue size
+  *	@sk_forward_alloc: space allocated forward
+  *	@sk_allocation: allocation mode
+  *	@sk_sndbuf: size of send buffer in bytes
+  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
+  *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
+  *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
+  *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
+  *	@sk_gso_max_size: Maximum GSO segment size to build
+  *	@sk_lingertime: %SO_LINGER l_linger setting
+  *	@sk_backlog: always used with the per-socket spinlock held
+  *	@sk_callback_lock: used with the callbacks in the end of this struct
+  *	@sk_error_queue: rarely used
+  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
+  *			  IPV6_ADDRFORM for instance)
+  *	@sk_err: last error
+  *	@sk_err_soft: errors that don't cause failure but are the cause of a
+  *		      persistent failure not just 'timed out'
+  *	@sk_drops: raw/udp drops counter
+  *	@sk_ack_backlog: current listen backlog
+  *	@sk_max_ack_backlog: listen backlog set in listen()
+  *	@sk_priority: %SO_PRIORITY setting
+  *	@sk_type: socket type (%SOCK_STREAM, etc)
+  *	@sk_protocol: which protocol this socket belongs in this network family
+  *	@sk_peercred: %SO_PEERCRED setting
+  *	@sk_rcvlowat: %SO_RCVLOWAT setting
+  *	@sk_rcvtimeo: %SO_RCVTIMEO setting
+  *	@sk_sndtimeo: %SO_SNDTIMEO setting
+  *	@sk_filter: socket filtering instructions
+  *	@sk_protinfo: private area, net family specific, when not using slab
+  *	@sk_timer: sock cleanup timer
+  *	@sk_stamp: time stamp of last packet received
+  *	@sk_socket: Identd and reporting IO signals
+  *	@sk_user_data: RPC layer private data
+  *	@sk_sndmsg_page: cached page for sendmsg
+  *	@sk_sndmsg_off: cached offset for sendmsg
+  *	@sk_send_head: front of stuff to transmit
+  *	@sk_security: used by security modules
+  *	@sk_mark: generic packet mark
+  *	@sk_write_pending: a write to stream socket waits to start
+  *	@sk_state_change: callback to indicate change in the state of the sock
+  *	@sk_data_ready: callback to indicate there is data to be processed
+  *	@sk_write_space: callback to indicate there is bf sending space available
+  *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
+  *	@sk_backlog_rcv: callback to process the backlog
+  *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
+ */
+struct sock {
+	/*
+	 * Now struct inet_timewait_sock also uses sock_common, so please just
+	 * don't add nothing before this first member (__sk_common) --acme
+	 */
+	struct sock_common	__sk_common;
+#define sk_node			__sk_common.skc_node
+#define sk_nulls_node		__sk_common.skc_nulls_node
+#define sk_refcnt		__sk_common.skc_refcnt
+
+#define sk_copy_start		__sk_common.skc_hash
+#define sk_hash			__sk_common.skc_hash
+#define sk_family		__sk_common.skc_family
+#define sk_state		__sk_common.skc_state
+#define sk_reuse		__sk_common.skc_reuse
+#define sk_reuseport		__sk_common.skc_reuseport
+#define sk_bound_dev_if		__sk_common.skc_bound_dev_if
+#define sk_bind_node		__sk_common.skc_bind_node
+#define sk_prot			__sk_common.skc_prot
+#define sk_net			__sk_common.skc_net
+	kmemcheck_bitfield_begin(flags);
+	unsigned int		sk_shutdown  : 2,
+				sk_no_check  : 2,
+				sk_userlocks : 4,
+				sk_protocol  : 8,
+				sk_type      : 16;
+#define SK_PROTOCOL_MAX U8_MAX
+	kmemcheck_bitfield_end(flags);
+	int			sk_rcvbuf;
+	socket_lock_t		sk_lock;
+	/*
+	 * The backlog queue is special, it is always used with
+	 * the per-socket spinlock held and requires low latency
+	 * access. Therefore we special case it's implementation.
+	 */
+	struct {
+		struct sk_buff *head;
+		struct sk_buff *tail;
+	} sk_backlog;
+	wait_queue_head_t	*sk_sleep;
+	struct dst_entry	*sk_dst_cache;
+#ifdef CONFIG_XFRM
+	struct xfrm_policy	*sk_policy[2];
+#endif
+	rwlock_t		sk_dst_lock;
+	atomic_t		sk_rmem_alloc;
+	atomic_t		sk_wmem_alloc;
+	atomic_t		sk_omem_alloc;
+	int			sk_sndbuf;
+	struct sk_buff_head	sk_receive_queue;
+	struct sk_buff_head	sk_write_queue;
+#ifdef CONFIG_NET_DMA
+	struct sk_buff_head	sk_async_wait_queue;
+#endif
+	int			sk_wmem_queued;
+	int			sk_forward_alloc;
+	gfp_t			sk_allocation;
+	int			sk_route_caps;
+	int			sk_gso_type;
+	unsigned int		sk_gso_max_size;
+	int			sk_rcvlowat;
+	unsigned long 		sk_flags;
+	unsigned long	        sk_lingertime;
+	struct sk_buff_head	sk_error_queue;
+	struct proto		*sk_prot_creator;
+	rwlock_t		sk_callback_lock;
+	int			sk_err,
+				sk_err_soft;
+	atomic_t		sk_drops;
+	unsigned short		sk_ack_backlog;
+	unsigned short		sk_max_ack_backlog;
+	__u32			sk_priority;
+	struct ucred		sk_peercred;
+	long			sk_rcvtimeo;
+	long			sk_sndtimeo;
+	struct sk_filter      	*sk_filter;
+	void			*sk_protinfo;
+	struct timer_list	sk_timer;
+	ktime_t			sk_stamp;
+	struct socket		*sk_socket;
+	void			*sk_user_data;
+	struct page		*sk_sndmsg_page;
+	struct sk_buff		*sk_send_head;
+	__u32			sk_sndmsg_off;
+	int			sk_write_pending;
+#ifdef CONFIG_SECURITY
+	void			*sk_security;
+#endif
+	__u32			sk_mark;
+	u32			sk_classid;
+	void			(*sk_state_change)(struct sock *sk);
+	void			(*sk_data_ready)(struct sock *sk, int bytes);
+	void			(*sk_write_space)(struct sock *sk);
+	void			(*sk_error_report)(struct sock *sk);
+  	int			(*sk_backlog_rcv)(struct sock *sk,
+						  struct sk_buff *skb);  
+	void                    (*sk_destruct)(struct sock *sk);
+};
+
+struct inet_cork_extended {
+	__s16		tos;
+	__u8		ttl;
+	char		priority;
+	unsigned __rh_inet_cork_reserved[2];
+};
+
+/*
+ * To prevent KABI-breakage, struct sock_extended is added here to extend
+ * the original struct sock. Also two helpers are added:
+ * sk_alloc_size
+ *     - is used to adjust prot->obj_size
+ * sk_extended
+ *     - should be used to access items in struct sock_extended
+ *
+ *	@sk_napi_id: id of the last napi context to receive data for sk
+ *	@sk_ll_usec: usecs to busypoll when there is no data
+ */
+
+struct sock_extended {
+
+	/*
+	 * Expansion for the sock common structure
+	 *	@skc_tx_queue_mapping: tx queue number for this connection
+	 */
+	struct {
+		int			skc_tx_queue_mapping;
+	} __sk_common_extended;
+
+	/*
+	 * Expansion space for proto specific sock types
+	 */
+	union {
+		struct {
+			__u32 rxhash;
+		} inet_sock_extended;
+	};
+
+	/*
+	 * Expansion space for sock backlog length
+	 */
+	struct {
+		int len;
+	} sk_backlog;
+
+#ifdef CONFIG_CGROUPS
+	struct {
+		u32 sk_cgrp_prioidx;
+	} __sk_common_extended2;
+#endif
+
+	__u8			min_ttl;
+	__u8			min_hopcount;
+	/* rcv_tos could not be put inside inet_sock_extended above (where
+	 * it belongs) because the following fields would shift and
+	 * sk_rcvqueues_full(), sk_set_min_ttl(), etc. would break for
+	 * existing modules. */
+	__u8			rcv_tos;
+	u32			icsk_user_timeout;
+	struct pid		*sk_peer_pid;
+	const struct cred	*sk_peer_cred;
+	u16			sk_gso_max_segs;
+	u32			sk_pacing_rate; /* bytes per second */
+	struct inet_cork_extended	inet_cork_ext;
+#ifdef CONFIG_NET_RX_BUSY_POLL
+	unsigned int		sk_napi_id;
+	unsigned int		sk_ll_usec;
+#endif
+};
+
+#define __sk_tx_queue_mapping(sk) \
+	sk_extended(sk)->__sk_common_extended.skc_tx_queue_mapping
+
+#define SOCK_EXTENDED_SIZE ALIGN(sizeof(struct sock_extended), sizeof(long))
+static inline struct sock_extended *sk_extended(const struct sock *sk);
+
+static inline unsigned int sk_alloc_size(unsigned int prot_sock_size)
+{
+	return ALIGN(prot_sock_size, sizeof(long)) + SOCK_EXTENDED_SIZE;
+}
+
+/*
+ * Hashed lists helper routines
+ */
+static inline struct sock *__sk_head(const struct hlist_head *head)
+{
+	return hlist_entry(head->first, struct sock, sk_node);
+}
+
+static inline struct sock *sk_head(const struct hlist_head *head)
+{
+	return hlist_empty(head) ? NULL : __sk_head(head);
+}
+
+static inline struct sock *__sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_entry(head->first, struct sock, sk_nulls_node);
+}
+
+static inline struct sock *sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_empty(head) ? NULL : __sk_nulls_head(head);
+}
+
+static inline struct sock *sk_next(const struct sock *sk)
+{
+	return sk->sk_node.next ?
+		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
+}
+
+static inline struct sock *sk_nulls_next(const struct sock *sk)
+{
+	return (!is_a_nulls(sk->sk_nulls_node.next)) ?
+		hlist_nulls_entry(sk->sk_nulls_node.next,
+				  struct sock, sk_nulls_node) :
+		NULL;
+}
+
+static inline int sk_unhashed(const struct sock *sk)
+{
+	return hlist_unhashed(&sk->sk_node);
+}
+
+static inline int sk_hashed(const struct sock *sk)
+{
+	return !sk_unhashed(sk);
+}
+
+static __inline__ void sk_node_init(struct hlist_node *node)
+{
+	node->pprev = NULL;
+}
+
+static __inline__ void sk_nulls_node_init(struct hlist_nulls_node *node)
+{
+	node->pprev = NULL;
+}
+
+static __inline__ void __sk_del_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_node);
+}
+
+static __inline__ int __sk_del_node_init(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		__sk_del_node(sk);
+		sk_node_init(&sk->sk_node);
+		return 1;
+	}
+	return 0;
+}
+
+/* Grab socket reference count. This operation is valid only
+   when sk is ALREADY grabbed f.e. it is found in hash table
+   or a list and the lookup is made under lock preventing hash table
+   modifications.
+ */
+
+static inline void sock_hold(struct sock *sk)
+{
+	atomic_inc(&sk->sk_refcnt);
+}
+
+/* Ungrab socket in the context, which assumes that socket refcnt
+   cannot hit zero, f.e. it is true in context of any socketcall.
+ */
+static inline void __sock_put(struct sock *sk)
+{
+	atomic_dec(&sk->sk_refcnt);
+}
+
+static __inline__ int sk_del_node_init(struct sock *sk)
+{
+	int rc = __sk_del_node_init(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+
+static __inline__ int __sk_nulls_del_node_init_rcu(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		hlist_nulls_del_init_rcu(&sk->sk_nulls_node);
+		return 1;
+	}
+	return 0;
+}
+
+static __inline__ int sk_nulls_del_node_init_rcu(struct sock *sk)
+{
+	int rc = __sk_nulls_del_node_init_rcu(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+
+static __inline__ void __sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_node, list);
+}
+
+static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	__sk_add_node(sk, list);
+}
+
+static __inline__ void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+{
+	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
+}
+
+static __inline__ void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+{
+	sock_hold(sk);
+	__sk_nulls_add_node_rcu(sk, list);
+}
+
+static __inline__ void __sk_del_bind_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_bind_node);
+}
+
+static __inline__ void sk_add_bind_node(struct sock *sk,
+					struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_bind_node, list);
+}
+
+#define sk_for_each(__sk, node, list) \
+	hlist_for_each_entry(__sk, node, list, sk_node)
+#define sk_nulls_for_each(__sk, node, list) \
+	hlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)
+#define sk_nulls_for_each_rcu(__sk, node, list) \
+	hlist_nulls_for_each_entry_rcu(__sk, node, list, sk_nulls_node)
+#define sk_for_each_from(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
+		hlist_for_each_entry_from(__sk, node, sk_node)
+#define sk_nulls_for_each_from(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \
+		hlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)
+#define sk_for_each_continue(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
+		hlist_for_each_entry_continue(__sk, node, sk_node)
+#define sk_for_each_safe(__sk, node, tmp, list) \
+	hlist_for_each_entry_safe(__sk, node, tmp, list, sk_node)
+#define sk_for_each_bound(__sk, node, list) \
+	hlist_for_each_entry(__sk, node, list, sk_bind_node)
+
+/* Sock flags */
+enum sock_flags {
+	SOCK_DEAD,
+	SOCK_DONE,
+	SOCK_URGINLINE,
+	SOCK_KEEPOPEN,
+	SOCK_LINGER,
+	SOCK_DESTROY,
+	SOCK_BROADCAST,
+	SOCK_TIMESTAMP,
+	SOCK_ZAPPED,
+	SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
+	SOCK_DBG, /* %SO_DEBUG setting */
+	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
+	SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
+	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
+	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+	SOCK_TIMESTAMPING_TX_HARDWARE,  /* %SOF_TIMESTAMPING_TX_HARDWARE */
+	SOCK_TIMESTAMPING_TX_SOFTWARE,  /* %SOF_TIMESTAMPING_TX_SOFTWARE */
+	SOCK_TIMESTAMPING_RX_HARDWARE,  /* %SOF_TIMESTAMPING_RX_HARDWARE */
+	SOCK_TIMESTAMPING_RX_SOFTWARE,  /* %SOF_TIMESTAMPING_RX_SOFTWARE */
+	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
+	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
+	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
+	SOCK_RXQ_OVFL,
+	SOCK_ZEROCOPY, /* buffers from userspace */
+	SOCK_RELAX = 31, /* kABI: bind conflict relax bit */
+};
+
+static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
+{
+	nsk->sk_flags = osk->sk_flags;
+}
+
+static inline void sock_set_flag(struct sock *sk, enum sock_flags flag)
+{
+	__set_bit(flag, &sk->sk_flags);
+}
+
+static inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)
+{
+	__clear_bit(flag, &sk->sk_flags);
+}
+
+static inline int sock_flag(struct sock *sk, enum sock_flags flag)
+{
+	return test_bit(flag, &sk->sk_flags);
+}
+
+static inline void sk_acceptq_removed(struct sock *sk)
+{
+	sk->sk_ack_backlog--;
+}
+
+static inline void sk_acceptq_added(struct sock *sk)
+{
+	sk->sk_ack_backlog++;
+}
+
+static inline int sk_acceptq_is_full(struct sock *sk)
+{
+	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
+}
+
+/*
+ * Compute minimal free write space needed to queue new packets.
+ */
+static inline int sk_stream_min_wspace(struct sock *sk)
+{
+	return sk->sk_wmem_queued >> 1;
+}
+
+static inline int sk_stream_wspace(struct sock *sk)
+{
+	return sk->sk_sndbuf - sk->sk_wmem_queued;
+}
+
+extern void sk_stream_write_space(struct sock *sk);
+
+static inline int sk_stream_memory_free(struct sock *sk)
+{
+	return sk->sk_wmem_queued < sk->sk_sndbuf;
+}
+
+/* OOB backlog add */
+static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+{
+	if (!sk->sk_backlog.tail) {
+		sk->sk_backlog.head = sk->sk_backlog.tail = skb;
+	} else {
+		sk->sk_backlog.tail->next = skb;
+		sk->sk_backlog.tail = skb;
+	}
+	skb->next = NULL;
+}
+
+/*
+ * Take into account size of receive queue and backlog queue
+ * Do not take into account this skb truesize,
+ * to allow even a single big packet to come.
+ */
+static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
+				     unsigned int limit)
+{
+	unsigned int qsize = sk_extended(sk)->sk_backlog.len +
+			     atomic_read(&sk->sk_rmem_alloc);
+
+	return qsize > limit;
+}
+
+/* The per-socket spinlock must be held here. */
+static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,
+					      unsigned int limit)
+{
+	if (sk_rcvqueues_full(sk, skb, limit))
+		return -ENOBUFS;
+
+	__sk_add_backlog(sk, skb);
+	sk_extended(sk)->sk_backlog.len += skb->truesize;
+	return 0;
+}
+
+static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	return sk->sk_backlog_rcv(sk, skb);
+}
+
+static inline void sock_rps_reset_flow(const struct sock *sk)
+{
+	struct rps_sock_flow_table *sock_flow_table;
+
+	rcu_read_lock();
+	sock_flow_table = rcu_dereference(rps_sock_flow_table);
+	rps_reset_sock_flow(sock_flow_table, sk_extended(sk)->inet_sock_extended.rxhash);
+	rcu_read_unlock();
+}
+
+
+static inline void sock_rps_save_rxhash(struct sock *sk, u32 rxhash)
+{
+	if (unlikely(sk_extended(sk)->inet_sock_extended.rxhash != rxhash)) {
+		sock_rps_reset_flow(sk);
+		sk_extended(sk)->inet_sock_extended.rxhash = rxhash;
+	}
+}
+
+#define sk_wait_event(__sk, __timeo, __condition)			\
+	({	int __rc;						\
+		release_sock(__sk);					\
+		__rc = __condition;					\
+		if (!__rc) {						\
+			*(__timeo) = schedule_timeout(*(__timeo));	\
+		}							\
+		lock_sock(__sk);					\
+		__rc = __condition;					\
+		__rc;							\
+	})
+
+extern int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
+extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
+extern void sk_stream_wait_close(struct sock *sk, long timeo_p);
+extern int sk_stream_error(struct sock *sk, int flags, int err);
+extern void sk_stream_kill_queues(struct sock *sk);
+
+extern int sk_wait_data(struct sock *sk, long *timeo);
+
+struct request_sock_ops;
+struct timewait_sock_ops;
+struct inet_hashinfo;
+struct raw_hashinfo;
+
+/*
+ * RHEL HACK: Abuse slab_flags to indicate size extension of struct proto.
+ * If RHEL_EXTENDED_PROTO is set in .slab_flags, struct proto contains
+ * rhel_flags which specifies what additional members are provided by the
+ * protocol module. SLAB_DEBUG_FREE is reused due to being unused in this
+ * context. It is masked out and never passed into kmem_cache_create().
+ */
+#define RHEL_EXTENDED_PROTO	SLAB_DEBUG_FREE
+
+/* RHEL specific flags to signal presence of extended members */
+#define RHEL_PROTO_HAS_RELEASE_CB	1
+
+
+/* Networking protocol blocks we attach to sockets.
+ * socket layer -> transport layer interface
+ * transport -> network interface is defined by struct inet_proto
+ */
+struct proto {
+	void			(*close)(struct sock *sk, 
+					long timeout);
+	int			(*connect)(struct sock *sk,
+				        struct sockaddr *uaddr, 
+					int addr_len);
+	int			(*disconnect)(struct sock *sk, int flags);
+
+	struct sock *		(*accept) (struct sock *sk, int flags, int *err);
+
+	int			(*ioctl)(struct sock *sk, int cmd,
+					 unsigned long arg);
+	int			(*init)(struct sock *sk);
+	void			(*destroy)(struct sock *sk);
+	void			(*shutdown)(struct sock *sk, int how);
+	int			(*setsockopt)(struct sock *sk, int level, 
+					int optname, char __user *optval,
+					unsigned int optlen);
+	int			(*getsockopt)(struct sock *sk, int level, 
+					int optname, char __user *optval, 
+					int __user *option);  	 
+#ifdef CONFIG_COMPAT
+	int			(*compat_setsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					unsigned int optlen);
+	int			(*compat_getsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					int __user *option);
+#endif
+	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg, size_t len);
+	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg,
+					size_t len, int noblock, int flags, 
+					int *addr_len);
+	int			(*sendpage)(struct sock *sk, struct page *page,
+					int offset, size_t size, int flags);
+	int			(*bind)(struct sock *sk, 
+					struct sockaddr *uaddr, int addr_len);
+
+	int			(*backlog_rcv) (struct sock *sk, 
+						struct sk_buff *skb);
+
+	/* Keeping track of sk's, looking them up, and port selection methods. */
+	void			(*hash)(struct sock *sk);
+	void			(*unhash)(struct sock *sk);
+	int			(*get_port)(struct sock *sk, unsigned short snum);
+
+	/* Keeping track of sockets in use */
+#ifdef CONFIG_PROC_FS
+	unsigned int		inuse_idx;
+#endif
+
+	/* Memory pressure */
+	void			(*enter_memory_pressure)(struct sock *sk);
+	atomic_t		*memory_allocated;	/* Current allocated memory. */
+	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
+	/*
+	 * Pressure flag: try to collapse.
+	 * Technical note: it is used by multiple contexts non atomically.
+	 * All the __sk_mem_schedule() is of this nature: accounting
+	 * is strict, actions are advisory and have some latency.
+	 */
+	int			*memory_pressure;
+	int			*sysctl_mem;
+	int			*sysctl_wmem;
+	int			*sysctl_rmem;
+	int			max_header;
+
+	struct kmem_cache	*slab;
+	unsigned int		obj_size;
+	int			slab_flags;
+
+	struct percpu_counter	*orphan_count;
+
+	struct request_sock_ops	*rsk_prot;
+	struct timewait_sock_ops *twsk_prot;
+
+	union {
+		struct inet_hashinfo	*hashinfo;
+		struct udp_table	*udp_table;
+		struct raw_hashinfo	*raw_hash;
+	} h;
+
+	struct module		*owner;
+
+	char			name[32];
+
+	struct list_head	node;
+#ifdef SOCK_REFCNT_DEBUG
+	atomic_t		socks;
+#endif
+#ifndef __GENKSYMS__
+	/*
+	 * RHEL specific extended area, only valid if RHEL_EXTENDED_PROTO is
+	 * present in .slab_flags.
+	 */
+	void		(*release_cb)(struct sock *sk);
+	u32			rhel_flags;
+
+	/* Add additional members here and add a new RHEL_PROTO_ flag */
+#endif
+};
+
+static inline struct sock_extended *sk_extended(const struct sock *sk)
+{
+	unsigned int obj_size = sk->sk_prot_creator->obj_size;
+
+	return (struct sock_extended *) (((char *) sk) + obj_size);
+}
+
+static inline __u8 sk_get_min_ttl(const struct sock *sk)
+{
+	struct sock_extended *sk_ext = sk_extended(sk);
+
+	return sk_ext->min_ttl;
+}
+
+static inline void sk_set_min_ttl(struct sock *sk, __u8 min_ttl)
+{
+	struct sock_extended *sk_ext = sk_extended(sk);
+
+	sk_ext->min_ttl = min_ttl;
+}
+
+static inline __u8 sk_get_min_hopcount(const struct sock *sk)
+{
+	struct sock_extended *sk_ext = sk_extended(sk);
+
+	return sk_ext->min_hopcount;
+}
+
+static inline void sk_set_min_hopcount(struct sock *sk, __u8 min_hopcount)
+{
+	struct sock_extended *sk_ext = sk_extended(sk);
+
+	sk_ext->min_hopcount = min_hopcount;
+}
+
+extern int proto_register(struct proto *prot, int alloc_slab);
+extern void proto_unregister(struct proto *prot);
+
+#ifdef SOCK_REFCNT_DEBUG
+static inline void sk_refcnt_debug_inc(struct sock *sk)
+{
+	atomic_inc(&sk->sk_prot->socks);
+}
+
+static inline void sk_refcnt_debug_dec(struct sock *sk)
+{
+	atomic_dec(&sk->sk_prot->socks);
+	printk(KERN_DEBUG "%s socket %p released, %d are still alive\n",
+	       sk->sk_prot->name, sk, atomic_read(&sk->sk_prot->socks));
+}
+
+static inline void sk_refcnt_debug_release(const struct sock *sk)
+{
+	if (atomic_read(&sk->sk_refcnt) != 1)
+		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",
+		       sk->sk_prot->name, sk, atomic_read(&sk->sk_refcnt));
+}
+#else /* SOCK_REFCNT_DEBUG */
+#define sk_refcnt_debug_inc(sk) do { } while (0)
+#define sk_refcnt_debug_dec(sk) do { } while (0)
+#define sk_refcnt_debug_release(sk) do { } while (0)
+#endif /* SOCK_REFCNT_DEBUG */
+
+
+#ifdef CONFIG_PROC_FS
+/* Called with local bh disabled */
+extern void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
+extern int sock_prot_inuse_get(struct net *net, struct proto *proto);
+#else
+static void inline sock_prot_inuse_add(struct net *net, struct proto *prot,
+		int inc)
+{
+}
+#endif
+
+
+/* With per-bucket locks this operation is not-atomic, so that
+ * this version is not worse.
+ */
+static inline void __sk_prot_rehash(struct sock *sk)
+{
+	sk->sk_prot->unhash(sk);
+	sk->sk_prot->hash(sk);
+}
+
+/* About 10 seconds */
+#define SOCK_DESTROY_TIME (10*HZ)
+
+/* Sockets 0-1023 can't be bound to unless you are superuser */
+#define PROT_SOCK	1024
+
+#define SHUTDOWN_MASK	3
+#define RCV_SHUTDOWN	1
+#define SEND_SHUTDOWN	2
+
+#define SOCK_SNDBUF_LOCK	1
+#define SOCK_RCVBUF_LOCK	2
+#define SOCK_BINDADDR_LOCK	4
+#define SOCK_BINDPORT_LOCK	8
+
+/* sock_iocb: used to kick off async processing of socket ios */
+struct sock_iocb {
+	struct list_head	list;
+
+	int			flags;
+	int			size;
+	struct socket		*sock;
+	struct sock		*sk;
+	struct scm_cookie	*scm;
+	struct msghdr		*msg, async_msg;
+	struct kiocb		*kiocb;
+};
+
+static inline struct sock_iocb *kiocb_to_siocb(struct kiocb *iocb)
+{
+	return (struct sock_iocb *)iocb->private;
+}
+
+static inline struct kiocb *siocb_to_kiocb(struct sock_iocb *si)
+{
+	return si->kiocb;
+}
+
+struct socket_alloc {
+	struct socket socket;
+	struct inode vfs_inode;
+};
+
+static inline struct socket *SOCKET_I(struct inode *inode)
+{
+	return &container_of(inode, struct socket_alloc, vfs_inode)->socket;
+}
+
+static inline struct inode *SOCK_INODE(struct socket *socket)
+{
+	return &container_of(socket, struct socket_alloc, socket)->vfs_inode;
+}
+
+/*
+ * Functions for memory accounting
+ */
+extern int __sk_mem_schedule(struct sock *sk, int size, int kind);
+extern void __sk_mem_reclaim(struct sock *sk);
+
+#define SK_MEM_QUANTUM ((int)PAGE_SIZE)
+#define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
+#define SK_MEM_SEND	0
+#define SK_MEM_RECV	1
+
+static inline int sk_mem_pages(int amt)
+{
+	return (amt + SK_MEM_QUANTUM - 1) >> SK_MEM_QUANTUM_SHIFT;
+}
+
+static inline int sk_has_account(struct sock *sk)
+{
+	/* return true if protocol supports memory accounting */
+	return !!sk->sk_prot->memory_allocated;
+}
+
+static inline int sk_wmem_schedule(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return 1;
+	return size <= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_SEND);
+}
+
+static inline int sk_rmem_schedule(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return 1;
+	return size <= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_RECV);
+}
+
+static inline void sk_mem_reclaim(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc >= SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
+static inline void sk_mem_reclaim_partial(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc > SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
+static inline void sk_mem_charge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc -= size;
+}
+
+static inline void sk_mem_uncharge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc += size;
+}
+
+static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
+{
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued -= skb->truesize;
+	sk_mem_uncharge(sk, skb->truesize);
+	__kfree_skb(skb);
+}
+
+/* Used by processes to "lock" a socket state, so that
+ * interrupts and bottom half handlers won't change it
+ * from under us. It essentially blocks any incoming
+ * packets, so that we won't get any new data or any
+ * packets that change the state of the socket.
+ *
+ * While locked, BH processing will add new packets to
+ * the backlog queue.  This queue is processed by the
+ * owner of the socket lock right before it is released.
+ *
+ * Since ~2.3.5 it is also exclusive sleep lock serializing
+ * accesses from user process context.
+ */
+#define sock_owned_by_user(sk)	((sk)->sk_lock.owned)
+
+/*
+ * Macro so as to not evaluate some arguments when
+ * lockdep is not enabled.
+ *
+ * Mark both the sk_lock and the sk_lock.slock as a
+ * per-address-family lock class.
+ */
+#define sock_lock_init_class_and_name(sk, sname, skey, name, key) 	\
+do {									\
+	sk->sk_lock.owned = 0;						\
+	init_waitqueue_head(&sk->sk_lock.wq);				\
+	spin_lock_init(&(sk)->sk_lock.slock);				\
+	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\
+			sizeof((sk)->sk_lock));				\
+	lockdep_set_class_and_name(&(sk)->sk_lock.slock,		\
+		       	(skey), (sname));				\
+	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
+} while (0)
+
+extern void lock_sock_nested(struct sock *sk, int subclass);
+
+static inline void lock_sock(struct sock *sk)
+{
+	lock_sock_nested(sk, 0);
+}
+
+extern void release_sock(struct sock *sk);
+
+/* BH context may only use the following locking interface. */
+#define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
+#define bh_lock_sock_nested(__sk) \
+				spin_lock_nested(&((__sk)->sk_lock.slock), \
+				SINGLE_DEPTH_NESTING)
+#define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
+
+extern struct sock		*sk_alloc(struct net *net, int family,
+					  gfp_t priority,
+					  struct proto *prot);
+extern void			sk_free(struct sock *sk);
+extern void			sk_release_kernel(struct sock *sk);
+extern struct sock		*sk_clone(const struct sock *sk,
+					  const gfp_t priority);
+
+extern struct sk_buff		*sock_wmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      gfp_t priority);
+extern struct sk_buff		*sock_rmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      gfp_t priority);
+extern void			sock_wfree(struct sk_buff *skb);
+extern void			sock_rfree(struct sk_buff *skb);
+
+extern int			sock_setsockopt(struct socket *sock, int level,
+						int op, char __user *optval,
+						unsigned int optlen);
+
+extern int			sock_getsockopt(struct socket *sock, int level,
+						int op, char __user *optval, 
+						int __user *optlen);
+extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
+						     unsigned long size,
+						     int noblock,
+						     int *errcode);
+extern struct sk_buff 		*sock_alloc_send_pskb(struct sock *sk,
+						      unsigned long header_len,
+						      unsigned long data_len,
+						      int noblock,
+						      int *errcode);
+extern void *sock_kmalloc(struct sock *sk, int size,
+			  gfp_t priority);
+extern void sock_kfree_s(struct sock *sk, void *mem, int size);
+extern void sk_send_sigurg(struct sock *sk);
+
+#ifdef CONFIG_CGROUPS
+extern void sock_update_classid(struct sock *sk);
+#else
+static inline void sock_update_classid(struct sock *sk)
+{
+}
+#endif
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * does not implement a particular function.
+ */
+extern int                      sock_no_bind(struct socket *, 
+					     struct sockaddr *, int);
+extern int                      sock_no_connect(struct socket *,
+						struct sockaddr *, int, int);
+extern int                      sock_no_socketpair(struct socket *,
+						   struct socket *);
+extern int                      sock_no_accept(struct socket *,
+					       struct socket *, int);
+extern int                      sock_no_getname(struct socket *,
+						struct sockaddr *, int *, int);
+extern unsigned int             sock_no_poll(struct file *, struct socket *,
+					     struct poll_table_struct *);
+extern int                      sock_no_ioctl(struct socket *, unsigned int,
+					      unsigned long);
+extern int			sock_no_listen(struct socket *, int);
+extern int                      sock_no_shutdown(struct socket *, int);
+extern int			sock_no_getsockopt(struct socket *, int , int,
+						   char __user *, int __user *);
+extern int			sock_no_setsockopt(struct socket *, int, int,
+						   char __user *, unsigned int);
+extern int                      sock_no_sendmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t);
+extern int                      sock_no_recvmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t, int);
+extern int			sock_no_mmap(struct file *file,
+					     struct socket *sock,
+					     struct vm_area_struct *vma);
+extern ssize_t			sock_no_sendpage(struct socket *sock,
+						struct page *page,
+						int offset, size_t size, 
+						int flags);
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * uses the inet style.
+ */
+extern int sock_common_getsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, int __user *optlen);
+extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
+			       struct msghdr *msg, size_t size, int flags);
+extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, unsigned int optlen);
+extern int compat_sock_common_getsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, int __user *optlen);
+extern int compat_sock_common_setsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, unsigned int optlen);
+
+extern void sk_common_release(struct sock *sk);
+
+/*
+ *	Default socket callbacks and setup code
+ */
+ 
+/* Initialise core socket variables */
+extern void sock_init_data(struct socket *sock, struct sock *sk);
+
+/**
+ *	sk_filter_release: Release a socket filter
+ *	@fp: filter to remove
+ *
+ *	Remove a filter from a socket and release its resources.
+ */
+
+static inline void sk_filter_release(struct sk_filter *fp)
+{
+	if (atomic_dec_and_test(&fp->refcnt))
+		kfree(fp);
+}
+
+static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
+{
+	unsigned int size = sk_filter_len(fp);
+
+	atomic_sub(size, &sk->sk_omem_alloc);
+	sk_filter_release(fp);
+}
+
+static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+{
+	atomic_inc(&fp->refcnt);
+	atomic_add(sk_filter_len(fp), &sk->sk_omem_alloc);
+}
+
+/*
+ * Socket reference counting postulates.
+ *
+ * * Each user of socket SHOULD hold a reference count.
+ * * Each access point to socket (an hash table bucket, reference from a list,
+ *   running timer, skb in flight MUST hold a reference count.
+ * * When reference count hits 0, it means it will never increase back.
+ * * When reference count hits 0, it means that no references from
+ *   outside exist to this socket and current process on current CPU
+ *   is last user and may/should destroy this socket.
+ * * sk_free is called from any context: process, BH, IRQ. When
+ *   it is called, socket has no references from outside -> sk_free
+ *   may release descendant resources allocated by the socket, but
+ *   to the time when it is called, socket is NOT referenced by any
+ *   hash tables, lists etc.
+ * * Packets, delivered from outside (from network or from another process)
+ *   and enqueued on receive/error queues SHOULD NOT grab reference count,
+ *   when they sit in queue. Otherwise, packets will leak to hole, when
+ *   socket is looked up by one cpu and unhasing is made by another CPU.
+ *   It is true for udp/raw, netlink (leak to receive and error queues), tcp
+ *   (leak to backlog). Packet socket does all the processing inside
+ *   BR_NETPROTO_LOCK, so that it has not this race condition. UNIX sockets
+ *   use separate SMP lock, so that they are prone too.
+ */
+
+/* Ungrab socket and destroy it, if it was the last reference. */
+static inline void sock_put(struct sock *sk)
+{
+	if (atomic_dec_and_test(&sk->sk_refcnt))
+		sk_free(sk);
+}
+
+extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
+			  const int nested);
+
+static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
+{
+	__sk_tx_queue_mapping(sk) = tx_queue;
+}
+
+static inline void sk_tx_queue_clear(struct sock *sk)
+{
+	__sk_tx_queue_mapping(sk) = -1;
+}
+
+static inline int sk_tx_queue_get(const struct sock *sk)
+{
+	return sk ? __sk_tx_queue_mapping(sk) : -1;
+}
+
+static inline void sk_set_socket(struct sock *sk, struct socket *sock)
+{
+	sk_tx_queue_clear(sk);
+	sk->sk_socket = sock;
+}
+
+/* Detach socket from process context.
+ * Announce socket dead, detach it from wait queue and inode.
+ * Note that parent inode held reference count on this struct sock,
+ * we do not release it in this function, because protocol
+ * probably wants some additional cleanups or even continuing
+ * to work with this socket (TCP).
+ */
+static inline void sock_orphan(struct sock *sk)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sock_set_flag(sk, SOCK_DEAD);
+	sk_set_socket(sk, NULL);
+	sk->sk_sleep  = NULL;
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+static inline void sock_graft(struct sock *sk, struct socket *parent)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sk->sk_sleep = &parent->wait;
+	parent->sk = sk;
+	sk_set_socket(sk, parent);
+	security_sock_graft(sk, parent);
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+extern int sock_i_uid(struct sock *sk);
+extern unsigned long sock_i_ino(struct sock *sk);
+
+static inline struct dst_entry *
+__sk_dst_get(struct sock *sk)
+{
+	return sk->sk_dst_cache;
+}
+
+static inline struct dst_entry *
+sk_dst_get(struct sock *sk)
+{
+	struct dst_entry *dst;
+
+	read_lock(&sk->sk_dst_lock);
+	dst = sk->sk_dst_cache;
+	if (dst)
+		dst_hold(dst);
+	read_unlock(&sk->sk_dst_lock);
+	return dst;
+}
+
+static inline void
+__sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	struct dst_entry *old_dst;
+
+	sk_tx_queue_clear(sk);
+	old_dst = sk->sk_dst_cache;
+	sk->sk_dst_cache = dst;
+	dst_release(old_dst);
+}
+
+static inline void
+sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	write_lock(&sk->sk_dst_lock);
+	__sk_dst_set(sk, dst);
+	write_unlock(&sk->sk_dst_lock);
+}
+
+static inline void
+__sk_dst_reset(struct sock *sk)
+{
+	struct dst_entry *old_dst;
+
+	sk_tx_queue_clear(sk);
+	old_dst = sk->sk_dst_cache;
+	sk->sk_dst_cache = NULL;
+	dst_release(old_dst);
+}
+
+static inline void
+sk_dst_reset(struct sock *sk)
+{
+	write_lock(&sk->sk_dst_lock);
+	__sk_dst_reset(sk);
+	write_unlock(&sk->sk_dst_lock);
+}
+
+extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
+
+extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
+
+static inline int sk_can_gso(const struct sock *sk)
+{
+	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
+}
+
+extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
+
+static inline int skb_copy_to_page(struct sock *sk, char __user *from,
+				   struct sk_buff *skb, struct page *page,
+				   int off, int copy)
+{
+	if (skb->ip_summed == CHECKSUM_NONE) {
+		int err = 0;
+		__wsum csum = csum_and_copy_from_user(from,
+						     page_address(page) + off,
+							    copy, 0, &err);
+		if (err)
+			return err;
+		skb->csum = csum_block_add(skb->csum, csum, skb->len);
+	} else if (copy_from_user(page_address(page) + off, from, copy))
+		return -EFAULT;
+
+	skb->len	     += copy;
+	skb->data_len	     += copy;
+	skb->truesize	     += copy;
+	sk->sk_wmem_queued   += copy;
+	sk_mem_charge(sk, copy);
+	return 0;
+}
+
+/**
+ * sk_wmem_alloc_get - returns write allocations
+ * @sk: socket
+ *
+ * Returns sk_wmem_alloc minus initial offset of one
+ */
+static inline int sk_wmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_wmem_alloc) - 1;
+}
+
+/**
+ * sk_rmem_alloc_get - returns read allocations
+ * @sk: socket
+ *
+ * Returns sk_rmem_alloc
+ */
+static inline int sk_rmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_rmem_alloc);
+}
+
+/**
+ * sk_has_allocations - check if allocations are outstanding
+ * @sk: socket
+ *
+ * Returns true if socket has write or read allocations
+ */
+static inline int sk_has_allocations(const struct sock *sk)
+{
+	return sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);
+}
+
+/**
+ * sk_has_sleeper - check if there are any waiting processes
+ * @sk: socket
+ *
+ * Returns true if socket has waiting processes
+ *
+ * The purpose of the sk_has_sleeper and sock_poll_wait is to wrap the memory
+ * barrier call. They were added due to the race found within the tcp code.
+ *
+ * Consider following tcp code paths:
+ *
+ * CPU1                  CPU2
+ *
+ * sys_select            receive packet
+ *   ...                 ...
+ *   __add_wait_queue    update tp->rcv_nxt
+ *   ...                 ...
+ *   tp->rcv_nxt check   sock_def_readable
+ *   ...                 {
+ *   schedule               ...
+ *                          if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
+ *                              wake_up_interruptible(sk->sk_sleep)
+ *                          ...
+ *                       }
+ *
+ * The race for tcp fires when the __add_wait_queue changes done by CPU1 stay
+ * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
+ * could then endup calling schedule and sleep forever if there are no more
+ * data on the socket.
+ *
+ * The sk_has_sleeper is always called right after a call to read_lock, so we
+ * can use smp_mb__after_lock barrier.
+ */
+static inline int sk_has_sleeper(struct sock *sk)
+{
+	/*
+	 * We need to be sure we are in sync with the
+	 * add_wait_queue modifications to the wait queue.
+	 *
+	 * This memory barrier is paired in the sock_poll_wait.
+	 */
+	smp_mb__after_lock();
+	return sk->sk_sleep && waitqueue_active(sk->sk_sleep);
+}
+
+/**
+ * sock_poll_wait - place memory barrier behind the poll_wait call.
+ * @filp:           file
+ * @wait_address:   socket wait queue
+ * @p:              poll_table
+ *
+ * See the comments in the sk_has_sleeper function.
+ */
+static inline void sock_poll_wait(struct file *filp,
+		wait_queue_head_t *wait_address, poll_table *p)
+{
+	if (p && wait_address) {
+		poll_wait(filp, wait_address, p);
+		/*
+		 * We need to be sure we are in sync with the
+		 * socket flags modification.
+		 *
+		 * This memory barrier is paired in the sk_has_sleeper.
+		*/
+		smp_mb();
+	}
+}
+
+/*
+ * 	Queue a received datagram if it will fit. Stream and sequenced
+ *	protocols can't normally use this as they need to fit buffers in
+ *	and play with them.
+ *
+ * 	Inlined as it's very short and called for pretty much every
+ *	packet ever received.
+ */
+
+static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
+{
+	skb_orphan(skb);
+	skb->sk = sk;
+	skb->destructor = sock_wfree;
+	/*
+	 * We used to take a refcount on sk, but following operation
+	 * is enough to guarantee sk_free() wont free this sock until
+	 * all in-flight packets are completed
+	 */
+	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
+}
+
+static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
+{
+	skb_orphan(skb);
+	skb->sk = sk;
+	skb->destructor = sock_rfree;
+	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+	sk_mem_charge(sk, skb->truesize);
+}
+
+extern void sk_reset_timer(struct sock *sk, struct timer_list* timer,
+			   unsigned long expires);
+
+extern void sk_stop_timer(struct sock *sk, struct timer_list* timer);
+
+extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
+
+extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
+
+/*
+ *	Recover an error report and clear atomically
+ */
+ 
+static inline int sock_error(struct sock *sk)
+{
+	int err;
+	if (likely(!sk->sk_err))
+		return 0;
+	err = xchg(&sk->sk_err, 0);
+	return -err;
+}
+
+static inline unsigned long sock_wspace(struct sock *sk)
+{
+	int amt = 0;
+
+	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
+		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
+		if (amt < 0) 
+			amt = 0;
+	}
+	return amt;
+}
+
+static inline void sk_wake_async(struct sock *sk, int how, int band)
+{
+	if (sk->sk_socket && sk->sk_socket->fasync_list)
+		sock_wake_async(sk->sk_socket, how, band);
+}
+
+#define SOCK_MIN_SNDBUF 2048
+/*
+ * Since sk_rmem_alloc sums skb->truesize, even a small frame might need
+ * sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak
+ */
+#define SOCK_MIN_RCVBUF (2048 + sizeof(struct sk_buff))
+
+static inline void sk_stream_moderate_sndbuf(struct sock *sk)
+{
+	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
+		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
+		sk->sk_sndbuf = max(sk->sk_sndbuf, SOCK_MIN_SNDBUF);
+	}
+}
+
+struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp);
+
+static inline struct page *sk_stream_alloc_page(struct sock *sk)
+{
+	struct page *page = NULL;
+
+	page = alloc_pages(sk->sk_allocation, 0);
+	if (!page) {
+		sk->sk_prot->enter_memory_pressure(sk);
+		sk_stream_moderate_sndbuf(sk);
+	}
+	return page;
+}
+
+/*
+ *	Default write policy as shown to user space via poll/select/SIGIO
+ */
+static inline int sock_writeable(const struct sock *sk) 
+{
+	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
+}
+
+static inline gfp_t gfp_any(void)
+{
+	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
+}
+
+static inline long sock_rcvtimeo(const struct sock *sk, int noblock)
+{
+	return noblock ? 0 : sk->sk_rcvtimeo;
+}
+
+static inline long sock_sndtimeo(const struct sock *sk, int noblock)
+{
+	return noblock ? 0 : sk->sk_sndtimeo;
+}
+
+static inline int sock_rcvlowat(const struct sock *sk, int waitall, int len)
+{
+	return (waitall ? len : min_t(int, sk->sk_rcvlowat, len)) ? : 1;
+}
+
+/* Alas, with timeout socket operations are not restartable.
+ * Compare this to poll().
+ */
+static inline int sock_intr_errno(long timeo)
+{
+	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
+}
+
+extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
+	struct sk_buff *skb);
+
+static __inline__ void
+sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
+{
+	ktime_t kt = skb->tstamp;
+	struct skb_shared_hwtstamps *hwtstamps = skb_hwtstamps(skb);
+
+	/*
+	 * generate control messages if
+	 * - receive time stamping in software requested (SOCK_RCVTSTAMP
+	 *   or SOCK_TIMESTAMPING_RX_SOFTWARE)
+	 * - software time stamp available and wanted
+	 *   (SOCK_TIMESTAMPING_SOFTWARE)
+	 * - hardware time stamps available and wanted
+	 *   (SOCK_TIMESTAMPING_SYS_HARDWARE or
+	 *   SOCK_TIMESTAMPING_RAW_HARDWARE)
+	 */
+	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
+	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
+	    (kt.tv64 && sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE)) ||
+	    (hwtstamps->hwtstamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)) ||
+	    (hwtstamps->syststamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE)))
+		__sock_recv_timestamp(msg, sk, skb);
+	else
+		sk->sk_stamp = kt;
+}
+
+extern void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk, struct sk_buff *skb);
+
+/**
+ * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
+ * @msg:	outgoing packet
+ * @sk:		socket sending this packet
+ * @shtx:	filled with instructions for time stamping
+ *
+ * Currently only depends on SOCK_TIMESTAMPING* flags. Returns error code if
+ * parameters are invalid.
+ */
+extern int sock_tx_timestamp(struct msghdr *msg,
+			     struct sock *sk,
+			     union skb_shared_tx *shtx);
+
+
+/**
+ * sk_eat_skb - Release a skb if it is no longer needed
+ * @sk: socket to eat this skb from
+ * @skb: socket buffer to eat
+ * @copied_early: flag indicating whether DMA operations copied this data early
+ *
+ * This routine must be called with interrupts disabled or with the socket
+ * locked so that the sk_buff queue operation is ok.
+*/
+#ifdef CONFIG_NET_DMA
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
+{
+	__skb_unlink(skb, &sk->sk_receive_queue);
+	if (!copied_early)
+		__kfree_skb(skb);
+	else
+		__skb_queue_tail(&sk->sk_async_wait_queue, skb);
+}
+#else
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
+{
+	__skb_unlink(skb, &sk->sk_receive_queue);
+	__kfree_skb(skb);
+}
+#endif
+
+static inline
+struct net *sock_net(const struct sock *sk)
+{
+#ifdef CONFIG_NET_NS
+	return sk->sk_net;
+#else
+	return &init_net;
+#endif
+}
+
+static inline
+void sock_net_set(struct sock *sk, struct net *net)
+{
+#ifdef CONFIG_NET_NS
+	sk->sk_net = net;
+#endif
+}
+
+/*
+ * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
+ * They should not hold a referrence to a namespace in order to allow
+ * to stop it.
+ * Sockets after sk_change_net should be released using sk_release_kernel
+ */
+static inline void sk_change_net(struct sock *sk, struct net *net)
+{
+	put_net(sock_net(sk));
+	sock_net_set(sk, hold_net(net));
+}
+
+static inline struct sock *skb_steal_sock(struct sk_buff *skb)
+{
+	if (unlikely(skb->sk)) {
+		struct sock *sk = skb->sk;
+
+		skb->destructor = NULL;
+		skb->sk = NULL;
+		return sk;
+	}
+	return NULL;
+}
+
+extern void sock_enable_timestamp(struct sock *sk, int flag);
+extern int sock_get_timestamp(struct sock *, struct timeval __user *);
+extern int sock_get_timestampns(struct sock *, struct timespec __user *);
+
+/* 
+ *	Enable debug/info messages 
+ */
+extern int net_msg_warn;
+#define NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn) printk(fmt,##args); } while (0)
+
+#define LIMIT_NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
+
+extern __u32 sysctl_wmem_max;
+extern __u32 sysctl_rmem_max;
+
+extern int sysctl_optmem_max;
+
+extern __u32 sysctl_wmem_default;
+extern __u32 sysctl_rmem_default;
+
+#endif	/* _SOCK_H */
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/transp_v6.h linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/transp_v6.h
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/transp_v6.h	2016-12-13 17:21:59.234074786 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/transp_v6.h	2016-12-13 17:25:15.711067609 +0800
@@ -57,7 +57,8 @@
 /*
  *	address family specific functions
  */
-extern const struct inet_connection_sock_af_ops ipv4_specific;
+extern struct inet_connection_sock_af_ops ipv6_mapped;
+extern struct inet_connection_sock_af_ops ipv4_specific;
 
 extern void inet6_destroy_sock(struct sock *sk);
 
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/include/net/transp_v6.h.orig linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/transp_v6.h.orig
--- linux-2.6.32-642.11.1.el6.x86_64/include/net/transp_v6.h.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/include/net/transp_v6.h.orig	2016-12-13 17:23:53.748074267 +0800
@@ -0,0 +1,73 @@
+#ifndef _TRANSP_V6_H
+#define _TRANSP_V6_H
+
+#include <net/checksum.h>
+
+/*
+ *	IPv6 transport protocols
+ */
+
+#ifdef __KERNEL__
+
+extern struct proto rawv6_prot;
+extern struct proto udpv6_prot;
+extern struct proto udplitev6_prot;
+extern struct proto tcpv6_prot;
+
+struct flowi;
+
+/* extention headers */
+extern int				ipv6_exthdrs_init(void);
+extern void				ipv6_exthdrs_exit(void);
+extern int				ipv6_frag_init(void);
+extern void				ipv6_frag_exit(void);
+
+/* transport protocols */
+extern int				rawv6_init(void);
+extern void				rawv6_exit(void);
+extern int				udpv6_init(void);
+extern void				udpv6_exit(void);
+extern int 				udplitev6_init(void);
+extern void 				udplitev6_exit(void);
+extern int				tcpv6_init(void);
+extern void				tcpv6_exit(void);
+
+extern int				udpv6_connect(struct sock *sk,
+						      struct sockaddr *uaddr,
+						      int addr_len);
+
+extern int			datagram_recv_ctl(struct sock *sk,
+						  struct msghdr *msg,
+						  struct sk_buff *skb);
+
+extern int			datagram_send_ctl(struct net *net,
+						  struct msghdr *msg,
+						  struct flowi *fl,
+						  struct ipv6_txoptions *opt,
+						  int *hlimit, int *tclass);
+
+extern void		ip6_dgram_sock_seq_show(struct seq_file *seq,
+						struct sock *sp,
+						__u16 srcp,
+						__u16 destp,
+						int bucket);
+
+#define		LOOPBACK4_IPV6		cpu_to_be32(0x7f000006)
+
+/*
+ *	address family specific functions
+ */
+extern const struct inet_connection_sock_af_ops ipv4_specific;
+
+extern void inet6_destroy_sock(struct sock *sk);
+
+#define IPV6_SEQ_DGRAM_HEADER \
+			   "  sl  "					\
+			   "local_address                         "	\
+			   "remote_address                        "	\
+			   "st tx_queue rx_queue tr tm->when retrnsmt"	\
+			   "   uid  timeout inode ref pointer drops\n"
+
+#endif
+
+#endif
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/kernel/hookers.c linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/hookers.c
--- linux-2.6.32-642.11.1.el6.x86_64/kernel/hookers.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/hookers.c	2016-12-13 17:25:10.423084079 +0800
@@ -0,0 +1,286 @@
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/rculist.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <linux/hookers.h>
+
+#include <net/net_namespace.h>
+#include <net/tcp.h>
+#include <net/transp_v6.h>
+#include <net/inet_common.h>
+#include <net/ipv6.h>
+#include <linux/inet.h>
+
+struct hooked_place {
+	char *name;	/* position information shown in procfs */
+	void *place;	/* the kernel address to be hook */
+	void *orig;	/* original content at hooked place */
+	void *stub;	/* hooker function stub */
+	int nr_hookers;	/* how many hookers are linked at below chain */
+	struct list_head chain;	/* hookers chain */
+};
+
+static spinlock_t hookers_lock;
+
+static struct sock *ipv4_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static struct sock *ipv6_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static struct sock *ipv6_mapped_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst);
+static int inet_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer);
+static int inet6_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer);
+
+static struct hooked_place place_table[] = {
+
+	{
+		.name = "ipv4_specific.syn_recv_sock",
+		.place = &ipv4_specific.syn_recv_sock,
+		.stub = ipv4_specific_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "ipv6_specific.syn_recv_sock",
+		.place = &ipv6_specific.syn_recv_sock,
+		.stub = ipv6_specific_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "ipv6_mapped.syn_recv_sock",
+		.place = &ipv6_mapped.syn_recv_sock,
+		.stub = ipv6_mapped_syn_recv_sock_stub,
+	},
+
+	{
+		.name = "inet_stream_ops.getname",
+		.place = &inet_stream_ops.getname,
+		.stub = inet_stream_ops_getname_stub,
+	},
+
+	{
+		.name = "inet6_stream_ops.getname",
+		.place = &inet6_stream_ops.getname,
+		.stub = inet6_stream_ops_getname_stub,
+	},
+
+};
+
+static struct sock *__syn_recv_sock_hstub(struct hooked_place *place,
+				struct sock *sk, struct sk_buff *skb,
+			  struct request_sock *req, struct dst_entry *dst)
+{
+	struct hooker *iter;
+	struct sock *(*hooker_func)(struct sock *sk, struct sk_buff *skb,
+		  struct request_sock *req, struct dst_entry *dst,
+						struct sock **ret);
+	struct sock *(*orig_func)(struct sock *sk, struct sk_buff *skb,
+		  struct request_sock *req, struct dst_entry *dst);
+	struct sock *ret;
+
+	orig_func = place->orig;
+	ret = orig_func(sk, skb, req, dst);
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(iter, &place->chain, chain) {
+		hooker_func = iter->func;
+		hooker_func(sk, skb, req, dst, &ret);
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
+static int __getname_hstub(struct hooked_place *place,
+				struct socket *sock, struct sockaddr *uaddr,
+						int *uaddr_len, int peer)
+{
+	struct hooker *iter;
+	int (*hooker_func)(struct socket *sock, struct sockaddr *uaddr,
+			 int *uaddr_len, int peer, int *ret);
+	int (*orig_func)(struct socket *sock, struct sockaddr *uaddr,
+			 int *uaddr_len, int peer);
+	int ret;
+
+	orig_func = place->orig;
+	ret = orig_func(sock, uaddr, uaddr_len, peer);
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(iter, &place->chain, chain) {
+		hooker_func = iter->func;
+		hooker_func(sock, uaddr, uaddr_len, peer, &ret);
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
+static struct sock *ipv4_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[0], sk, skb, req, dst);
+}
+
+static struct sock *ipv6_specific_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[1], sk, skb, req, dst);
+}
+
+static struct sock *ipv6_mapped_syn_recv_sock_stub(struct sock *sk,
+		struct sk_buff *skb, struct request_sock *req,
+		struct dst_entry *dst)
+{
+	return __syn_recv_sock_hstub(&place_table[2], sk, skb, req, dst);
+}
+
+static int inet_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer)
+{
+	return __getname_hstub(&place_table[3], sock, uaddr, uaddr_len, peer);
+}
+
+static int inet6_stream_ops_getname_stub(struct socket *sock,
+		struct sockaddr *uaddr, int *uaddr_len, int peer)
+{
+	return __getname_hstub(&place_table[4], sock, uaddr, uaddr_len, peer);
+}
+
+#define PLACE_TABLE_SZ	(sizeof((place_table))/sizeof((place_table)[0]))
+
+int hooker_install(void *place, struct hooker *h)
+{
+	int i;
+	struct hooked_place *hplace;
+
+	might_sleep(); /* synchronize_rcu() */
+
+	if (!place || !h || !h->func)
+		return -EINVAL;
+
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		hplace = &place_table[i];
+		if (hplace->place == place) {
+			INIT_LIST_HEAD(&h->chain);
+			spin_lock(&hookers_lock);
+			hplace->nr_hookers++;
+			h->hplace = hplace;
+			list_add_tail_rcu(&h->chain, &place_table[i].chain);
+			spin_unlock(&hookers_lock);
+			synchronize_rcu();
+			break;
+		}
+	}
+
+	return (i >= PLACE_TABLE_SZ) ? -EINVAL : 0;
+}
+EXPORT_SYMBOL_GPL(hooker_install);
+
+void hooker_uninstall(struct hooker *h)
+{
+	might_sleep(); /* synchronize_rcu(); */
+
+	spin_lock(&hookers_lock);
+	list_del_rcu(&h->chain);
+	h->hplace->nr_hookers--;
+	h->hplace = NULL;
+	spin_unlock(&hookers_lock);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL_GPL(hooker_uninstall);
+
+static void *hookers_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	if (*pos < PLACE_TABLE_SZ)
+		return &place_table[*pos];
+	return NULL;
+}
+
+static void *hookers_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	if (++(*pos) >= PLACE_TABLE_SZ)
+		return NULL;
+
+	return (void *)&place_table[*pos];
+}
+
+static void hookers_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int hookers_seq_show(struct seq_file *seq, void *v)
+{
+	struct hooked_place *hplace = (struct hooked_place *)v;
+
+	seq_printf(seq, "name:%-24s addr:0x%p hookers:%-10d\n",
+			hplace->name, hplace->place, hplace->nr_hookers);
+	return 0;
+}
+
+static const struct seq_operations hookers_seq_ops = {
+	.start = hookers_seq_start,
+	.next  = hookers_seq_next,
+	.stop  = hookers_seq_stop,
+	.show  = hookers_seq_show,
+};
+
+static int hookers_seq_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &hookers_seq_ops);
+}
+
+static const struct file_operations hookers_seq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = hookers_seq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int hookers_init(void)
+{
+	int i;
+
+	if (!proc_create("hookers", 0, NULL, &hookers_seq_fops))
+		return -ENODEV;
+
+	spin_lock_init(&hookers_lock);
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		void **place = place_table[i].place;
+
+		place_table[i].orig = *place;
+		INIT_LIST_HEAD(&place_table[i].chain);
+		if (!place_table[i].stub)
+			break;
+		*place = place_table[i].stub;
+	}
+
+	return 0;
+}
+
+static void hookers_exit(void)
+{
+	int i;
+
+	remove_proc_entry("hookers", NULL);
+
+	for (i = 0; i < PLACE_TABLE_SZ; i++) {
+		void **place = place_table[i].place;
+		*place = place_table[i].orig;
+	}
+	synchronize_rcu();
+}
+
+module_init(hookers_init);
+module_exit(hookers_exit);
+MODULE_LICENSE("GPL");
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/kernel/kallsyms.c linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/kallsyms.c
--- linux-2.6.32-642.11.1.el6.x86_64/kernel/kallsyms.c	2016-12-13 17:22:01.246074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/kallsyms.c	2016-12-13 17:25:15.712067714 +0800
@@ -181,6 +181,7 @@
 	}
 	return module_kallsyms_lookup_name(name);
 }
+EXPORT_SYMBOL_GPL(kallsyms_lookup_name);
 
 int kallsyms_on_each_symbol(int (*fn)(void *, const char *, struct module *,
 				      unsigned long),
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/kernel/Makefile linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/Makefile
--- linux-2.6.32-642.11.1.el6.x86_64/kernel/Makefile	2016-12-13 17:22:01.248074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/Makefile	2016-12-13 17:25:10.423084079 +0800
@@ -107,6 +107,7 @@
 obj-$(CONFIG_SLOW_WORK_DEBUG) += slow-work-debugfs.o
 obj-$(CONFIG_PERF_EVENTS) += events/
 obj-$(CONFIG_USER_RETURN_NOTIFIER) += user-return-notifier.o
+obj-$(CONFIG_HOOKERS) += hookers.o
 
 ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 # According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/kernel/Makefile.orig linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/Makefile.orig
--- linux-2.6.32-642.11.1.el6.x86_64/kernel/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/kernel/Makefile.orig	2016-12-13 17:23:55.751074270 +0800
@@ -0,0 +1,140 @@
+#
+# Makefile for the linux kernel.
+#
+
+obj-y     = sched.o fork.o exec_domain.o panic.o printk.o \
+	    cpu.o exit.o itimer.o time.o softirq.o resource.o \
+	    sysctl.o capability.o ptrace.o timer.o user.o \
+	    signal.o sys.o kmod.o workqueue.o pid.o \
+	    rcupdate.o extable.o params.o posix-timers.o kthread.o \
+	    wait.o kfifo.o kfifo-new.o sys_ni.o posix-cpu-timers.o mutex.o \
+	    hrtimer.o rwsem.o nsproxy.o srcu.o semaphore.o \
+	    notifier.o ksysfs.o pm_qos_params.o sched_clock.o cred.o \
+	    async.o range.o rh_taint.o rh_kabi.o
+obj-y += groups.o
+
+ifdef CONFIG_FUNCTION_TRACER
+# Do not trace debug files and internal ftrace files
+CFLAGS_REMOVE_lockdep.o = -pg
+CFLAGS_REMOVE_lockdep_proc.o = -pg
+CFLAGS_REMOVE_mutex-debug.o = -pg
+CFLAGS_REMOVE_rtmutex-debug.o = -pg
+CFLAGS_REMOVE_cgroup-debug.o = -pg
+CFLAGS_REMOVE_sched_clock.o = -pg
+CFLAGS_REMOVE_irq_work.o = -pg
+endif
+
+obj-$(CONFIG_FREEZER) += freezer.o
+obj-$(CONFIG_PROFILING) += profile.o
+obj-$(CONFIG_SYSCTL_SYSCALL_CHECK) += sysctl_check.o
+obj-$(CONFIG_STACKTRACE) += stacktrace.o
+obj-y += time/
+obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+obj-$(CONFIG_LOCKDEP) += lockdep.o
+ifeq ($(CONFIG_PROC_FS),y)
+obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
+endif
+obj-$(CONFIG_FUTEX) += futex.o
+ifeq ($(CONFIG_COMPAT),y)
+obj-$(CONFIG_FUTEX) += futex_compat.o
+endif
+obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
+obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
+obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
+obj-$(CONFIG_GENERIC_ISA_DMA) += dma.o
+obj-$(CONFIG_USE_GENERIC_SMP_HELPERS) += smp.o
+ifneq ($(CONFIG_SMP),y)
+obj-y += up.o
+endif
+obj-$(CONFIG_SMP) += spinlock.o
+obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
+obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
+obj-$(CONFIG_UID16) += uid16.o
+obj-$(CONFIG_MODULES) += module.o
+obj-$(CONFIG_MODULE_VERIFY) += module-verify.o
+obj-$(CONFIG_MODULE_VERIFY_ELF) += module-verify-elf.o
+obj-$(CONFIG_MODULE_SIG) += module-verify-sig.o
+obj-$(CONFIG_KALLSYMS) += kallsyms.o
+obj-$(CONFIG_PM) += power/
+obj-$(CONFIG_FREEZER) += power/
+obj-$(CONFIG_BSD_PROCESS_ACCT) += acct.o
+obj-$(CONFIG_KEXEC) += kexec.o
+obj-$(CONFIG_BACKTRACE_SELF_TEST) += backtracetest.o
+obj-$(CONFIG_COMPAT) += compat.o
+obj-$(CONFIG_CGROUPS) += cgroup.o
+obj-$(CONFIG_CGROUP_FREEZER) += cgroup_freezer.o
+obj-$(CONFIG_CPUSETS) += cpuset.o
+obj-$(CONFIG_CGROUP_NS) += ns_cgroup.o
+obj-$(CONFIG_UTS_NS) += utsname.o
+obj-$(CONFIG_USER_NS) += user_namespace.o
+obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_IKCONFIG) += configs.o
+obj-$(CONFIG_RESOURCE_COUNTERS) += res_counter.o
+obj-$(CONFIG_SMP) += stop_machine.o
+obj-$(CONFIG_KPROBES_SANITY_TEST) += test_kprobes.o
+obj-$(CONFIG_UTRACE) += utrace.o
+obj-$(CONFIG_UTRACE) += ptrace-utrace.o
+obj-$(CONFIG_AUDIT) += audit.o auditfilter.o audit_watch.o
+obj-$(CONFIG_AUDITSYSCALL) += auditsc.o
+obj-$(CONFIG_GCOV_KERNEL) += gcov/
+obj-$(CONFIG_AUDIT_TREE) += audit_tree.o
+obj-$(CONFIG_KPROBES) += kprobes.o
+obj-$(CONFIG_KGDB) += kgdb.o
+obj-$(CONFIG_DETECT_HUNG_TASK) += hung_task.o
+obj-$(CONFIG_LOCKUP_DETECTOR) += watchdog.o
+obj-$(CONFIG_GENERIC_HARDIRQS) += irq/
+obj-$(CONFIG_SECCOMP) += seccomp.o
+obj-$(CONFIG_RCU_TORTURE_TEST) += rcutorture.o
+obj-$(CONFIG_TREE_RCU) += rcutree.o
+obj-$(CONFIG_TREE_PREEMPT_RCU) += rcutree.o
+obj-$(CONFIG_TREE_RCU_TRACE) += rcutree_trace.o
+obj-$(CONFIG_RELAY) += relay.o
+obj-$(CONFIG_SYSCTL) += utsname_sysctl.o
+obj-$(CONFIG_TASK_DELAY_ACCT) += delayacct.o
+obj-$(CONFIG_TASKSTATS) += taskstats.o tsacct.o
+obj-$(CONFIG_TRACEPOINTS) += tracepoint.o
+obj-$(CONFIG_LATENCYTOP) += latencytop.o
+obj-$(CONFIG_BINFMT_ELF) += elfcore.o
+obj-$(CONFIG_COMPAT_BINFMT_ELF) += elfcore.o
+obj-$(CONFIG_BINFMT_ELF_FDPIC) += elfcore.o
+obj-$(CONFIG_FUNCTION_TRACER) += trace/
+obj-$(CONFIG_TRACING) += trace/
+obj-$(CONFIG_X86_DS) += trace/
+obj-$(CONFIG_RING_BUFFER) += trace/
+obj-$(CONFIG_SMP) += sched_cpupri.o
+obj-$(CONFIG_IRQ_WORK) += irq_work.o
+obj-$(CONFIG_SLOW_WORK) += slow-work.o
+obj-$(CONFIG_SLOW_WORK_DEBUG) += slow-work-debugfs.o
+obj-$(CONFIG_PERF_EVENTS) += events/
+obj-$(CONFIG_USER_RETURN_NOTIFIER) += user-return-notifier.o
+
+ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
+# According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
+# needed for x86 only.  Why this used to be enabled for all architectures is beyond
+# me.  I suspect most platforms don't need this, but until we know that for sure
+# I turn this off for IA-64 only.  Andreas Schwab says it's also needed on m68k
+# to get a correct value for the wait-channel (WCHAN in ps). --davidm
+CFLAGS_sched.o := $(PROFILING) -fno-omit-frame-pointer
+endif
+
+$(obj)/configs.o: $(obj)/config_data.h
+
+# config_data.h contains the same information as ikconfig.h but gzipped.
+# Info from config_data can be extracted from /proc/config*
+targets += config_data.gz
+$(obj)/config_data.gz: .config FORCE
+	$(call if_changed,gzip)
+
+quiet_cmd_ikconfiggz = IKCFG   $@
+      cmd_ikconfiggz = (echo "static const char kernel_config_data[] __used = MAGIC_START"; cat $< | scripts/bin2c; echo "MAGIC_END;") > $@
+targets += config_data.h
+$(obj)/config_data.h: $(obj)/config_data.gz FORCE
+	$(call if_changed,ikconfiggz)
+
+$(obj)/time.o: $(obj)/timeconst.h
+
+quiet_cmd_timeconst  = TIMEC   $@
+      cmd_timeconst  = $(PERL) $< $(CONFIG_HZ) > $@
+targets += timeconst.h
+$(obj)/timeconst.h: $(src)/timeconst.pl FORCE
+	$(call if_changed,timeconst)
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/af_inet.c linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/af_inet.c
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/af_inet.c	2016-12-13 17:21:59.506074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/af_inet.c	2016-12-13 17:25:15.712067714 +0800
@@ -873,7 +873,7 @@
 }
 EXPORT_SYMBOL(inet_ioctl);
 
-const struct proto_ops inet_stream_ops = {
+struct proto_ops inet_stream_ops = {
 	.family		   = PF_INET,
 	.owner		   = THIS_MODULE,
 	.release	   = inet_release,
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/af_inet.c.orig linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/af_inet.c.orig
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/af_inet.c.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/af_inet.c.orig	2016-12-13 17:23:54.015074270 +0800
@@ -0,0 +1,1815 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		PF_INET protocol family socket handler.
+ *
+ * Authors:	Ross Biro
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *		Florian La Roche, <flla@stud.uni-sb.de>
+ *		Alan Cox, <A.Cox@swansea.ac.uk>
+ *
+ * Changes (see also sock.c)
+ *
+ *		piggy,
+ *		Karl Knutson	:	Socket protocol table
+ *		A.N.Kuznetsov	:	Socket death error in accept().
+ *		John Richardson :	Fix non blocking error in connect()
+ *					so sockets that fail to connect
+ *					don't return -EINPROGRESS.
+ *		Alan Cox	:	Asynchronous I/O support
+ *		Alan Cox	:	Keep correct socket pointer on sock
+ *					structures
+ *					when accept() ed
+ *		Alan Cox	:	Semantics of SO_LINGER aren't state
+ *					moved to close when you look carefully.
+ *					With this fixed and the accept bug fixed
+ *					some RPC stuff seems happier.
+ *		Niibe Yutaka	:	4.4BSD style write async I/O
+ *		Alan Cox,
+ *		Tony Gale 	:	Fixed reuse semantics.
+ *		Alan Cox	:	bind() shouldn't abort existing but dead
+ *					sockets. Stops FTP netin:.. I hope.
+ *		Alan Cox	:	bind() works correctly for RAW sockets.
+ *					Note that FreeBSD at least was broken
+ *					in this respect so be careful with
+ *					compatibility tests...
+ *		Alan Cox	:	routing cache support
+ *		Alan Cox	:	memzero the socket structure for
+ *					compactness.
+ *		Matt Day	:	nonblock connect error handler
+ *		Alan Cox	:	Allow large numbers of pending sockets
+ *					(eg for big web sites), but only if
+ *					specifically application requested.
+ *		Alan Cox	:	New buffering throughout IP. Used
+ *					dumbly.
+ *		Alan Cox	:	New buffering now used smartly.
+ *		Alan Cox	:	BSD rather than common sense
+ *					interpretation of listen.
+ *		Germano Caronni	:	Assorted small races.
+ *		Alan Cox	:	sendmsg/recvmsg basic support.
+ *		Alan Cox	:	Only sendmsg/recvmsg now supported.
+ *		Alan Cox	:	Locked down bind (see security list).
+ *		Alan Cox	:	Loosened bind a little.
+ *		Mike McLagan	:	ADD/DEL DLCI Ioctls
+ *	Willy Konynenberg	:	Transparent proxying support.
+ *		David S. Miller	:	New socket lookup architecture.
+ *					Some other random speedups.
+ *		Cyrus Durgin	:	Cleaned up file for kmod hacks.
+ *		Andi Kleen	:	Fix inet_stream_connect TCP race.
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <linux/string.h>
+#include <linux/sockios.h>
+#include <linux/net.h>
+#include <linux/capability.h>
+#include <linux/fcntl.h>
+#include <linux/mm.h>
+#include <linux/interrupt.h>
+#include <linux/stat.h>
+#include <linux/init.h>
+#include <linux/poll.h>
+#include <linux/netfilter_ipv4.h>
+#include <linux/random.h>
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+
+#include <linux/inet.h>
+#include <linux/igmp.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <net/checksum.h>
+#include <net/ip.h>
+#include <net/protocol.h>
+#include <net/arp.h>
+#include <net/route.h>
+#include <net/ip_fib.h>
+#include <net/inet_connection_sock.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/udplite.h>
+#include <net/ping.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <net/raw.h>
+#include <net/icmp.h>
+#include <net/inet_common.h>
+#include <net/xfrm.h>
+#include <net/net_namespace.h>
+#ifdef CONFIG_IP_MROUTE
+#include <linux/mroute.h>
+#endif
+
+
+/* The inetsw table contains everything that inet_create needs to
+ * build a new socket.
+ */
+static struct list_head inetsw[SOCK_MAX];
+static DEFINE_SPINLOCK(inetsw_lock);
+
+/* New destruction routine */
+
+void inet_sock_destruct(struct sock *sk)
+{
+	struct inet_sock *inet = inet_sk(sk);
+
+	__skb_queue_purge(&sk->sk_receive_queue);
+	__skb_queue_purge(&sk->sk_error_queue);
+
+	sk_mem_reclaim(sk);
+
+	if (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {
+		pr_err("Attempt to release TCP socket in state %d %p\n",
+		       sk->sk_state, sk);
+		return;
+	}
+	if (!sock_flag(sk, SOCK_DEAD)) {
+		pr_err("Attempt to release alive inet socket %p\n", sk);
+		return;
+	}
+
+	WARN_ON(atomic_read(&sk->sk_rmem_alloc));
+	WARN_ON(atomic_read(&sk->sk_wmem_alloc));
+	WARN_ON(sk->sk_wmem_queued);
+	WARN_ON(sk->sk_forward_alloc);
+
+	kfree_ip_options(rcu_dereference(inet->opt));
+	dst_release(sk->sk_dst_cache);
+	sk_refcnt_debug_dec(sk);
+}
+EXPORT_SYMBOL(inet_sock_destruct);
+
+/*
+ *	The routines beyond this point handle the behaviour of an AF_INET
+ *	socket object. Mostly it punts to the subprotocols of IP to do
+ *	the work.
+ */
+
+/*
+ *	Automatically bind an unbound socket.
+ */
+
+static int inet_autobind(struct sock *sk)
+{
+	struct inet_sock *inet;
+	/* We may need to bind the socket. */
+	lock_sock(sk);
+	inet = inet_sk(sk);
+	if (!inet->num) {
+		if (sk->sk_prot->get_port(sk, 0)) {
+			release_sock(sk);
+			return -EAGAIN;
+		}
+		inet->sport = htons(inet->num);
+	}
+	release_sock(sk);
+	return 0;
+}
+
+/*
+ *	Move a socket into listening state.
+ */
+int inet_listen(struct socket *sock, int backlog)
+{
+	struct sock *sk = sock->sk;
+	unsigned char old_state;
+	int err;
+
+	lock_sock(sk);
+
+	err = -EINVAL;
+	if (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)
+		goto out;
+
+	old_state = sk->sk_state;
+	if (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))
+		goto out;
+
+	/* Really, if the socket is already in listen state
+	 * we can only allow the backlog to be adjusted.
+	 */
+	if (old_state != TCP_LISTEN) {
+		err = inet_csk_listen_start(sk, backlog);
+		if (err)
+			goto out;
+	}
+	sk->sk_max_ack_backlog = backlog;
+	err = 0;
+
+out:
+	release_sock(sk);
+	return err;
+}
+EXPORT_SYMBOL(inet_listen);
+
+u32 inet_ehash_secret __read_mostly;
+EXPORT_SYMBOL(inet_ehash_secret);
+
+/*
+ * inet_ehash_secret must be set exactly once
+ * Instead of using a dedicated spinlock, we (ab)use inetsw_lock
+ */
+void build_ehash_secret(void)
+{
+	u32 rnd;
+	do {
+		get_random_bytes(&rnd, sizeof(rnd));
+	} while (rnd == 0);
+	spin_lock_bh(&inetsw_lock);
+	if (!inet_ehash_secret)
+		inet_ehash_secret = rnd;
+	spin_unlock_bh(&inetsw_lock);
+}
+EXPORT_SYMBOL(build_ehash_secret);
+
+static inline int inet_netns_ok(struct net *net, int protocol)
+{
+	int hash;
+	const struct net_protocol *ipprot;
+
+	if (net_eq(net, &init_net))
+		return 1;
+
+	hash = protocol & (MAX_INET_PROTOS - 1);
+	ipprot = rcu_dereference(inet_protos[hash]);
+
+	if (ipprot == NULL)
+		/* raw IP is OK */
+		return 1;
+	return ipprot->netns_ok;
+}
+
+/*
+ *	Create an inet socket.
+ */
+
+static int inet_create(struct net *net, struct socket *sock, int protocol,
+		       int kern)
+{
+	struct sock *sk;
+	struct inet_protosw *answer;
+	struct inet_sock *inet;
+	struct proto *answer_prot;
+	unsigned char answer_flags;
+	char answer_no_check;
+	int try_loading_module = 0;
+	int err;
+
+	if (protocol < 0 || protocol >= IPPROTO_MAX)
+		return -EINVAL;
+
+	if (unlikely(!inet_ehash_secret))
+		if (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)
+			build_ehash_secret();
+
+	sock->state = SS_UNCONNECTED;
+
+	/* Look for the requested type/protocol pair. */
+lookup_protocol:
+	err = -ESOCKTNOSUPPORT;
+	rcu_read_lock();
+	list_for_each_entry_rcu(answer, &inetsw[sock->type], list) {
+
+		err = 0;
+		/* Check the non-wild match. */
+		if (protocol == answer->protocol) {
+			if (protocol != IPPROTO_IP)
+				break;
+		} else {
+			/* Check for the two wild cases. */
+			if (IPPROTO_IP == protocol) {
+				protocol = answer->protocol;
+				break;
+			}
+			if (IPPROTO_IP == answer->protocol)
+				break;
+		}
+		err = -EPROTONOSUPPORT;
+	}
+
+	if (unlikely(err)) {
+		if (try_loading_module < 2) {
+			rcu_read_unlock();
+			/*
+			 * Be more specific, e.g. net-pf-2-proto-132-type-1
+			 * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)
+			 */
+			if (++try_loading_module == 1)
+				request_module("net-pf-%d-proto-%d-type-%d",
+					       PF_INET, protocol, sock->type);
+			/*
+			 * Fall back to generic, e.g. net-pf-2-proto-132
+			 * (net-pf-PF_INET-proto-IPPROTO_SCTP)
+			 */
+			else
+				request_module("net-pf-%d-proto-%d",
+					       PF_INET, protocol);
+			goto lookup_protocol;
+		} else
+			goto out_rcu_unlock;
+	}
+
+	err = -EPERM;
+	if (sock->type == SOCK_RAW && !kern && !capable(CAP_NET_RAW))
+		goto out_rcu_unlock;
+
+	err = -EAFNOSUPPORT;
+	if (!inet_netns_ok(net, protocol))
+		goto out_rcu_unlock;
+
+	sock->ops = answer->ops;
+	answer_prot = answer->prot;
+	answer_no_check = answer->no_check;
+	answer_flags = answer->flags;
+	rcu_read_unlock();
+
+	WARN_ON(answer_prot->slab == NULL);
+
+	err = -ENOBUFS;
+	sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot);
+	if (sk == NULL)
+		goto out;
+
+	err = 0;
+	sk->sk_no_check = answer_no_check;
+	if (INET_PROTOSW_REUSE & answer_flags)
+		sk->sk_reuse = 1;
+
+	inet = inet_sk(sk);
+	inet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;
+
+	if (SOCK_RAW == sock->type) {
+		inet->num = protocol;
+		if (IPPROTO_RAW == protocol)
+			inet->hdrincl = 1;
+	}
+
+	if (net->sysctl_ip_no_pmtu_disc)
+		inet->pmtudisc = IP_PMTUDISC_DONT;
+	else
+		inet->pmtudisc = IP_PMTUDISC_WANT;
+
+	inet->id = 0;
+
+	sock_init_data(sock, sk);
+
+	sk->sk_destruct	   = inet_sock_destruct;
+	sk->sk_protocol	   = protocol;
+	sk->sk_backlog_rcv = sk->sk_prot->backlog_rcv;
+
+	inet->uc_ttl	= -1;
+	inet->mc_loop	= 1;
+	inet->mc_ttl	= 1;
+	inet->mc_all	= 1;
+	inet->mc_index	= 0;
+	inet->mc_list	= NULL;
+	sk_extended(sk)->rcv_tos = 0;
+
+	sk_refcnt_debug_inc(sk);
+
+	if (inet->num) {
+		/* It assumes that any protocol which allows
+		 * the user to assign a number at socket
+		 * creation time automatically
+		 * shares.
+		 */
+		inet->sport = htons(inet->num);
+		/* Add to protocol hash chains. */
+		sk->sk_prot->hash(sk);
+	}
+
+	if (sk->sk_prot->init) {
+		err = sk->sk_prot->init(sk);
+		if (err)
+			sk_common_release(sk);
+	}
+out:
+	return err;
+out_rcu_unlock:
+	rcu_read_unlock();
+	goto out;
+}
+
+
+/*
+ *	The peer socket should always be NULL (or else). When we call this
+ *	function we are destroying the object and from then on nobody
+ *	should refer to it.
+ */
+int inet_release(struct socket *sock)
+{
+	struct sock *sk = sock->sk;
+
+	if (sk) {
+		long timeout;
+
+		inet_rps_reset_flow(sk);
+
+		/* Applications forget to leave groups before exiting */
+		ip_mc_drop_socket(sk);
+
+		/* If linger is set, we don't return until the close
+		 * is complete.  Otherwise we return immediately. The
+		 * actually closing is done the same either way.
+		 *
+		 * If the close is due to the process exiting, we never
+		 * linger..
+		 */
+		timeout = 0;
+		if (sock_flag(sk, SOCK_LINGER) &&
+		    !(current->flags & PF_EXITING))
+			timeout = sk->sk_lingertime;
+		sock->sk = NULL;
+		sk->sk_prot->close(sk, timeout);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(inet_release);
+
+/* It is off by default, see below. */
+int sysctl_ip_nonlocal_bind __read_mostly;
+EXPORT_SYMBOL(sysctl_ip_nonlocal_bind);
+
+int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
+{
+	struct sockaddr_in *addr = (struct sockaddr_in *)uaddr;
+	struct sock *sk = sock->sk;
+	struct inet_sock *inet = inet_sk(sk);
+	unsigned short snum;
+	int chk_addr_ret;
+	int err;
+
+	/* If the socket has its own bind function then use it. (RAW) */
+	if (sk->sk_prot->bind) {
+		err = sk->sk_prot->bind(sk, uaddr, addr_len);
+		goto out;
+	}
+	err = -EINVAL;
+	if (addr_len < sizeof(struct sockaddr_in))
+		goto out;
+
+	chk_addr_ret = inet_addr_type(sock_net(sk), addr->sin_addr.s_addr);
+
+	/* Not specified by any standard per-se, however it breaks too
+	 * many applications when removed.  It is unfortunate since
+	 * allowing applications to make a non-local bind solves
+	 * several problems with systems using dynamic addressing.
+	 * (ie. your servers still start up even if your ISDN link
+	 *  is temporarily down)
+	 */
+	err = -EADDRNOTAVAIL;
+	if (!sysctl_ip_nonlocal_bind &&
+	    !(inet->freebind || inet->transparent) &&
+	    addr->sin_addr.s_addr != htonl(INADDR_ANY) &&
+	    chk_addr_ret != RTN_LOCAL &&
+	    chk_addr_ret != RTN_MULTICAST &&
+	    chk_addr_ret != RTN_BROADCAST)
+		goto out;
+
+	snum = ntohs(addr->sin_port);
+	err = -EACCES;
+	if (snum && snum < PROT_SOCK && !capable(CAP_NET_BIND_SERVICE))
+		goto out;
+
+	/*      We keep a pair of addresses. rcv_saddr is the one
+	 *      used by hash lookups, and saddr is used for transmit.
+	 *
+	 *      In the BSD API these are the same except where it
+	 *      would be illegal to use them (multicast/broadcast) in
+	 *      which case the sending device address is used.
+	 */
+	lock_sock(sk);
+
+	/* Check these errors (active socket, double bind). */
+	err = -EINVAL;
+	if (sk->sk_state != TCP_CLOSE || inet->num)
+		goto out_release_sock;
+
+	inet->rcv_saddr = inet->saddr = addr->sin_addr.s_addr;
+	if (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)
+		inet->saddr = 0;  /* Use device */
+
+	/* Make sure we are allowed to bind here. */
+	if (sk->sk_prot->get_port(sk, snum)) {
+		inet->saddr = inet->rcv_saddr = 0;
+		err = -EADDRINUSE;
+		goto out_release_sock;
+	}
+
+	if (inet->rcv_saddr)
+		sk->sk_userlocks |= SOCK_BINDADDR_LOCK;
+	if (snum)
+		sk->sk_userlocks |= SOCK_BINDPORT_LOCK;
+	inet->sport = htons(inet->num);
+	inet->daddr = 0;
+	inet->dport = 0;
+	sk_dst_reset(sk);
+	err = 0;
+out_release_sock:
+	release_sock(sk);
+out:
+	return err;
+}
+EXPORT_SYMBOL(inet_bind);
+
+int inet_dgram_connect(struct socket *sock, struct sockaddr * uaddr,
+		       int addr_len, int flags)
+{
+	struct sock *sk = sock->sk;
+
+	if (uaddr->sa_family == AF_UNSPEC)
+		return sk->sk_prot->disconnect(sk, flags);
+
+	if (!inet_sk(sk)->num && inet_autobind(sk))
+		return -EAGAIN;
+	return sk->sk_prot->connect(sk, (struct sockaddr *)uaddr, addr_len);
+}
+EXPORT_SYMBOL(inet_dgram_connect);
+
+static long inet_wait_for_connect(struct sock *sk, long timeo)
+{
+	DEFINE_WAIT(wait);
+
+	prepare_to_wait(sk->sk_sleep, &wait, TASK_INTERRUPTIBLE);
+
+	/* Basic assumption: if someone sets sk->sk_err, he _must_
+	 * change state of the socket from TCP_SYN_*.
+	 * Connect() does not allow to get error notifications
+	 * without closing the socket.
+	 */
+	while ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
+		release_sock(sk);
+		timeo = schedule_timeout(timeo);
+		lock_sock(sk);
+		if (signal_pending(current) || !timeo)
+			break;
+		prepare_to_wait(sk->sk_sleep, &wait, TASK_INTERRUPTIBLE);
+	}
+	finish_wait(sk->sk_sleep, &wait);
+	return timeo;
+}
+
+/*
+ *	Connect to a remote host. There is regrettably still a little
+ *	TCP 'magic' in here.
+ */
+int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
+			int addr_len, int flags)
+{
+	struct sock *sk = sock->sk;
+	int err;
+	long timeo;
+
+	lock_sock(sk);
+
+	if (uaddr->sa_family == AF_UNSPEC) {
+		err = sk->sk_prot->disconnect(sk, flags);
+		sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
+		goto out;
+	}
+
+	switch (sock->state) {
+	default:
+		err = -EINVAL;
+		goto out;
+	case SS_CONNECTED:
+		err = -EISCONN;
+		goto out;
+	case SS_CONNECTING:
+		err = -EALREADY;
+		/* Fall out of switch with err, set for this state */
+		break;
+	case SS_UNCONNECTED:
+		err = -EISCONN;
+		if (sk->sk_state != TCP_CLOSE)
+			goto out;
+
+		err = sk->sk_prot->connect(sk, uaddr, addr_len);
+		if (err < 0)
+			goto out;
+
+		sock->state = SS_CONNECTING;
+
+		/* Just entered SS_CONNECTING state; the only
+		 * difference is that return value in non-blocking
+		 * case is EINPROGRESS, rather than EALREADY.
+		 */
+		err = -EINPROGRESS;
+		break;
+	}
+
+	timeo = sock_sndtimeo(sk, flags & O_NONBLOCK);
+
+	if ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
+		/* Error code is set above */
+		if (!timeo || !inet_wait_for_connect(sk, timeo))
+			goto out;
+
+		err = sock_intr_errno(timeo);
+		if (signal_pending(current))
+			goto out;
+	}
+
+	/* Connection was closed by RST, timeout, ICMP error
+	 * or another process disconnected us.
+	 */
+	if (sk->sk_state == TCP_CLOSE)
+		goto sock_error;
+
+	/* sk->sk_err may be not zero now, if RECVERR was ordered by user
+	 * and error was received after socket entered established state.
+	 * Hence, it is handled normally after connect() return successfully.
+	 */
+
+	sock->state = SS_CONNECTED;
+	err = 0;
+out:
+	release_sock(sk);
+	return err;
+
+sock_error:
+	err = sock_error(sk) ? : -ECONNABORTED;
+	sock->state = SS_UNCONNECTED;
+	if (sk->sk_prot->disconnect(sk, flags))
+		sock->state = SS_DISCONNECTING;
+	goto out;
+}
+EXPORT_SYMBOL(inet_stream_connect);
+
+/*
+ *	Accept a pending connection. The TCP layer now gives BSD semantics.
+ */
+
+int inet_accept(struct socket *sock, struct socket *newsock, int flags)
+{
+	struct sock *sk1 = sock->sk;
+	int err = -EINVAL;
+	struct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err);
+
+	if (!sk2)
+		goto do_err;
+
+	lock_sock(sk2);
+
+	inet_rps_record_flow(sk2);
+	WARN_ON(!((1 << sk2->sk_state) &
+		  (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_CLOSE)));
+
+	sock_graft(sk2, newsock);
+
+	newsock->state = SS_CONNECTED;
+	err = 0;
+	release_sock(sk2);
+do_err:
+	return err;
+}
+EXPORT_SYMBOL(inet_accept);
+
+
+/*
+ *	This does both peername and sockname.
+ */
+int inet_getname(struct socket *sock, struct sockaddr *uaddr,
+			int *uaddr_len, int peer)
+{
+	struct sock *sk		= sock->sk;
+	struct inet_sock *inet	= inet_sk(sk);
+	struct sockaddr_in *sin	= (struct sockaddr_in *)uaddr;
+
+	sin->sin_family = AF_INET;
+	if (peer) {
+		if (!inet->dport ||
+		    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&
+		     peer == 1))
+			return -ENOTCONN;
+		sin->sin_port = inet->dport;
+		sin->sin_addr.s_addr = inet->daddr;
+	} else {
+		__be32 addr = inet->rcv_saddr;
+		if (!addr)
+			addr = inet->saddr;
+		sin->sin_port = inet->sport;
+		sin->sin_addr.s_addr = addr;
+	}
+	memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
+	*uaddr_len = sizeof(*sin);
+	return 0;
+}
+EXPORT_SYMBOL(inet_getname);
+
+int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
+		 size_t size)
+{
+	struct sock *sk = sock->sk;
+
+	inet_rps_record_flow(sk);
+
+	/* We may need to bind the socket. */
+	if (!inet_sk(sk)->num && inet_autobind(sk))
+		return -EAGAIN;
+
+	return sk->sk_prot->sendmsg(iocb, sk, msg, size);
+}
+EXPORT_SYMBOL(inet_sendmsg);
+
+static ssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,
+			     size_t size, int flags)
+{
+	struct sock *sk = sock->sk;
+
+	inet_rps_record_flow(sk);
+
+	/* We may need to bind the socket. */
+	if (!inet_sk(sk)->num && inet_autobind(sk))
+		return -EAGAIN;
+
+	if (sk->sk_prot->sendpage)
+		return sk->sk_prot->sendpage(sk, page, offset, size, flags);
+	return sock_no_sendpage(sock, page, offset, size, flags);
+}
+
+int inet_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
+		 size_t size, int flags)
+{
+	struct sock *sk = sock->sk;
+	int addr_len = 0;
+	int err;
+
+	inet_rps_record_flow(sk);
+
+	err = sk->sk_prot->recvmsg(iocb, sk, msg, size, flags & MSG_DONTWAIT,
+				   flags & ~MSG_DONTWAIT, &addr_len);
+	if (err >= 0)
+		msg->msg_namelen = addr_len;
+	return err;
+}
+EXPORT_SYMBOL(inet_recvmsg);
+
+int inet_shutdown(struct socket *sock, int how)
+{
+	struct sock *sk = sock->sk;
+	int err = 0;
+
+	/* This should really check to make sure
+	 * the socket is a TCP socket. (WHY AC...)
+	 */
+	how++; /* maps 0->1 has the advantage of making bit 1 rcvs and
+		       1->2 bit 2 snds.
+		       2->3 */
+	if ((how & ~SHUTDOWN_MASK) || !how)	/* MAXINT->0 */
+		return -EINVAL;
+
+	lock_sock(sk);
+	if (sock->state == SS_CONNECTING) {
+		if ((1 << sk->sk_state) &
+		    (TCPF_SYN_SENT | TCPF_SYN_RECV | TCPF_CLOSE))
+			sock->state = SS_DISCONNECTING;
+		else
+			sock->state = SS_CONNECTED;
+	}
+
+	switch (sk->sk_state) {
+	case TCP_CLOSE:
+		err = -ENOTCONN;
+		/* Hack to wake up other listeners, who can poll for
+		   POLLHUP, even on eg. unconnected UDP sockets -- RR */
+	default:
+		sk->sk_shutdown |= how;
+		if (sk->sk_prot->shutdown)
+			sk->sk_prot->shutdown(sk, how);
+		break;
+
+	/* Remaining two branches are temporary solution for missing
+	 * close() in multithreaded environment. It is _not_ a good idea,
+	 * but we have no choice until close() is repaired at VFS level.
+	 */
+	case TCP_LISTEN:
+		if (!(how & RCV_SHUTDOWN))
+			break;
+		/* Fall through */
+	case TCP_SYN_SENT:
+		err = sk->sk_prot->disconnect(sk, O_NONBLOCK);
+		sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
+		break;
+	}
+
+	/* Wake up anyone sleeping in poll. */
+	sk->sk_state_change(sk);
+	release_sock(sk);
+	return err;
+}
+EXPORT_SYMBOL(inet_shutdown);
+
+/*
+ *	ioctl() calls you can issue on an INET socket. Most of these are
+ *	device configuration and stuff and very rarely used. Some ioctls
+ *	pass on to the socket itself.
+ *
+ *	NOTE: I like the idea of a module for the config stuff. ie ifconfig
+ *	loads the devconfigure module does its configuring and unloads it.
+ *	There's a good 20K of config code hanging around the kernel.
+ */
+
+int inet_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)
+{
+	struct sock *sk = sock->sk;
+	int err = 0;
+	struct net *net = sock_net(sk);
+
+	switch (cmd) {
+	case SIOCGSTAMP:
+		err = sock_get_timestamp(sk, (struct timeval __user *)arg);
+		break;
+	case SIOCGSTAMPNS:
+		err = sock_get_timestampns(sk, (struct timespec __user *)arg);
+		break;
+	case SIOCADDRT:
+	case SIOCDELRT:
+	case SIOCRTMSG:
+		err = ip_rt_ioctl(net, cmd, (void __user *)arg);
+		break;
+	case SIOCDARP:
+	case SIOCGARP:
+	case SIOCSARP:
+		err = arp_ioctl(net, cmd, (void __user *)arg);
+		break;
+	case SIOCGIFADDR:
+	case SIOCSIFADDR:
+	case SIOCGIFBRDADDR:
+	case SIOCSIFBRDADDR:
+	case SIOCGIFNETMASK:
+	case SIOCSIFNETMASK:
+	case SIOCGIFDSTADDR:
+	case SIOCSIFDSTADDR:
+	case SIOCSIFPFLAGS:
+	case SIOCGIFPFLAGS:
+	case SIOCSIFFLAGS:
+		err = devinet_ioctl(net, cmd, (void __user *)arg);
+		break;
+	default:
+		if (sk->sk_prot->ioctl)
+			err = sk->sk_prot->ioctl(sk, cmd, arg);
+		else
+			err = -ENOIOCTLCMD;
+		break;
+	}
+	return err;
+}
+EXPORT_SYMBOL(inet_ioctl);
+
+const struct proto_ops inet_stream_ops = {
+	.family		   = PF_INET,
+	.owner		   = THIS_MODULE,
+	.release	   = inet_release,
+	.bind		   = inet_bind,
+	.connect	   = inet_stream_connect,
+	.socketpair	   = sock_no_socketpair,
+	.accept		   = inet_accept,
+	.getname	   = inet_getname,
+	.poll		   = tcp_poll,
+	.ioctl		   = inet_ioctl,
+	.listen		   = inet_listen,
+	.shutdown	   = inet_shutdown,
+	.setsockopt	   = sock_common_setsockopt,
+	.getsockopt	   = sock_common_getsockopt,
+	.sendmsg	   = tcp_sendmsg,
+	.recvmsg	   = inet_recvmsg,
+	.mmap		   = sock_no_mmap,
+	.sendpage	   = tcp_sendpage,
+	.splice_read	   = tcp_splice_read,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_sock_common_setsockopt,
+	.compat_getsockopt = compat_sock_common_getsockopt,
+#endif
+};
+EXPORT_SYMBOL(inet_stream_ops);
+
+const struct proto_ops inet_dgram_ops = {
+	.family		   = PF_INET,
+	.owner		   = THIS_MODULE,
+	.release	   = inet_release,
+	.bind		   = inet_bind,
+	.connect	   = inet_dgram_connect,
+	.socketpair	   = sock_no_socketpair,
+	.accept		   = sock_no_accept,
+	.getname	   = inet_getname,
+	.poll		   = udp_poll,
+	.ioctl		   = inet_ioctl,
+	.listen		   = sock_no_listen,
+	.shutdown	   = inet_shutdown,
+	.setsockopt	   = sock_common_setsockopt,
+	.getsockopt	   = sock_common_getsockopt,
+	.sendmsg	   = inet_sendmsg,
+	.recvmsg	   = inet_recvmsg,
+	.mmap		   = sock_no_mmap,
+	.sendpage	   = inet_sendpage,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_sock_common_setsockopt,
+	.compat_getsockopt = compat_sock_common_getsockopt,
+#endif
+};
+EXPORT_SYMBOL(inet_dgram_ops);
+
+/*
+ * For SOCK_RAW sockets; should be the same as inet_dgram_ops but without
+ * udp_poll
+ */
+static const struct proto_ops inet_sockraw_ops = {
+	.family		   = PF_INET,
+	.owner		   = THIS_MODULE,
+	.release	   = inet_release,
+	.bind		   = inet_bind,
+	.connect	   = inet_dgram_connect,
+	.socketpair	   = sock_no_socketpair,
+	.accept		   = sock_no_accept,
+	.getname	   = inet_getname,
+	.poll		   = datagram_poll,
+	.ioctl		   = inet_ioctl,
+	.listen		   = sock_no_listen,
+	.shutdown	   = inet_shutdown,
+	.setsockopt	   = sock_common_setsockopt,
+	.getsockopt	   = sock_common_getsockopt,
+	.sendmsg	   = inet_sendmsg,
+	.recvmsg	   = inet_recvmsg,
+	.mmap		   = sock_no_mmap,
+	.sendpage	   = inet_sendpage,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_sock_common_setsockopt,
+	.compat_getsockopt = compat_sock_common_getsockopt,
+#endif
+};
+
+static struct net_proto_family inet_family_ops = {
+	.family = PF_INET,
+	.create = inet_create,
+	.owner	= THIS_MODULE,
+};
+
+/* Upon startup we insert all the elements in inetsw_array[] into
+ * the linked list inetsw.
+ */
+static struct inet_protosw inetsw_array[] =
+{
+	{
+		.type =       SOCK_STREAM,
+		.protocol =   IPPROTO_TCP,
+		.prot =       &tcp_prot,
+		.ops =        &inet_stream_ops,
+		.no_check =   0,
+		.flags =      INET_PROTOSW_PERMANENT |
+			      INET_PROTOSW_ICSK,
+	},
+
+	{
+		.type =       SOCK_DGRAM,
+		.protocol =   IPPROTO_UDP,
+		.prot =       &udp_prot,
+		.ops =        &inet_dgram_ops,
+		.no_check =   UDP_CSUM_DEFAULT,
+		.flags =      INET_PROTOSW_PERMANENT,
+       },
+
+       {
+		.type =       SOCK_DGRAM,
+		.protocol =   IPPROTO_ICMP,
+		.prot =       &ping_prot,
+		.ops =        &inet_dgram_ops,
+		.no_check =   UDP_CSUM_DEFAULT,
+		.flags =      INET_PROTOSW_REUSE,
+       },
+
+       {
+	       .type =       SOCK_RAW,
+	       .protocol =   IPPROTO_IP,	/* wild card */
+	       .prot =       &raw_prot,
+	       .ops =        &inet_sockraw_ops,
+	       .no_check =   UDP_CSUM_DEFAULT,
+	       .flags =      INET_PROTOSW_REUSE,
+       }
+};
+
+#define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)
+
+void inet_register_protosw(struct inet_protosw *p)
+{
+	struct list_head *lh;
+	struct inet_protosw *answer;
+	int protocol = p->protocol;
+	struct list_head *last_perm;
+
+	spin_lock_bh(&inetsw_lock);
+
+	if (p->type >= SOCK_MAX)
+		goto out_illegal;
+
+	/* If we are trying to override a permanent protocol, bail. */
+	answer = NULL;
+	last_perm = &inetsw[p->type];
+	list_for_each(lh, &inetsw[p->type]) {
+		answer = list_entry(lh, struct inet_protosw, list);
+
+		/* Check only the non-wild match. */
+		if (INET_PROTOSW_PERMANENT & answer->flags) {
+			if (protocol == answer->protocol)
+				break;
+			last_perm = lh;
+		}
+
+		answer = NULL;
+	}
+	if (answer)
+		goto out_permanent;
+
+	/* Add the new entry after the last permanent entry if any, so that
+	 * the new entry does not override a permanent entry when matched with
+	 * a wild-card protocol. But it is allowed to override any existing
+	 * non-permanent entry.  This means that when we remove this entry, the
+	 * system automatically returns to the old behavior.
+	 */
+	list_add_rcu(&p->list, last_perm);
+out:
+	spin_unlock_bh(&inetsw_lock);
+
+	return;
+
+out_permanent:
+	printk(KERN_ERR "Attempt to override permanent protocol %d.\n",
+	       protocol);
+	goto out;
+
+out_illegal:
+	printk(KERN_ERR
+	       "Ignoring attempt to register invalid socket type %d.\n",
+	       p->type);
+	goto out;
+}
+EXPORT_SYMBOL(inet_register_protosw);
+
+void inet_unregister_protosw(struct inet_protosw *p)
+{
+	if (INET_PROTOSW_PERMANENT & p->flags) {
+		printk(KERN_ERR
+		       "Attempt to unregister permanent protocol %d.\n",
+		       p->protocol);
+	} else {
+		spin_lock_bh(&inetsw_lock);
+		list_del_rcu(&p->list);
+		spin_unlock_bh(&inetsw_lock);
+
+		synchronize_net();
+	}
+}
+EXPORT_SYMBOL(inet_unregister_protosw);
+
+/*
+ *      Shall we try to damage output packets if routing dev changes?
+ */
+
+int sysctl_ip_dynaddr __read_mostly;
+
+static int inet_sk_reselect_saddr(struct sock *sk)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	int err;
+	struct rtable *rt;
+	__be32 old_saddr = inet->saddr;
+	__be32 new_saddr;
+	__be32 daddr = inet->daddr;
+	struct ip_options *inet_opt;
+
+	inet_opt = rcu_dereference(inet->opt);
+	if (inet_opt && inet_opt->srr)
+		daddr = inet_opt->faddr;
+
+	/* Query new route. */
+	err = ip_route_connect(&rt, daddr, 0,
+			       RT_CONN_FLAGS(sk),
+			       sk->sk_bound_dev_if,
+			       sk->sk_protocol,
+			       inet->sport, inet->dport, sk, 0);
+	if (err)
+		return err;
+
+	sk_setup_caps(sk, &rt->u.dst);
+
+	new_saddr = rt->rt_src;
+
+	if (new_saddr == old_saddr)
+		return 0;
+
+	if (sysctl_ip_dynaddr > 1) {
+		printk(KERN_INFO "%s(): shifting inet->saddr from %pI4 to %pI4\n",
+		       __func__, &old_saddr, &new_saddr);
+	}
+
+	inet->saddr = inet->rcv_saddr = new_saddr;
+
+	/*
+	 * XXX The only one ugly spot where we need to
+	 * XXX really change the sockets identity after
+	 * XXX it has entered the hashes. -DaveM
+	 *
+	 * Besides that, it does not check for connection
+	 * uniqueness. Wait for troubles.
+	 */
+	__sk_prot_rehash(sk);
+	return 0;
+}
+
+int inet_sk_rebuild_header(struct sock *sk)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	struct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);
+	__be32 daddr;
+	struct ip_options *inet_opt;
+	int err;
+
+	/* Route is OK, nothing to do. */
+	if (rt)
+		return 0;
+
+	/* Reroute. */
+	rcu_read_lock();
+	inet_opt = rcu_dereference(inet->opt);
+	daddr = inet->daddr;
+	if (inet_opt && inet_opt->srr)
+		daddr = inet_opt->faddr;
+	rcu_read_unlock();
+{
+	struct flowi fl = {
+		.oif = sk->sk_bound_dev_if,
+		.mark = sk->sk_mark,
+		.nl_u = {
+			.ip4_u = {
+				.daddr	= daddr,
+				.saddr	= inet->saddr,
+				.tos	= RT_CONN_FLAGS(sk),
+			},
+		},
+		.proto = sk->sk_protocol,
+		.flags = inet_sk_flowi_flags(sk),
+		.uli_u = {
+			.ports = {
+				.sport = inet->sport,
+				.dport = inet->dport,
+			},
+		},
+	};
+
+	security_sk_classify_flow(sk, &fl);
+	err = ip_route_output_flow(sock_net(sk), &rt, &fl, sk, 0);
+}
+	if (!err)
+		sk_setup_caps(sk, &rt->u.dst);
+	else {
+		/* Routing failed... */
+		sk->sk_route_caps = 0;
+		/*
+		 * Other protocols have to map its equivalent state to TCP_SYN_SENT.
+		 * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme
+		 */
+		if (!sysctl_ip_dynaddr ||
+		    sk->sk_state != TCP_SYN_SENT ||
+		    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||
+		    (err = inet_sk_reselect_saddr(sk)) != 0)
+			sk->sk_err_soft = -err;
+	}
+
+	return err;
+}
+EXPORT_SYMBOL(inet_sk_rebuild_header);
+
+static int inet_gso_send_check(struct sk_buff *skb)
+{
+	struct iphdr *iph;
+	const struct net_offload *ops;
+	const struct net_protocol *proto_ops;
+	int proto;
+	int ihl;
+	int err = -EINVAL;
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(*iph))))
+		goto out;
+
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+	if (ihl < sizeof(*iph))
+		goto out;
+
+	if (unlikely(!pskb_may_pull(skb, ihl)))
+		goto out;
+
+	__skb_pull(skb, ihl);
+	skb_reset_transport_header(skb);
+	iph = ip_hdr(skb);
+	proto = iph->protocol & (MAX_INET_PROTOS - 1);
+	err = -EPROTONOSUPPORT;
+
+	rcu_read_lock();
+	ops = rcu_dereference(inet_offloads[proto]);
+	if (likely(ops && ops->gso_send_check))
+		err = ops->gso_send_check(skb);
+	else {
+		/* Check protocols array for any protocols
+		 * using old API.
+		 */
+		proto_ops = rcu_dereference(inet_protos[proto]);
+		if (proto_ops && proto_ops->gso_send_check)
+			err = proto_ops->gso_send_check(skb);
+	}
+
+	rcu_read_unlock();
+
+out:
+	return err;
+}
+
+static struct sk_buff *inet_gso_segment(struct sk_buff *skb, int features)
+{
+	struct sk_buff *segs = ERR_PTR(-EINVAL);
+	struct iphdr *iph;
+	const struct net_protocol *proto_ops;
+	const struct net_offload *ops;
+	int proto;
+	int ihl;
+	int id;
+	unsigned int offset = 0;
+	bool tunnel;
+
+	if (unlikely(skb_shinfo(skb)->gso_type &
+		     ~(SKB_GSO_TCPV4 |
+		       SKB_GSO_UDP |
+		       SKB_GSO_DODGY |
+		       SKB_GSO_TCP_ECN |
+		       SKB_GSO_GRE |
+		       SKB_GSO_TCPV6 |
+		       SKB_GSO_UDP_TUNNEL |
+		       0)))
+		goto out;
+
+	skb_reset_network_header(skb);
+	if (unlikely(!pskb_may_pull(skb, sizeof(*iph))))
+		goto out;
+
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+	if (ihl < sizeof(*iph))
+		goto out;
+
+	if (unlikely(!pskb_may_pull(skb, ihl)))
+		goto out;
+
+	tunnel = !!skb->encapsulation;
+
+	__skb_pull(skb, ihl);
+	skb_reset_transport_header(skb);
+	iph = ip_hdr(skb);
+	id = ntohs(iph->id);
+	proto = iph->protocol & (MAX_INET_PROTOS - 1);
+	segs = ERR_PTR(-EPROTONOSUPPORT);
+
+	rcu_read_lock();
+	ops = rcu_dereference(inet_offloads[proto]);
+	if (likely(ops && ops->gso_segment))
+		segs = ops->gso_segment(skb, features);
+	else {
+		/* Check protocols array for any protocols
+		 * using old API.
+		 */
+		proto_ops = rcu_dereference(inet_protos[proto]);
+		if (proto_ops && proto_ops->gso_segment)
+			segs = proto_ops->gso_segment(skb, features);
+	}
+
+	rcu_read_unlock();
+
+	if (!segs || IS_ERR(segs))
+		goto out;
+
+	skb = segs;
+	do {
+		iph = ip_hdr(skb);
+		if (!tunnel && proto == IPPROTO_UDP) {
+			iph->id = htons(id);
+			iph->frag_off = htons(offset >> 3);
+			if (skb->next != NULL)
+				iph->frag_off |= htons(IP_MF);
+			offset += (skb->len - skb->mac_len - iph->ihl * 4);
+		} else  {
+			iph->id = htons(id++);
+		}
+		iph->tot_len = htons(skb->len - skb->mac_len);
+		iph->check = 0;
+		iph->check = ip_fast_csum(skb_network_header(skb), iph->ihl);
+	} while ((skb = skb->next));
+
+out:
+	return segs;
+}
+
+static struct sk_buff **inet_gro_receive(struct sk_buff **head,
+					 struct sk_buff *skb)
+{
+	const struct net_offload *ops;
+	const struct net_protocol *proto_ops = NULL;
+	struct sk_buff **pp = NULL;
+	struct sk_buff *p;
+	struct iphdr *iph;
+	unsigned int hlen;
+	unsigned int off;
+	unsigned int id;
+	int flush = 1;
+	int proto;
+
+	off = skb_gro_offset(skb);
+	hlen = off + sizeof(*iph);
+	iph = skb_gro_header_fast(skb, off);
+	if (skb_gro_header_hard(skb, hlen)) {
+		iph = skb_gro_header_slow(skb, hlen, off);
+		if (unlikely(!iph))
+			goto out;
+	}
+
+	proto = iph->protocol & (MAX_INET_PROTOS - 1);
+
+	rcu_read_lock();
+	ops = rcu_dereference(inet_offloads[proto]);
+	if (!ops || !ops->gro_receive) {
+		/* Try the protocols array */
+		proto_ops = rcu_dereference(inet_protos[proto]);
+		if (!proto_ops || !proto_ops->gro_receive)
+			goto out_unlock;
+
+		ops = NULL;
+	}
+
+	if (*(u8 *)iph != 0x45)
+		goto out_unlock;
+
+	if (unlikely(ip_fast_csum((u8 *)iph, iph->ihl)))
+		goto out_unlock;
+
+	id = ntohl(*(u32 *)&iph->id);
+	flush = (u16)((ntohl(*(u32 *)iph) ^ skb_gro_len(skb)) | (id ^ IP_DF));
+	id >>= 16;
+
+	for (p = *head; p; p = p->next) {
+		struct iphdr *iph2;
+
+		if (!NAPI_GRO_CB(p)->same_flow)
+			continue;
+
+		iph2 = ip_hdr(p);
+
+		if ((iph->protocol ^ iph2->protocol) |
+		    (iph->tos ^ iph2->tos) |
+		    (iph->saddr ^ iph2->saddr) |
+		    (iph->daddr ^ iph2->daddr)) {
+			NAPI_GRO_CB(p)->same_flow = 0;
+			continue;
+		}
+
+		/* All fields must match except length and checksum. */
+		NAPI_GRO_CB(p)->flush |=
+			(iph->ttl ^ iph2->ttl) |
+			((u16)(ntohs(iph2->id) + NAPI_GRO_CB(p)->count) ^ id);
+
+		NAPI_GRO_CB(p)->flush |= flush;
+	}
+
+	NAPI_GRO_CB(skb)->flush |= flush;
+	skb_gro_pull(skb, sizeof(*iph));
+	skb_set_transport_header(skb, skb_gro_offset(skb));
+
+	if (ops)
+		pp = ops->gro_receive(head, skb);
+	else
+		pp = proto_ops->gro_receive(head, skb);
+
+out_unlock:
+	rcu_read_unlock();
+
+out:
+	NAPI_GRO_CB(skb)->flush |= flush;
+
+	return pp;
+}
+
+static int inet_gro_complete(struct sk_buff *skb)
+{
+	const struct net_protocol *proto_ops;
+	const struct net_offload *ops;
+	struct iphdr *iph = ip_hdr(skb);
+	int proto = iph->protocol & (MAX_INET_PROTOS - 1);
+	int err = -ENOSYS;
+	__be16 newlen = htons(skb->len - skb_network_offset(skb));
+
+	csum_replace2(&iph->check, iph->tot_len, newlen);
+	iph->tot_len = newlen;
+
+	rcu_read_lock();
+	ops = rcu_dereference(inet_offloads[proto]);
+	if (unlikely(!ops || !ops->gro_complete)) {
+		/* Check the old protocol array */
+		proto_ops = rcu_dereference(inet_protos[proto]);
+		if (!proto_ops || !proto_ops->gro_complete) {
+			WARN_ON(true);
+			goto out_unlock;
+		}
+		err = proto_ops->gro_complete(skb);
+	} else
+		err = ops->gro_complete(skb);
+
+out_unlock:
+	rcu_read_unlock();
+
+	return err;
+}
+
+int inet_ctl_sock_create(struct sock **sk, unsigned short family,
+			 unsigned short type, unsigned char protocol,
+			 struct net *net)
+{
+	struct socket *sock;
+	int rc = sock_create_kern(family, type, protocol, &sock);
+
+	if (rc == 0) {
+		*sk = sock->sk;
+		(*sk)->sk_allocation = GFP_ATOMIC;
+		/*
+		 * Unhash it so that IP input processing does not even see it,
+		 * we do not wish this socket to see incoming packets.
+		 */
+		(*sk)->sk_prot->unhash(*sk);
+
+		sk_change_net(*sk, net);
+	}
+	return rc;
+}
+EXPORT_SYMBOL_GPL(inet_ctl_sock_create);
+
+unsigned long snmp_fold_field(void *mib[], int offt)
+{
+	unsigned long res = 0;
+	int i;
+
+	for_each_possible_cpu(i) {
+		res += *(((unsigned long *) per_cpu_ptr(mib[0], i)) + offt);
+		res += *(((unsigned long *) per_cpu_ptr(mib[1], i)) + offt);
+	}
+	return res;
+}
+EXPORT_SYMBOL_GPL(snmp_fold_field);
+
+int snmp_mib_init(void *ptr[2], size_t mibsize)
+{
+	BUG_ON(ptr == NULL);
+	ptr[0] = __alloc_percpu(mibsize, __alignof__(unsigned long long));
+	if (!ptr[0])
+		goto err0;
+	ptr[1] = __alloc_percpu(mibsize, __alignof__(unsigned long long));
+	if (!ptr[1])
+		goto err1;
+	return 0;
+err1:
+	free_percpu(ptr[0]);
+	ptr[0] = NULL;
+err0:
+	return -ENOMEM;
+}
+EXPORT_SYMBOL_GPL(snmp_mib_init);
+
+void snmp_mib_free(void *ptr[2])
+{
+	BUG_ON(ptr == NULL);
+	free_percpu(ptr[0]);
+	free_percpu(ptr[1]);
+	ptr[0] = ptr[1] = NULL;
+}
+EXPORT_SYMBOL_GPL(snmp_mib_free);
+
+#ifdef CONFIG_IP_MULTICAST
+static const struct net_protocol igmp_protocol = {
+	.handler =	igmp_rcv,
+	.netns_ok =	1,
+};
+#endif
+
+static const struct net_protocol tcp_protocol = {
+	.handler =	tcp_v4_rcv,
+	.err_handler =	tcp_v4_err,
+	.no_policy =	1,
+	.netns_ok =	1,
+};
+
+static const struct net_offload tcp_offload = {
+	.gso_send_check	=	tcp_v4_gso_send_check,
+	.gso_segment	=	tcp_tso_segment,
+	.gro_receive	=	tcp4_gro_receive,
+	.gro_complete	=	tcp4_gro_complete,
+};
+
+static const struct net_protocol udp_protocol = {
+	.handler =	udp_rcv,
+	.err_handler =	udp_err,
+	.no_policy =	1,
+	.netns_ok =	1,
+};
+
+static const struct net_offload udp_offload = {
+	.gso_send_check = udp4_ufo_send_check,
+	.gso_segment = udp4_ufo_fragment,
+};
+
+static const struct net_protocol icmp_protocol = {
+	.handler =	icmp_rcv,
+	.err_handler =	ping_err,
+	.no_policy =	1,
+	.netns_ok =	1,
+};
+
+static __net_init int ipv4_mib_init_net(struct net *net)
+{
+	if (snmp_mib_init((void **)net->mib.tcp_statistics,
+			  sizeof(struct tcp_mib)) < 0)
+		goto err_tcp_mib;
+	if (snmp_mib_init((void **)net->mib.ip_statistics,
+			  sizeof(struct ipstats_mib)) < 0)
+		goto err_ip_mib;
+	if (snmp_mib_init((void **)net->mib.net_statistics,
+			  sizeof(struct linux_mib)) < 0)
+		goto err_net_mib;
+	if (snmp_mib_init((void **)net->mib.udp_statistics,
+			  sizeof(struct udp_mib)) < 0)
+		goto err_udp_mib;
+	if (snmp_mib_init((void **)net->mib.udplite_statistics,
+			  sizeof(struct udp_mib)) < 0)
+		goto err_udplite_mib;
+	if (snmp_mib_init((void **)net->mib.icmp_statistics,
+			  sizeof(struct icmp_mib)) < 0)
+		goto err_icmp_mib;
+	if (snmp_mib_init((void **)net->mib.icmpmsg_statistics,
+			  sizeof(struct icmpmsg_mib)) < 0)
+		goto err_icmpmsg_mib;
+
+	tcp_mib_init(net);
+	return 0;
+
+err_icmpmsg_mib:
+	snmp_mib_free((void **)net->mib.icmp_statistics);
+err_icmp_mib:
+	snmp_mib_free((void **)net->mib.udplite_statistics);
+err_udplite_mib:
+	snmp_mib_free((void **)net->mib.udp_statistics);
+err_udp_mib:
+	snmp_mib_free((void **)net->mib.net_statistics);
+err_net_mib:
+	snmp_mib_free((void **)net->mib.ip_statistics);
+err_ip_mib:
+	snmp_mib_free((void **)net->mib.tcp_statistics);
+err_tcp_mib:
+	return -ENOMEM;
+}
+
+static __net_exit void ipv4_mib_exit_net(struct net *net)
+{
+	snmp_mib_free((void **)net->mib.icmpmsg_statistics);
+	snmp_mib_free((void **)net->mib.icmp_statistics);
+	snmp_mib_free((void **)net->mib.udplite_statistics);
+	snmp_mib_free((void **)net->mib.udp_statistics);
+	snmp_mib_free((void **)net->mib.net_statistics);
+	snmp_mib_free((void **)net->mib.ip_statistics);
+	snmp_mib_free((void **)net->mib.tcp_statistics);
+}
+
+static __net_initdata struct pernet_operations ipv4_mib_ops = {
+	.init = ipv4_mib_init_net,
+	.exit = ipv4_mib_exit_net,
+};
+
+static int __init init_ipv4_mibs(void)
+{
+	return register_pernet_subsys(&ipv4_mib_ops);
+}
+
+static int ipv4_proc_init(void);
+
+/*
+ *	IP protocol layer initialiser
+ */
+
+static struct packet_offload ip_packet_offload __read_mostly = {
+	.type = cpu_to_be16(ETH_P_IP),
+	.gso_send_check = inet_gso_send_check,
+	.gso_segment = inet_gso_segment,
+	.gro_receive = inet_gro_receive,
+	.gro_complete = inet_gro_complete,
+};
+
+static int __init ipv4_offload_init(void)
+{
+	/*
+	 * Add offloads
+	 */
+	if (inet_add_offload(&udp_offload, IPPROTO_UDP) < 0)
+		pr_crit("%s: Cannot add UDP protocol offload\n", __func__);
+	if (inet_add_offload(&tcp_offload, IPPROTO_TCP) < 0)
+		pr_crit("%s: Cannot add TCP protocol offlaod\n", __func__);
+
+	dev_add_offload(&ip_packet_offload);
+	return 0;
+}
+
+fs_initcall(ipv4_offload_init);
+
+static struct packet_type ip_packet_type __read_mostly = {
+	.type = cpu_to_be16(ETH_P_IP),
+	.func = ip_rcv,
+};
+
+static int __init inet_init(void)
+{
+	struct sk_buff *dummy_skb;
+	struct inet_protosw *q;
+	struct list_head *r;
+	int rc = -EINVAL;
+
+	BUILD_BUG_ON(sizeof(struct inet_skb_parm) > sizeof(dummy_skb->cb));
+
+	sysctl_local_reserved_ports = kzalloc(65536 / 8, GFP_KERNEL);
+	if (!sysctl_local_reserved_ports)
+		goto out;
+
+	rc = proto_register(&tcp_prot, 1);
+	if (rc)
+		goto out_free_reserved_ports;
+
+	rc = proto_register(&udp_prot, 1);
+	if (rc)
+		goto out_unregister_tcp_proto;
+
+	rc = proto_register(&raw_prot, 1);
+	if (rc)
+		goto out_unregister_udp_proto;
+
+	rc = proto_register(&ping_prot, 1);
+	if (rc)
+		goto out_unregister_raw_proto;
+
+	/*
+	 *	Tell SOCKET that we are alive...
+	 */
+
+	(void)sock_register(&inet_family_ops);
+
+#ifdef CONFIG_SYSCTL
+	ip_static_sysctl_init();
+#endif
+
+	/*
+	 *	Add all the base protocols.
+	 */
+
+	if (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)
+		printk(KERN_CRIT "inet_init: Cannot add ICMP protocol\n");
+	if (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)
+		printk(KERN_CRIT "inet_init: Cannot add UDP protocol\n");
+	if (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)
+		printk(KERN_CRIT "inet_init: Cannot add TCP protocol\n");
+#ifdef CONFIG_IP_MULTICAST
+	if (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)
+		printk(KERN_CRIT "inet_init: Cannot add IGMP protocol\n");
+#endif
+
+	/* Register the socket-side information for inet_create. */
+	for (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)
+		INIT_LIST_HEAD(r);
+
+	for (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)
+		inet_register_protosw(q);
+
+	/*
+	 *	Set the ARP module up
+	 */
+
+	arp_init();
+
+	/*
+	 *	Set the IP module up
+	 */
+
+	ip_init();
+
+	tcp_v4_init();
+
+	/* Setup TCP slab cache for open requests. */
+	tcp_init();
+
+	/* Setup UDP memory threshold */
+	udp_init();
+
+	/* Add UDP-Lite (RFC 3828) */
+	udplite4_register();
+
+	ping_init();
+
+	/*
+	 *	Set the ICMP layer up
+	 */
+
+	if (icmp_init() < 0)
+		panic("Failed to create the ICMP control socket.\n");
+
+	/*
+	 *	Initialise the multicast router
+	 */
+#if defined(CONFIG_IP_MROUTE)
+	if (ip_mr_init())
+		printk(KERN_CRIT "inet_init: Cannot init ipv4 mroute\n");
+#endif
+	/*
+	 *	Initialise per-cpu ipv4 mibs
+	 */
+
+	if (init_ipv4_mibs())
+		printk(KERN_CRIT "inet_init: Cannot init ipv4 mibs\n");
+
+	ipv4_proc_init();
+
+	ipfrag_init();
+
+	dev_add_pack(&ip_packet_type);
+
+	rc = 0;
+out:
+	return rc;
+out_unregister_raw_proto:
+	proto_unregister(&raw_prot);
+out_unregister_udp_proto:
+	proto_unregister(&udp_prot);
+out_unregister_tcp_proto:
+	proto_unregister(&tcp_prot);
+out_free_reserved_ports:
+	kfree(sysctl_local_reserved_ports);
+	goto out;
+}
+
+fs_initcall(inet_init);
+
+/* ------------------------------------------------------------------------ */
+
+#ifdef CONFIG_PROC_FS
+static int __init ipv4_proc_init(void)
+{
+	int rc = 0;
+
+	if (raw_proc_init())
+		goto out_raw;
+	if (tcp4_proc_init())
+		goto out_tcp;
+	if (udp4_proc_init())
+		goto out_udp;
+	if (ping_proc_init())
+		goto out_ping;
+	if (ip_misc_proc_init())
+		goto out_misc;
+out:
+	return rc;
+out_misc:
+	ping_proc_exit();
+out_ping:
+	udp4_proc_exit();
+out_udp:
+	tcp4_proc_exit();
+out_tcp:
+	raw_proc_exit();
+out_raw:
+	rc = -ENOMEM;
+	goto out;
+}
+
+#else /* CONFIG_PROC_FS */
+static int __init ipv4_proc_init(void)
+{
+	return 0;
+}
+#endif /* CONFIG_PROC_FS */
+
+MODULE_ALIAS_NETPROTO(PF_INET);
+
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/tcp_ipv4.c linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/tcp_ipv4.c
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/tcp_ipv4.c	2016-12-13 17:21:59.504074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/tcp_ipv4.c	2016-12-13 17:25:15.713067817 +0800
@@ -1800,7 +1800,7 @@
 	return 0;
 }
 
-const struct inet_connection_sock_af_ops ipv4_specific = {
+struct inet_connection_sock_af_ops ipv4_specific = {
 	.queue_xmit	   = ip_queue_xmit,
 	.send_check	   = tcp_v4_send_check,
 	.rebuild_header	   = inet_sk_rebuild_header,
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/tcp_ipv4.c.orig linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/tcp_ipv4.c.orig
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv4/tcp_ipv4.c.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv4/tcp_ipv4.c.orig	2016-12-13 17:23:54.013074270 +0800
@@ -0,0 +1,2557 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Implementation of the Transmission Control Protocol(TCP).
+ *
+ *		IPv4 specific functions
+ *
+ *
+ *		code split from:
+ *		linux/ipv4/tcp.c
+ *		linux/ipv4/tcp_input.c
+ *		linux/ipv4/tcp_output.c
+ *
+ *		See tcp.c for author information
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+/*
+ * Changes:
+ *		David S. Miller	:	New socket lookup architecture.
+ *					This code is dedicated to John Dyson.
+ *		David S. Miller :	Change semantics of established hash,
+ *					half is devoted to TIME_WAIT sockets
+ *					and the rest go in the other half.
+ *		Andi Kleen :		Add support for syncookies and fixed
+ *					some bugs: ip options weren't passed to
+ *					the TCP layer, missed a check for an
+ *					ACK bit.
+ *		Andi Kleen :		Implemented fast path mtu discovery.
+ *	     				Fixed many serious bugs in the
+ *					request_sock handling and moved
+ *					most of it into the af independent code.
+ *					Added tail drop and some other bugfixes.
+ *					Added new listen semantics.
+ *		Mike McLagan	:	Routing by source
+ *	Juan Jose Ciarlante:		ip_dynaddr bits
+ *		Andi Kleen:		various fixes.
+ *	Vitaly E. Lavrov	:	Transparent proxy revived after year
+ *					coma.
+ *	Andi Kleen		:	Fix new listen.
+ *	Andi Kleen		:	Fix accept error reporting.
+ *	YOSHIFUJI Hideaki @USAGI and:	Support IPV6_V6ONLY socket option, which
+ *	Alexey Kuznetsov		allow both IPv4 and IPv6 sockets to bind
+ *					a single port at the same time.
+ */
+
+
+#include <linux/bottom_half.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/cache.h>
+#include <linux/jhash.h>
+#include <linux/init.h>
+#include <linux/times.h>
+
+#include <net/net_namespace.h>
+#include <net/icmp.h>
+#include <net/inet_hashtables.h>
+#include <net/tcp.h>
+#include <net/transp_v6.h>
+#include <net/ipv6.h>
+#include <net/inet_common.h>
+#include <net/timewait_sock.h>
+#include <net/xfrm.h>
+#include <net/netdma.h>
+#include <net/secure_seq.h>
+#include <net/busy_poll.h>
+
+#include <linux/inet.h>
+#include <linux/ipv6.h>
+#include <linux/stddef.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <linux/crypto.h>
+#include <linux/scatterlist.h>
+
+int sysctl_tcp_tw_reuse __read_mostly;
+int sysctl_tcp_low_latency __read_mostly;
+
+
+#ifdef CONFIG_TCP_MD5SIG
+static struct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk,
+						   __be32 addr);
+static int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,
+			       __be32 daddr, __be32 saddr, struct tcphdr *th);
+#else
+static inline
+struct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)
+{
+	return NULL;
+}
+#endif
+
+struct inet_hashinfo tcp_hashinfo;
+
+static inline __u32 tcp_v4_init_sequence(struct sk_buff *skb)
+{
+	return secure_tcp_sequence_number(ip_hdr(skb)->daddr,
+					  ip_hdr(skb)->saddr,
+					  tcp_hdr(skb)->dest,
+					  tcp_hdr(skb)->source);
+}
+
+int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)
+{
+	const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	/* With PAWS, it is safe from the viewpoint
+	   of data integrity. Even without PAWS it is safe provided sequence
+	   spaces do not overlap i.e. at data rates <= 80Mbit/sec.
+
+	   Actually, the idea is close to VJ's one, only timestamp cache is
+	   held not per host, but per port pair and TW bucket is used as state
+	   holder.
+
+	   If TW bucket has been already destroyed we fall back to VJ's scheme
+	   and use initial timestamp retrieved from peer table.
+	 */
+	if (tcptw->tw_ts_recent_stamp &&
+	    (twp == NULL || (sysctl_tcp_tw_reuse &&
+			     get_seconds() - tcptw->tw_ts_recent_stamp > 1))) {
+		tp->write_seq = tcptw->tw_snd_nxt + 65535 + 2;
+		if (tp->write_seq == 0)
+			tp->write_seq = 1;
+		tp->rx_opt.ts_recent	   = tcptw->tw_ts_recent;
+		tp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;
+		sock_hold(sktw);
+		return 1;
+	}
+
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(tcp_twsk_unique);
+
+/* This will initiate an outgoing connection. */
+int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;
+	struct rtable *rt;
+	__be32 daddr, nexthop;
+	int tmp;
+	int err;
+	struct ip_options *inet_opt;
+
+	if (addr_len < sizeof(struct sockaddr_in))
+		return -EINVAL;
+
+	if (usin->sin_family != AF_INET)
+		return -EAFNOSUPPORT;
+
+	nexthop = daddr = usin->sin_addr.s_addr;
+	inet_opt = rcu_dereference(inet->opt);
+	if (inet_opt && inet_opt->srr) {
+		if (!daddr)
+			return -EINVAL;
+		nexthop = inet_opt->faddr;
+	}
+
+	tmp = ip_route_connect(&rt, nexthop, inet->saddr,
+			       RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,
+			       IPPROTO_TCP,
+			       inet->sport, usin->sin_port, sk, 1);
+	if (tmp < 0) {
+		if (tmp == -ENETUNREACH)
+			IP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);
+		return tmp;
+	}
+
+	if (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {
+		ip_rt_put(rt);
+		return -ENETUNREACH;
+	}
+
+	if (!inet_opt || !inet_opt->srr)
+		daddr = rt->rt_dst;
+
+	if (!inet->saddr)
+		inet->saddr = rt->rt_src;
+	inet->rcv_saddr = inet->saddr;
+
+	if (tp->rx_opt.ts_recent_stamp && inet->daddr != daddr) {
+		/* Reset inherited state */
+		tp->rx_opt.ts_recent	   = 0;
+		tp->rx_opt.ts_recent_stamp = 0;
+		tp->write_seq		   = 0;
+	}
+
+	if (tcp_death_row.sysctl_tw_recycle &&
+	    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {
+		struct inet_peer *peer = rt_get_peer(rt);
+		/*
+		 * VJ's idea. We save last timestamp seen from
+		 * the destination in peer table, when entering state
+		 * TIME-WAIT * and initialize rx_opt.ts_recent from it,
+		 * when trying new connection.
+		 */
+		if (peer != NULL &&
+		    peer->tcp_ts_stamp + TCP_PAWS_MSL >= get_seconds()) {
+			tp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;
+			tp->rx_opt.ts_recent = peer->tcp_ts;
+		}
+	}
+
+	inet->dport = usin->sin_port;
+	inet->daddr = daddr;
+
+	inet_csk(sk)->icsk_ext_hdr_len = 0;
+	if (inet_opt)
+		inet_csk(sk)->icsk_ext_hdr_len = inet_opt->optlen;
+
+	tp->rx_opt.mss_clamp = 536;
+
+	/* Socket identity is still unknown (sport may be zero).
+	 * However we set state to SYN-SENT and not releasing socket
+	 * lock select source port, enter ourselves into the hash tables and
+	 * complete initialization after this.
+	 */
+	tcp_set_state(sk, TCP_SYN_SENT);
+	err = inet_hash_connect(&tcp_death_row, sk);
+	if (err)
+		goto failure;
+
+	err = ip_route_newports(&rt, IPPROTO_TCP,
+				inet->sport, inet->dport, sk);
+	if (err)
+		goto failure;
+
+	/* OK, now commit destination to socket.  */
+	sk->sk_gso_type = SKB_GSO_TCPV4;
+	sk_setup_caps(sk, &rt->u.dst);
+
+	if (!tp->write_seq)
+		tp->write_seq = secure_tcp_sequence_number(inet->saddr,
+							   inet->daddr,
+							   inet->sport,
+							   usin->sin_port);
+
+	inet->id = tp->write_seq ^ jiffies;
+
+	err = tcp_connect(sk);
+	rt = NULL;
+	if (err)
+		goto failure;
+
+	return 0;
+
+failure:
+	/*
+	 * This unhashes the socket and releases the local port,
+	 * if necessary.
+	 */
+	tcp_set_state(sk, TCP_CLOSE);
+	ip_rt_put(rt);
+	sk->sk_route_caps = 0;
+	inet->dport = 0;
+	return err;
+}
+
+/*
+ * This routine does path mtu discovery as defined in RFC1191.
+ */
+static void do_pmtu_discovery(struct sock *sk, struct iphdr *iph, u32 mtu)
+{
+	struct dst_entry *dst;
+	struct inet_sock *inet = inet_sk(sk);
+
+	/* We are not interested in TCP_LISTEN and open_requests (SYN-ACKs
+	 * send out by Linux are always <576bytes so they should go through
+	 * unfragmented).
+	 */
+	if (sk->sk_state == TCP_LISTEN)
+		return;
+
+	/* We don't check in the destentry if pmtu discovery is forbidden
+	 * on this route. We just assume that no packet_to_big packets
+	 * are send back when pmtu discovery is not active.
+	 * There is a small race when the user changes this flag in the
+	 * route, but I think that's acceptable.
+	 */
+	if ((dst = __sk_dst_check(sk, 0)) == NULL)
+		return;
+
+	dst->ops->update_pmtu(dst, mtu);
+
+	/* Something is about to be wrong... Remember soft error
+	 * for the case, if this connection will not able to recover.
+	 */
+	if (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))
+		sk->sk_err_soft = EMSGSIZE;
+
+	mtu = dst_mtu(dst);
+
+	if (inet->pmtudisc != IP_PMTUDISC_DONT &&
+	    inet_csk(sk)->icsk_pmtu_cookie > mtu) {
+		tcp_sync_mss(sk, mtu);
+
+		/* Resend the TCP packet because it's
+		 * clear that the old packet has been
+		 * dropped. This is the new "fast" path mtu
+		 * discovery.
+		 */
+		tcp_simple_retransmit(sk);
+	} /* else let the usual retransmit timer handle it */
+}
+
+/*
+ * This routine is called by the ICMP module when it gets some
+ * sort of error condition.  If err < 0 then the socket should
+ * be closed and the error returned to the user.  If err > 0
+ * it's just the icmp type << 8 | icmp code.  After adjustment
+ * header points to the first 8 bytes of the tcp header.  We need
+ * to find the appropriate port.
+ *
+ * The locking strategy used here is very "optimistic". When
+ * someone else accesses the socket the ICMP is just dropped
+ * and for some paths there is no check at all.
+ * A more general error queue to queue errors for later handling
+ * is probably better.
+ *
+ */
+
+void tcp_v4_err(struct sk_buff *icmp_skb, u32 info)
+{
+	struct iphdr *iph = (struct iphdr *)icmp_skb->data;
+	struct tcphdr *th = (struct tcphdr *)(icmp_skb->data + (iph->ihl << 2));
+	struct inet_connection_sock *icsk;
+	struct tcp_sock *tp;
+	struct inet_sock *inet;
+	const int type = icmp_hdr(icmp_skb)->type;
+	const int code = icmp_hdr(icmp_skb)->code;
+	struct sock *sk;
+	struct sk_buff *skb;
+	__u32 seq;
+	__u32 remaining;
+	int err;
+	struct net *net = dev_net(icmp_skb->dev);
+
+	if (icmp_skb->len < (iph->ihl << 2) + 8) {
+		ICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);
+		return;
+	}
+
+	sk = inet_lookup(net, &tcp_hashinfo, iph->daddr, th->dest,
+			iph->saddr, th->source, inet_iif(icmp_skb));
+	if (!sk) {
+		ICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);
+		return;
+	}
+	if (sk->sk_state == TCP_TIME_WAIT) {
+		inet_twsk_put(inet_twsk(sk));
+		return;
+	}
+
+	bh_lock_sock(sk);
+	/* If too many ICMPs get dropped on busy
+	 * servers this needs to be solved differently.
+	 */
+	if (sock_owned_by_user(sk))
+		NET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);
+
+	if (sk->sk_state == TCP_CLOSE)
+		goto out;
+
+	icsk = inet_csk(sk);
+	tp = tcp_sk(sk);
+	seq = ntohl(th->seq);
+	if (sk->sk_state != TCP_LISTEN &&
+	    !between(seq, tp->snd_una, tp->snd_nxt)) {
+		NET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);
+		goto out;
+	}
+
+	switch (type) {
+	case ICMP_SOURCE_QUENCH:
+		/* Just silently ignore these. */
+		goto out;
+	case ICMP_PARAMETERPROB:
+		err = EPROTO;
+		break;
+	case ICMP_DEST_UNREACH:
+		if (code > NR_ICMP_UNREACH)
+			goto out;
+
+		if (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */
+			if (!sock_owned_by_user(sk))
+				do_pmtu_discovery(sk, iph, info);
+			goto out;
+		}
+
+		err = icmp_err_convert[code].errno;
+		/* check if icmp_skb allows revert of backoff
+		 * (see draft-zimmermann-tcp-lcd) */
+		if (code != ICMP_NET_UNREACH && code != ICMP_HOST_UNREACH)
+			break;
+		if (seq != tp->snd_una  || !icsk->icsk_retransmits ||
+		    !icsk->icsk_backoff)
+			break;
+
+		if (sock_owned_by_user(sk))
+			break;
+
+		icsk->icsk_backoff--;
+		inet_csk(sk)->icsk_rto = (tp->srtt ? __tcp_set_rto(tp) :
+			TCP_TIMEOUT_INIT) << icsk->icsk_backoff;
+		tcp_bound_rto(sk);
+
+		skb = tcp_write_queue_head(sk);
+		BUG_ON(!skb);
+
+		remaining = icsk->icsk_rto - min(icsk->icsk_rto,
+				tcp_time_stamp - TCP_SKB_CB(skb)->when);
+
+		if (remaining) {
+			inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
+						  remaining, TCP_RTO_MAX);
+		} else {
+			/* RTO revert clocked out retransmission.
+			 * Will retransmit now */
+			tcp_retransmit_timer(sk);
+		}
+
+		break;
+	case ICMP_TIME_EXCEEDED:
+		err = EHOSTUNREACH;
+		break;
+	default:
+		goto out;
+	}
+
+	switch (sk->sk_state) {
+		struct request_sock *req, **prev;
+	case TCP_LISTEN:
+		if (sock_owned_by_user(sk))
+			goto out;
+
+		req = inet_csk_search_req(sk, &prev, th->dest,
+					  iph->daddr, iph->saddr);
+		if (!req)
+			goto out;
+
+		/* ICMPs are not backlogged, hence we cannot get
+		   an established socket here.
+		 */
+		WARN_ON(req->sk);
+
+		if (seq != tcp_rsk(req)->snt_isn) {
+			NET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);
+			goto out;
+		}
+
+		/*
+		 * Still in SYN_RECV, just remove it silently.
+		 * There is no good way to pass the error to the newly
+		 * created socket, and POSIX does not want network
+		 * errors returned from accept().
+		 */
+		inet_csk_reqsk_queue_drop(sk, req, prev);
+		goto out;
+
+	case TCP_SYN_SENT:
+	case TCP_SYN_RECV:  /* Cannot happen.
+			       It can f.e. if SYNs crossed.
+			     */
+		if (!sock_owned_by_user(sk)) {
+			sk->sk_err = err;
+
+			sk->sk_error_report(sk);
+
+			tcp_done(sk);
+		} else {
+			sk->sk_err_soft = err;
+		}
+		goto out;
+	}
+
+	/* If we've already connected we will keep trying
+	 * until we time out, or the user gives up.
+	 *
+	 * rfc1122 4.2.3.9 allows to consider as hard errors
+	 * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,
+	 * but it is obsoleted by pmtu discovery).
+	 *
+	 * Note, that in modern internet, where routing is unreliable
+	 * and in each dark corner broken firewalls sit, sending random
+	 * errors ordered by their masters even this two messages finally lose
+	 * their original sense (even Linux sends invalid PORT_UNREACHs)
+	 *
+	 * Now we are in compliance with RFCs.
+	 *							--ANK (980905)
+	 */
+
+	inet = inet_sk(sk);
+	if (!sock_owned_by_user(sk) && inet->recverr) {
+		sk->sk_err = err;
+		sk->sk_error_report(sk);
+	} else	{ /* Only an error on timeout */
+		sk->sk_err_soft = err;
+	}
+
+out:
+	bh_unlock_sock(sk);
+	sock_put(sk);
+}
+
+/* This routine computes an IPv4 TCP checksum. */
+void tcp_v4_send_check(struct sock *sk, int len, struct sk_buff *skb)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	struct tcphdr *th = tcp_hdr(skb);
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		th->check = ~tcp_v4_check(len, inet->saddr,
+					  inet->daddr, 0);
+		skb->csum_start = skb_transport_header(skb) - skb->head;
+		skb->csum_offset = offsetof(struct tcphdr, check);
+	} else {
+		th->check = tcp_v4_check(len, inet->saddr, inet->daddr,
+					 csum_partial(th,
+						      th->doff << 2,
+						      skb->csum));
+	}
+}
+
+int tcp_v4_gso_send_check(struct sk_buff *skb)
+{
+	const struct iphdr *iph;
+	struct tcphdr *th;
+
+	if (!pskb_may_pull(skb, sizeof(*th)))
+		return -EINVAL;
+
+	iph = ip_hdr(skb);
+	th = tcp_hdr(skb);
+
+	th->check = 0;
+	th->check = ~tcp_v4_check(skb->len, iph->saddr, iph->daddr, 0);
+	skb->csum_start = skb_transport_header(skb) - skb->head;
+	skb->csum_offset = offsetof(struct tcphdr, check);
+	skb->ip_summed = CHECKSUM_PARTIAL;
+	return 0;
+}
+
+/*
+ *	This routine will send an RST to the other tcp.
+ *
+ *	Someone asks: why I NEVER use socket parameters (TOS, TTL etc.)
+ *		      for reset.
+ *	Answer: if a packet caused RST, it is not for a socket
+ *		existing in our system, if it is matched to a socket,
+ *		it is just duplicate segment or bug in other side's TCP.
+ *		So that we build reply only basing on parameters
+ *		arrived with segment.
+ *	Exception: precedence violation. We do not implement it in any case.
+ */
+
+static void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcphdr *th = tcp_hdr(skb);
+	struct {
+		struct tcphdr th;
+#ifdef CONFIG_TCP_MD5SIG
+		__be32 opt[(TCPOLEN_MD5SIG_ALIGNED >> 2)];
+#endif
+	} rep;
+	struct ip_reply_arg arg;
+#ifdef CONFIG_TCP_MD5SIG
+	struct tcp_md5sig_key *key;
+#endif
+	struct net *net;
+
+	/* Never send a reset in response to a reset. */
+	if (th->rst)
+		return;
+
+	if (skb_rtable(skb)->rt_type != RTN_LOCAL)
+		return;
+
+	/* Swap the send and the receive. */
+	memset(&rep, 0, sizeof(rep));
+	rep.th.dest   = th->source;
+	rep.th.source = th->dest;
+	rep.th.doff   = sizeof(struct tcphdr) / 4;
+	rep.th.rst    = 1;
+
+	if (th->ack) {
+		rep.th.seq = th->ack_seq;
+	} else {
+		rep.th.ack = 1;
+		rep.th.ack_seq = htonl(ntohl(th->seq) + th->syn + th->fin +
+				       skb->len - (th->doff << 2));
+	}
+
+	memset(&arg, 0, sizeof(arg));
+	arg.iov[0].iov_base = (unsigned char *)&rep;
+	arg.iov[0].iov_len  = sizeof(rep.th);
+
+#ifdef CONFIG_TCP_MD5SIG
+	key = (sk && sk->sk_state != TCP_TIME_WAIT) ?
+		tcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr) : NULL;
+	if (key) {
+		rep.opt[0] = htonl((TCPOPT_NOP << 24) |
+				   (TCPOPT_NOP << 16) |
+				   (TCPOPT_MD5SIG << 8) |
+				   TCPOLEN_MD5SIG);
+		/* Update length and the length the header thinks exists */
+		arg.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;
+		rep.th.doff = arg.iov[0].iov_len / 4;
+
+		tcp_v4_md5_hash_hdr((__u8 *) &rep.opt[1],
+				     key, ip_hdr(skb)->saddr,
+				     ip_hdr(skb)->daddr, &rep.th);
+	}
+#endif
+	arg.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,
+				      ip_hdr(skb)->saddr, /* XXX */
+				      arg.iov[0].iov_len, IPPROTO_TCP, 0);
+	arg.csumoffset = offsetof(struct tcphdr, check) / 2;
+	arg.flags = (sk && inet_sk_transparent(sk)) ? IP_REPLY_ARG_NOSRCCHECK : 0;
+
+	/* When socket is gone, all binding information is lost.
+	 * routing might fail in this case. No choice here, if we choose to force
+	 * input interface, we will misroute in case of asymmetric route.
+	 */
+	if (sk)
+		arg.bound_dev_if = sk->sk_bound_dev_if;
+
+	BUILD_BUG_ON(offsetof(struct sock, sk_bound_dev_if) !=
+		     offsetof(struct inet_timewait_sock, tw_bound_dev_if));
+
+	net = dev_net(skb_dst(skb)->dev);
+	ip_send_reply(net->ipv4.tcp_sock, skb,
+		      &arg, arg.iov[0].iov_len);
+
+	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
+	TCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);
+}
+
+/* The code following below sending ACKs in SYN-RECV and TIME-WAIT states
+   outside socket context is ugly, certainly. What can I do?
+ */
+
+static void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,
+			    u32 win, u32 ts, int oif,
+			    struct tcp_md5sig_key *key,
+			    int reply_flags)
+{
+	struct tcphdr *th = tcp_hdr(skb);
+	struct {
+		struct tcphdr th;
+		__be32 opt[(TCPOLEN_TSTAMP_ALIGNED >> 2)
+#ifdef CONFIG_TCP_MD5SIG
+			   + (TCPOLEN_MD5SIG_ALIGNED >> 2)
+#endif
+			];
+	} rep;
+	struct ip_reply_arg arg;
+	struct net *net = dev_net(skb_dst(skb)->dev);
+
+	memset(&rep.th, 0, sizeof(struct tcphdr));
+	memset(&arg, 0, sizeof(arg));
+
+	arg.iov[0].iov_base = (unsigned char *)&rep;
+	arg.iov[0].iov_len  = sizeof(rep.th);
+	if (ts) {
+		rep.opt[0] = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
+				   (TCPOPT_TIMESTAMP << 8) |
+				   TCPOLEN_TIMESTAMP);
+		rep.opt[1] = htonl(tcp_time_stamp);
+		rep.opt[2] = htonl(ts);
+		arg.iov[0].iov_len += TCPOLEN_TSTAMP_ALIGNED;
+	}
+
+	/* Swap the send and the receive. */
+	rep.th.dest    = th->source;
+	rep.th.source  = th->dest;
+	rep.th.doff    = arg.iov[0].iov_len / 4;
+	rep.th.seq     = htonl(seq);
+	rep.th.ack_seq = htonl(ack);
+	rep.th.ack     = 1;
+	rep.th.window  = htons(win);
+
+#ifdef CONFIG_TCP_MD5SIG
+	if (key) {
+		int offset = (ts) ? 3 : 0;
+
+		rep.opt[offset++] = htonl((TCPOPT_NOP << 24) |
+					  (TCPOPT_NOP << 16) |
+					  (TCPOPT_MD5SIG << 8) |
+					  TCPOLEN_MD5SIG);
+		arg.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;
+		rep.th.doff = arg.iov[0].iov_len/4;
+
+		tcp_v4_md5_hash_hdr((__u8 *) &rep.opt[offset],
+				    key, ip_hdr(skb)->saddr,
+				    ip_hdr(skb)->daddr, &rep.th);
+	}
+#endif
+	arg.flags = reply_flags;
+	arg.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,
+				      ip_hdr(skb)->saddr, /* XXX */
+				      arg.iov[0].iov_len, IPPROTO_TCP, 0);
+	arg.csumoffset = offsetof(struct tcphdr, check) / 2;
+	if (oif)
+		arg.bound_dev_if = oif;
+
+	ip_send_reply(net->ipv4.tcp_sock, skb,
+		      &arg, arg.iov[0].iov_len);
+
+	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
+}
+
+static void tcp_v4_timewait_ack(struct sock *sk, struct sk_buff *skb)
+{
+	struct inet_timewait_sock *tw = inet_twsk(sk);
+	struct tcp_timewait_sock *tcptw = tcp_twsk(sk);
+
+	tcp_v4_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,
+			tcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,
+			tcptw->tw_ts_recent,
+			tw->tw_bound_dev_if,
+			tcp_twsk_md5_key(tcptw),
+			tw->tw_transparent ? IP_REPLY_ARG_NOSRCCHECK : 0
+			);
+
+	inet_twsk_put(tw);
+}
+
+static void tcp_v4_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,
+				  struct request_sock *req)
+{
+	tcp_v4_send_ack(skb, tcp_rsk(req)->snt_isn + 1,
+			tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd,
+			req->ts_recent,
+			0,
+			tcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr),
+			inet_rsk(req)->no_srccheck ? IP_REPLY_ARG_NOSRCCHECK : 0);
+}
+
+/*
+ *	Send a SYN-ACK after having received a SYN.
+ *	This still operates on a request_sock only, not on a big
+ *	socket.
+ */
+static int __tcp_v4_send_synack(struct sock *sk, struct request_sock *req,
+				struct dst_entry *dst)
+{
+	const struct inet_request_sock *ireq = inet_rsk(req);
+	int err = -1;
+	struct sk_buff * skb;
+
+	/* First, grab a route. */
+	if (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)
+		return -1;
+
+	skb = tcp_make_synack(sk, dst, req);
+
+	if (skb) {
+		struct tcphdr *th = tcp_hdr(skb);
+
+		th->check = tcp_v4_check(skb->len,
+					 ireq->loc_addr,
+					 ireq->rmt_addr,
+					 csum_partial(th, skb->len,
+						      skb->csum));
+
+		err = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,
+					    ireq->rmt_addr,
+					    ireq->opt);
+		err = net_xmit_eval(err);
+	}
+
+	dst_release(dst);
+	return err;
+}
+
+static int tcp_v4_send_synack(struct sock *sk, struct request_sock *req)
+{
+	return __tcp_v4_send_synack(sk, req, NULL);
+}
+
+/*
+ *	IPv4 request_sock destructor.
+ */
+static void tcp_v4_reqsk_destructor(struct request_sock *req)
+{
+	kfree_ip_options(inet_rsk(req)->opt);
+}
+
+#ifdef CONFIG_SYN_COOKIES
+static void syn_flood_warning(struct sk_buff *skb)
+{
+	static unsigned long warntime;
+
+	if (time_after(jiffies, (warntime + HZ * 60))) {
+		warntime = jiffies;
+		printk(KERN_INFO
+		       "possible SYN flooding on port %d. Sending cookies.\n",
+		       ntohs(tcp_hdr(skb)->dest));
+	}
+}
+#endif
+
+/*
+ * Save and compile IPv4 options into the request_sock if needed.
+ */
+static struct ip_options *tcp_v4_save_options(struct sock *sk,
+					      struct sk_buff *skb)
+{
+	struct ip_options *opt = &(IPCB(skb)->opt);
+	struct ip_options *dopt = NULL;
+
+	if (opt && opt->optlen) {
+		dopt = kmalloc_ip_options(opt->optlen, GFP_ATOMIC);
+		if (dopt) {
+			if (ip_options_echo(dopt, skb)) {
+				kfree_ip_options(dopt);
+				dopt = NULL;
+			} else {
+				rhel_ip_options_set_alloc_flag(dopt);
+			}
+		}
+	}
+	return dopt;
+}
+
+#ifdef CONFIG_TCP_MD5SIG
+/*
+ * RFC2385 MD5 checksumming requires a mapping of
+ * IP address->MD5 Key.
+ * We need to maintain these in the sk structure.
+ */
+
+/* Find the Key structure for an address.  */
+static struct tcp_md5sig_key *
+			tcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int i;
+
+	if (!tp->md5sig_info || !tp->md5sig_info->entries4)
+		return NULL;
+	for (i = 0; i < tp->md5sig_info->entries4; i++) {
+		if (tp->md5sig_info->keys4[i].addr == addr)
+			return &tp->md5sig_info->keys4[i].base;
+	}
+	return NULL;
+}
+
+struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
+					 struct sock *addr_sk)
+{
+	return tcp_v4_md5_do_lookup(sk, inet_sk(addr_sk)->daddr);
+}
+
+EXPORT_SYMBOL(tcp_v4_md5_lookup);
+
+static struct tcp_md5sig_key *tcp_v4_reqsk_md5_lookup(struct sock *sk,
+						      struct request_sock *req)
+{
+	return tcp_v4_md5_do_lookup(sk, inet_rsk(req)->rmt_addr);
+}
+
+/* This can be called on a newly created socket, from other files */
+int tcp_v4_md5_do_add(struct sock *sk, __be32 addr,
+		      u8 *newkey, u8 newkeylen)
+{
+	/* Add Key to the list */
+	struct tcp_md5sig_key *key;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp4_md5sig_key *keys;
+
+	key = tcp_v4_md5_do_lookup(sk, addr);
+	if (key) {
+		/* Pre-existing entry - just update that one. */
+		kfree(key->key);
+		key->key = newkey;
+		key->keylen = newkeylen;
+	} else {
+		struct tcp_md5sig_info *md5sig;
+
+		if (!tp->md5sig_info) {
+			tp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info),
+						  GFP_ATOMIC);
+			if (!tp->md5sig_info) {
+				kfree(newkey);
+				return -ENOMEM;
+			}
+			sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
+		}
+		if (tcp_alloc_md5sig_pool(sk) == NULL) {
+			kfree(newkey);
+			return -ENOMEM;
+		}
+		md5sig = tp->md5sig_info;
+
+		if (md5sig->alloced4 == md5sig->entries4) {
+			keys = kmalloc((sizeof(*keys) *
+					(md5sig->entries4 + 1)), GFP_ATOMIC);
+			if (!keys) {
+				kfree(newkey);
+				tcp_free_md5sig_pool();
+				return -ENOMEM;
+			}
+
+			if (md5sig->entries4)
+				memcpy(keys, md5sig->keys4,
+				       sizeof(*keys) * md5sig->entries4);
+
+			/* Free old key list, and reference new one */
+			kfree(md5sig->keys4);
+			md5sig->keys4 = keys;
+			md5sig->alloced4++;
+		}
+		md5sig->entries4++;
+		md5sig->keys4[md5sig->entries4 - 1].addr        = addr;
+		md5sig->keys4[md5sig->entries4 - 1].base.key    = newkey;
+		md5sig->keys4[md5sig->entries4 - 1].base.keylen = newkeylen;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(tcp_v4_md5_do_add);
+
+static int tcp_v4_md5_add_func(struct sock *sk, struct sock *addr_sk,
+			       u8 *newkey, u8 newkeylen)
+{
+	return tcp_v4_md5_do_add(sk, inet_sk(addr_sk)->daddr,
+				 newkey, newkeylen);
+}
+
+int tcp_v4_md5_do_del(struct sock *sk, __be32 addr)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int i;
+
+	for (i = 0; i < tp->md5sig_info->entries4; i++) {
+		if (tp->md5sig_info->keys4[i].addr == addr) {
+			/* Free the key */
+			kfree(tp->md5sig_info->keys4[i].base.key);
+			tp->md5sig_info->entries4--;
+
+			if (tp->md5sig_info->entries4 == 0) {
+				kfree(tp->md5sig_info->keys4);
+				tp->md5sig_info->keys4 = NULL;
+				tp->md5sig_info->alloced4 = 0;
+			} else if (tp->md5sig_info->entries4 != i) {
+				/* Need to do some manipulation */
+				memmove(&tp->md5sig_info->keys4[i],
+					&tp->md5sig_info->keys4[i+1],
+					(tp->md5sig_info->entries4 - i) *
+					 sizeof(struct tcp4_md5sig_key));
+			}
+			tcp_free_md5sig_pool();
+			return 0;
+		}
+	}
+	return -ENOENT;
+}
+
+EXPORT_SYMBOL(tcp_v4_md5_do_del);
+
+static void tcp_v4_clear_md5_list(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	/* Free each key, then the set of key keys,
+	 * the crypto element, and then decrement our
+	 * hold on the last resort crypto.
+	 */
+	if (tp->md5sig_info->entries4) {
+		int i;
+		for (i = 0; i < tp->md5sig_info->entries4; i++)
+			kfree(tp->md5sig_info->keys4[i].base.key);
+		tp->md5sig_info->entries4 = 0;
+		tcp_free_md5sig_pool();
+	}
+	if (tp->md5sig_info->keys4) {
+		kfree(tp->md5sig_info->keys4);
+		tp->md5sig_info->keys4 = NULL;
+		tp->md5sig_info->alloced4  = 0;
+	}
+}
+
+static int tcp_v4_parse_md5_keys(struct sock *sk, char __user *optval,
+				 int optlen)
+{
+	struct tcp_md5sig cmd;
+	struct sockaddr_in *sin = (struct sockaddr_in *)&cmd.tcpm_addr;
+	u8 *newkey;
+
+	if (optlen < sizeof(cmd))
+		return -EINVAL;
+
+	if (copy_from_user(&cmd, optval, sizeof(cmd)))
+		return -EFAULT;
+
+	if (sin->sin_family != AF_INET)
+		return -EINVAL;
+
+	if (!cmd.tcpm_key || !cmd.tcpm_keylen) {
+		if (!tcp_sk(sk)->md5sig_info)
+			return -ENOENT;
+		return tcp_v4_md5_do_del(sk, sin->sin_addr.s_addr);
+	}
+
+	if (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)
+		return -EINVAL;
+
+	if (!tcp_sk(sk)->md5sig_info) {
+		struct tcp_sock *tp = tcp_sk(sk);
+		struct tcp_md5sig_info *p;
+
+		p = kzalloc(sizeof(*p), sk->sk_allocation);
+		if (!p)
+			return -EINVAL;
+
+		tp->md5sig_info = p;
+		sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
+	}
+
+	newkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, sk->sk_allocation);
+	if (!newkey)
+		return -ENOMEM;
+	return tcp_v4_md5_do_add(sk, sin->sin_addr.s_addr,
+				 newkey, cmd.tcpm_keylen);
+}
+
+static int tcp_v4_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,
+					__be32 daddr, __be32 saddr, int nbytes)
+{
+	struct tcp4_pseudohdr *bp;
+	struct scatterlist sg;
+
+	bp = &hp->md5_blk.ip4;
+
+	/*
+	 * 1. the TCP pseudo-header (in the order: source IP address,
+	 * destination IP address, zero-padded protocol number, and
+	 * segment length)
+	 */
+	bp->saddr = saddr;
+	bp->daddr = daddr;
+	bp->pad = 0;
+	bp->protocol = IPPROTO_TCP;
+	bp->len = cpu_to_be16(nbytes);
+
+	sg_init_one(&sg, bp, sizeof(*bp));
+	return crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));
+}
+
+static int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,
+			       __be32 daddr, __be32 saddr, struct tcphdr *th)
+{
+	struct tcp_md5sig_pool *hp;
+	struct hash_desc *desc;
+
+	hp = tcp_get_md5sig_pool();
+	if (!hp)
+		goto clear_hash_noput;
+	desc = &hp->md5_desc;
+
+	if (crypto_hash_init(desc))
+		goto clear_hash;
+	if (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))
+		goto clear_hash;
+	if (tcp_md5_hash_header(hp, th))
+		goto clear_hash;
+	if (tcp_md5_hash_key(hp, key))
+		goto clear_hash;
+	if (crypto_hash_final(desc, md5_hash))
+		goto clear_hash;
+
+	tcp_put_md5sig_pool();
+	return 0;
+
+clear_hash:
+	tcp_put_md5sig_pool();
+clear_hash_noput:
+	memset(md5_hash, 0, 16);
+	return 1;
+}
+
+int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
+			struct sock *sk, struct request_sock *req,
+			struct sk_buff *skb)
+{
+	struct tcp_md5sig_pool *hp;
+	struct hash_desc *desc;
+	struct tcphdr *th = tcp_hdr(skb);
+	__be32 saddr, daddr;
+
+	if (sk) {
+		saddr = inet_sk(sk)->saddr;
+		daddr = inet_sk(sk)->daddr;
+	} else if (req) {
+		saddr = inet_rsk(req)->loc_addr;
+		daddr = inet_rsk(req)->rmt_addr;
+	} else {
+		const struct iphdr *iph = ip_hdr(skb);
+		saddr = iph->saddr;
+		daddr = iph->daddr;
+	}
+
+	hp = tcp_get_md5sig_pool();
+	if (!hp)
+		goto clear_hash_noput;
+	desc = &hp->md5_desc;
+
+	if (crypto_hash_init(desc))
+		goto clear_hash;
+
+	if (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))
+		goto clear_hash;
+	if (tcp_md5_hash_header(hp, th))
+		goto clear_hash;
+	if (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))
+		goto clear_hash;
+	if (tcp_md5_hash_key(hp, key))
+		goto clear_hash;
+	if (crypto_hash_final(desc, md5_hash))
+		goto clear_hash;
+
+	tcp_put_md5sig_pool();
+	return 0;
+
+clear_hash:
+	tcp_put_md5sig_pool();
+clear_hash_noput:
+	memset(md5_hash, 0, 16);
+	return 1;
+}
+
+EXPORT_SYMBOL(tcp_v4_md5_hash_skb);
+
+static int tcp_v4_inbound_md5_hash(struct sock *sk, struct sk_buff *skb)
+{
+	/*
+	 * This gets called for each TCP segment that arrives
+	 * so we want to be efficient.
+	 * We have 3 drop cases:
+	 * o No MD5 hash and one expected.
+	 * o MD5 hash and we're not expecting one.
+	 * o MD5 hash and its wrong.
+	 */
+	__u8 *hash_location = NULL;
+	struct tcp_md5sig_key *hash_expected;
+	const struct iphdr *iph = ip_hdr(skb);
+	struct tcphdr *th = tcp_hdr(skb);
+	int genhash;
+	unsigned char newhash[16];
+
+	hash_expected = tcp_v4_md5_do_lookup(sk, iph->saddr);
+	hash_location = tcp_parse_md5sig_option(th);
+
+	/* We've parsed the options - do we have a hash? */
+	if (!hash_expected && !hash_location)
+		return 0;
+
+	if (hash_expected && !hash_location) {
+		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);
+		return 1;
+	}
+
+	if (!hash_expected && hash_location) {
+		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);
+		return 1;
+	}
+
+	/* Okay, so this is hash_expected and hash_location -
+	 * so we need to calculate the checksum.
+	 */
+	genhash = tcp_v4_md5_hash_skb(newhash,
+				      hash_expected,
+				      NULL, NULL, skb);
+
+	if (genhash || memcmp(hash_location, newhash, 16) != 0) {
+		if (net_ratelimit()) {
+			printk(KERN_INFO "MD5 Hash failed for (%pI4, %d)->(%pI4, %d)%s\n",
+			       &iph->saddr, ntohs(th->source),
+			       &iph->daddr, ntohs(th->dest),
+			       genhash ? " tcp_v4_calc_md5_hash failed" : "");
+		}
+		return 1;
+	}
+	return 0;
+}
+
+#endif
+
+struct request_sock_ops tcp_request_sock_ops __read_mostly = {
+	.family		=	PF_INET,
+	.obj_size	=	sizeof(struct tcp_request_sock),
+	.rtx_syn_ack	=	tcp_v4_send_synack,
+	.send_ack	=	tcp_v4_reqsk_send_ack,
+	.destructor	=	tcp_v4_reqsk_destructor,
+	.send_reset	=	tcp_v4_send_reset,
+};
+
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
+	.md5_lookup	=	tcp_v4_reqsk_md5_lookup,
+	.calc_md5_hash	=	tcp_v4_md5_hash_skb,
+};
+#endif
+
+static struct timewait_sock_ops tcp_timewait_sock_ops = {
+	.twsk_obj_size	= sizeof(struct tcp_timewait_sock),
+	.twsk_unique	= tcp_twsk_unique,
+	.twsk_destructor= tcp_twsk_destructor,
+};
+
+int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
+{
+	struct inet_request_sock *ireq;
+	struct tcp_options_received tmp_opt;
+	struct request_sock *req;
+	__be32 saddr = ip_hdr(skb)->saddr;
+	__be32 daddr = ip_hdr(skb)->daddr;
+	__u32 isn = TCP_SKB_CB(skb)->when;
+	struct dst_entry *dst = NULL;
+#ifdef CONFIG_SYN_COOKIES
+	int want_cookie = 0;
+#else
+#define want_cookie 0 /* Argh, why doesn't gcc optimize this :( */
+#endif
+
+	/* Never answer to SYNs send to broadcast or multicast */
+	if (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))
+		goto drop;
+
+	/* TW buckets are converted to open requests without
+	 * limitations, they conserve resources and peer is
+	 * evidently real one.
+	 */
+	if (inet_csk_reqsk_queue_is_full(sk) && !isn) {
+#ifdef CONFIG_SYN_COOKIES
+		if (sysctl_tcp_syncookies) {
+			want_cookie = 1;
+		} else
+#endif
+		goto drop;
+	}
+
+	/* Accept backlog is full. If we have already queued enough
+	 * of warm entries in syn queue, drop request. It is better than
+	 * clogging syn queue with openreqs with exponentially increasing
+	 * timeout.
+	 */
+	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)
+		goto drop;
+
+	req = inet_reqsk_alloc(&tcp_request_sock_ops);
+	if (!req)
+		goto drop;
+
+#ifdef CONFIG_TCP_MD5SIG
+	tcp_rsk(req)->af_specific = &tcp_request_sock_ipv4_ops;
+#endif
+
+	tcp_clear_options(&tmp_opt);
+	tmp_opt.mss_clamp = 536;
+	tmp_opt.user_mss  = tcp_sk(sk)->rx_opt.user_mss;
+
+	tcp_parse_options(skb, &tmp_opt, 0);
+
+	if (want_cookie && !tmp_opt.saw_tstamp)
+		tcp_clear_options(&tmp_opt);
+
+	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+
+	tcp_openreq_init(req, &tmp_opt, skb);
+
+	ireq = inet_rsk(req);
+	ireq->loc_addr = daddr;
+	ireq->rmt_addr = saddr;
+	ireq->no_srccheck = inet_sk(sk)->transparent;
+	ireq->opt = tcp_v4_save_options(sk, skb);
+
+	if (security_inet_conn_request(sk, skb, req))
+		goto drop_and_free;
+
+	if (!want_cookie)
+		TCP_ECN_create_request(req, tcp_hdr(skb));
+
+	if (want_cookie) {
+#ifdef CONFIG_SYN_COOKIES
+		syn_flood_warning(skb);
+		req->cookie_ts = tmp_opt.tstamp_ok;
+#endif
+		isn = cookie_v4_init_sequence(sk, skb, &req->mss);
+	} else if (!isn) {
+		struct inet_peer *peer = NULL;
+
+		/* VJ's idea. We save last timestamp seen
+		 * from the destination in peer table, when entering
+		 * state TIME-WAIT, and check against it before
+		 * accepting new connection request.
+		 *
+		 * If "isn" is not zero, this request hit alive
+		 * timewait bucket, so that all the necessary checks
+		 * are made in the function processing timewait state.
+		 */
+		if (tmp_opt.saw_tstamp &&
+		    tcp_death_row.sysctl_tw_recycle &&
+		    (dst = inet_csk_route_req(sk, req)) != NULL &&
+		    (peer = rt_get_peer((struct rtable *)dst)) != NULL &&
+		    peer->v4daddr == saddr) {
+			if (get_seconds() < peer->tcp_ts_stamp + TCP_PAWS_MSL &&
+			    (s32)(peer->tcp_ts - req->ts_recent) >
+							TCP_PAWS_WINDOW) {
+				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+				goto drop_and_release;
+			}
+		}
+		/* Kill the following clause, if you dislike this way. */
+		else if (!sysctl_tcp_syncookies &&
+			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+			  (sysctl_max_syn_backlog >> 2)) &&
+			 (!peer || !peer->tcp_ts_stamp) &&
+			 (!dst || !dst_metric(dst, RTAX_RTT))) {
+			/* Without syncookies last quarter of
+			 * backlog is filled with destinations,
+			 * proven to be alive.
+			 * It means that we continue to communicate
+			 * to destinations, already remembered
+			 * to the moment of synflood.
+			 */
+			LIMIT_NETDEBUG(KERN_DEBUG "TCP: drop open request from %pI4/%u\n",
+				       &saddr, ntohs(tcp_hdr(skb)->source));
+			goto drop_and_release;
+		}
+
+		isn = tcp_v4_init_sequence(skb);
+	}
+	tcp_rsk(req)->snt_isn = isn;
+	tcp_rsk(req)->snt_synack = tcp_time_stamp;
+
+	if (__tcp_v4_send_synack(sk, req, dst) || want_cookie)
+		goto drop_and_free;
+
+	inet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+	return 0;
+
+drop_and_release:
+	dst_release(dst);
+drop_and_free:
+	reqsk_free(req);
+drop:
+	return 0;
+}
+
+
+/*
+ * The three way handshake has completed - we got a valid synack -
+ * now create the new socket.
+ */
+struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+				  struct request_sock *req,
+				  struct dst_entry *dst)
+{
+	struct inet_request_sock *ireq;
+	struct inet_sock *newinet;
+	struct tcp_sock *newtp;
+	struct sock *newsk;
+#ifdef CONFIG_TCP_MD5SIG
+	struct tcp_md5sig_key *key;
+#endif
+	struct ip_options *inet_opt;
+
+	if (sk_acceptq_is_full(sk))
+		goto exit_overflow;
+
+	if (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)
+		goto exit;
+
+	newsk = tcp_create_openreq_child(sk, req, skb);
+	if (!newsk)
+		goto exit_nonewsk;
+
+	newsk->sk_gso_type = SKB_GSO_TCPV4;
+	sk_setup_caps(newsk, dst);
+
+	newtp		      = tcp_sk(newsk);
+	newinet		      = inet_sk(newsk);
+	ireq		      = inet_rsk(req);
+	newinet->daddr	      = ireq->rmt_addr;
+	newinet->rcv_saddr    = ireq->loc_addr;
+	newinet->saddr	      = ireq->loc_addr;
+	inet_opt	      = ireq->opt;
+	rcu_assign_pointer(newinet->opt, inet_opt);
+	ireq->opt	      = NULL;
+	newinet->mc_index     = inet_iif(skb);
+	newinet->mc_ttl	      = ip_hdr(skb)->ttl;
+	sk_extended(newsk)->rcv_tos = ip_hdr(skb)->tos;
+	inet_csk(newsk)->icsk_ext_hdr_len = 0;
+	if (inet_opt)
+		inet_csk(newsk)->icsk_ext_hdr_len = inet_opt->optlen;
+	newinet->id = newtp->write_seq ^ jiffies;
+
+	tcp_mtup_init(newsk);
+	tcp_sync_mss(newsk, dst_mtu(dst));
+	newtp->advmss = dst_metric_advmss(dst);
+	if (tcp_sk(sk)->rx_opt.user_mss &&
+	    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)
+		newtp->advmss = tcp_sk(sk)->rx_opt.user_mss;
+
+	tcp_initialize_rcv_mss(newsk);
+	if (tcp_rsk(req)->snt_synack)
+		tcp_valid_rtt_meas(newsk,
+		    tcp_time_stamp - tcp_rsk(req)->snt_synack);
+	newtp->total_retrans = req->retrans;
+
+#ifdef CONFIG_TCP_MD5SIG
+	/* Copy over the MD5 key from the original socket */
+	if ((key = tcp_v4_md5_do_lookup(sk, newinet->daddr)) != NULL) {
+		/*
+		 * We're using one, so create a matching key
+		 * on the newsk structure. If we fail to get
+		 * memory, then we end up not copying the key
+		 * across. Shucks.
+		 */
+		char *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);
+		if (newkey != NULL)
+			tcp_v4_md5_do_add(newsk, newinet->daddr,
+					  newkey, key->keylen);
+		newsk->sk_route_caps &= ~NETIF_F_GSO_MASK;
+	}
+#endif
+
+	if (__inet_inherit_port(sk, newsk) < 0) {
+		sock_put(newsk);
+		goto exit;
+	}
+	__inet_hash_nolisten(newsk, NULL);
+
+	return newsk;
+
+exit_overflow:
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+exit_nonewsk:
+	dst_release(dst);
+exit:
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+	return NULL;
+}
+
+static struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcphdr *th = tcp_hdr(skb);
+	const struct iphdr *iph = ip_hdr(skb);
+	struct sock *nsk;
+	struct request_sock **prev;
+	/* Find possible connection requests. */
+	struct request_sock *req = inet_csk_search_req(sk, &prev, th->source,
+						       iph->saddr, iph->daddr);
+	if (req)
+		return tcp_check_req(sk, skb, req, prev);
+
+	nsk = inet_lookup_established(sock_net(sk), &tcp_hashinfo, iph->saddr,
+			th->source, iph->daddr, th->dest, inet_iif(skb));
+
+	if (nsk) {
+		if (nsk->sk_state != TCP_TIME_WAIT) {
+			bh_lock_sock(nsk);
+			return nsk;
+		}
+		inet_twsk_put(inet_twsk(nsk));
+		return NULL;
+	}
+
+#ifdef CONFIG_SYN_COOKIES
+	if (!th->rst && !th->syn && th->ack)
+		sk = cookie_v4_check(sk, skb, &(IPCB(skb)->opt));
+#endif
+	return sk;
+}
+
+static __sum16 tcp_v4_checksum_init(struct sk_buff *skb)
+{
+	const struct iphdr *iph = ip_hdr(skb);
+
+	if (skb->ip_summed == CHECKSUM_COMPLETE) {
+		if (!tcp_v4_check(skb->len, iph->saddr,
+				  iph->daddr, skb->csum)) {
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return 0;
+		}
+	}
+
+	skb->csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,
+				       skb->len, IPPROTO_TCP, 0);
+
+	if (skb->len <= 76) {
+		return __skb_checksum_complete(skb);
+	}
+	return 0;
+}
+
+
+/* The socket must have it's spinlock held when we get
+ * here.
+ *
+ * We have a potential double-lock case here, so even when
+ * doing backlog processing we use the BH locking scheme.
+ * This is because we cannot sleep with the original spinlock
+ * held.
+ */
+int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	struct sock *rsk;
+#ifdef CONFIG_TCP_MD5SIG
+	/*
+	 * We really want to reject the packet as early as possible
+	 * if:
+	 *  o We're expecting an MD5'd packet and this is no MD5 tcp option
+	 *  o There is an MD5 option and we're not expecting one
+	 */
+	if (tcp_v4_inbound_md5_hash(sk, skb))
+		goto discard;
+#endif
+
+	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
+		TCP_CHECK_TIMER(sk);
+		if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len)) {
+			rsk = sk;
+			goto reset;
+		}
+		TCP_CHECK_TIMER(sk);
+		return 0;
+	}
+
+	if (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))
+		goto csum_err;
+
+	if (sk->sk_state == TCP_LISTEN) {
+		struct sock *nsk = tcp_v4_hnd_req(sk, skb);
+		if (!nsk)
+			goto discard;
+
+		if (nsk != sk) {
+			sock_rps_save_rxhash(nsk, skb->rxhash);
+			if (tcp_child_process(sk, nsk, skb)) {
+				rsk = nsk;
+				goto reset;
+			}
+			return 0;
+		}
+	}
+
+	TCP_CHECK_TIMER(sk);
+	if (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len)) {
+		rsk = sk;
+		goto reset;
+	}
+	TCP_CHECK_TIMER(sk);
+	return 0;
+
+reset:
+	tcp_v4_send_reset(rsk, skb);
+discard:
+	kfree_skb(skb);
+	/* Be careful here. If this function gets more complicated and
+	 * gcc suffers from register pressure on the x86, sk (in %ebx)
+	 * might be destroyed here. This current version compiles correctly,
+	 * but you have been warned.
+	 */
+	return 0;
+
+csum_err:
+	TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+	goto discard;
+}
+
+/*
+ *	From tcp_input.c
+ */
+
+int tcp_v4_rcv(struct sk_buff *skb)
+{
+	const struct iphdr *iph;
+	struct tcphdr *th;
+	struct sock *sk;
+	int ret;
+	struct net *net = dev_net(skb->dev);
+
+	if (skb->pkt_type != PACKET_HOST)
+		goto discard_it;
+
+	/* Count it even if it's bad */
+	TCP_INC_STATS_BH(net, TCP_MIB_INSEGS);
+
+	if (!pskb_may_pull(skb, sizeof(struct tcphdr)))
+		goto discard_it;
+
+	th = tcp_hdr(skb);
+
+	if (th->doff < sizeof(struct tcphdr) / 4)
+		goto bad_packet;
+	if (!pskb_may_pull(skb, th->doff * 4))
+		goto discard_it;
+
+	/* An explanation is required here, I think.
+	 * Packet length and doff are validated by header prediction,
+	 * provided case of th->doff==0 is eliminated.
+	 * So, we defer the checks. */
+	if (!skb_csum_unnecessary(skb) && tcp_v4_checksum_init(skb))
+		goto bad_packet;
+
+	th = tcp_hdr(skb);
+	iph = ip_hdr(skb);
+	TCP_SKB_CB(skb)->seq = ntohl(th->seq);
+	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
+				    skb->len - th->doff * 4);
+	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
+	TCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);
+	TCP_SKB_CB(skb)->when	 = 0;
+	TCP_SKB_CB(skb)->ip_dsfield = ipv4_get_dsfield(iph);
+	TCP_SKB_CB(skb)->sacked	 = 0;
+
+	sk = __inet_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);
+	if (!sk)
+		goto no_tcp_socket;
+
+process:
+	if (sk->sk_state == TCP_TIME_WAIT)
+		goto do_time_wait;
+
+	if (unlikely(iph->ttl < sk_get_min_ttl(sk))) {
+		NET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);
+		goto discard_and_relse;
+	}
+
+	if (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))
+		goto discard_and_relse;
+	nf_reset(skb);
+
+	if (sk_filter(sk, skb))
+		goto discard_and_relse;
+
+	sk_mark_napi_id(sk, skb);
+	skb->dev = NULL;
+
+	inet_rps_save_rxhash(sk, skb->rxhash);
+
+	bh_lock_sock_nested(sk);
+	tcp_sk(sk)->segs_in += max_t(u16, 1, skb_shinfo(skb)->gso_segs);
+	ret = 0;
+	if (!sock_owned_by_user(sk)) {
+#ifdef CONFIG_NET_DMA
+		struct tcp_sock *tp = tcp_sk(sk);
+		if (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)
+			tp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);
+		if (tp->ucopy.dma_chan)
+			ret = tcp_v4_do_rcv(sk, skb);
+		else
+#endif
+		{
+			if (!tcp_prequeue(sk, skb))
+				ret = tcp_v4_do_rcv(sk, skb);
+		}
+	} else if (unlikely(sk_add_backlog(sk, skb,
+					   sk->sk_rcvbuf + sk->sk_sndbuf))) {
+		bh_unlock_sock(sk);
+		NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);
+		goto discard_and_relse;
+	}
+	bh_unlock_sock(sk);
+
+	sock_put(sk);
+
+	return ret;
+
+no_tcp_socket:
+	if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))
+		goto discard_it;
+
+	if (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {
+bad_packet:
+		TCP_INC_STATS_BH(net, TCP_MIB_INERRS);
+	} else {
+		tcp_v4_send_reset(NULL, skb);
+	}
+
+discard_it:
+	/* Discard frame. */
+	kfree_skb(skb);
+	return 0;
+
+discard_and_relse:
+	sock_put(sk);
+	goto discard_it;
+
+do_time_wait:
+	if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	}
+
+	if (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {
+		TCP_INC_STATS_BH(net, TCP_MIB_INERRS);
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	}
+	switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
+	case TCP_TW_SYN: {
+		struct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),
+							&tcp_hashinfo,
+							iph->saddr, th->source,
+							iph->daddr, th->dest,
+							inet_iif(skb));
+		if (sk2) {
+			inet_twsk_deschedule(inet_twsk(sk), &tcp_death_row);
+			inet_twsk_put(inet_twsk(sk));
+			sk = sk2;
+			goto process;
+		}
+		/* Fall through to ACK */
+	}
+	case TCP_TW_ACK:
+		tcp_v4_timewait_ack(sk, skb);
+		break;
+	case TCP_TW_RST:
+		tcp_v4_send_reset(sk, skb);
+		inet_twsk_deschedule(inet_twsk(sk), &tcp_death_row);
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	case TCP_TW_SUCCESS:;
+	}
+	goto discard_it;
+}
+
+/* VJ's idea. Save last timestamp seen from this destination
+ * and hold it at least for normal timewait interval to use for duplicate
+ * segment detection in subsequent connections, before they enter synchronized
+ * state.
+ */
+
+int tcp_v4_remember_stamp(struct sock *sk)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct rtable *rt = (struct rtable *)__sk_dst_get(sk);
+	struct inet_peer *peer = NULL;
+	int release_it = 0;
+
+	if (!rt || rt->rt_dst != inet->daddr) {
+		peer = inet_getpeer(inet->daddr, 1);
+		release_it = 1;
+	} else {
+		if (!rt->peer)
+			rt_bind_peer(rt, 1);
+		peer = rt->peer;
+	}
+
+	if (peer) {
+		if ((s32)(peer->tcp_ts - tp->rx_opt.ts_recent) <= 0 ||
+		    (peer->tcp_ts_stamp + TCP_PAWS_MSL < get_seconds() &&
+		     peer->tcp_ts_stamp <= tp->rx_opt.ts_recent_stamp)) {
+			peer->tcp_ts_stamp = tp->rx_opt.ts_recent_stamp;
+			peer->tcp_ts = tp->rx_opt.ts_recent;
+		}
+		if (release_it)
+			inet_putpeer(peer);
+		return 1;
+	}
+
+	return 0;
+}
+
+int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw)
+{
+	struct inet_peer *peer = inet_getpeer(tw->tw_daddr, 1);
+
+	if (peer) {
+		const struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);
+
+		if ((s32)(peer->tcp_ts - tcptw->tw_ts_recent) <= 0 ||
+		    (peer->tcp_ts_stamp + TCP_PAWS_MSL < get_seconds() &&
+		     peer->tcp_ts_stamp <= tcptw->tw_ts_recent_stamp)) {
+			peer->tcp_ts_stamp = tcptw->tw_ts_recent_stamp;
+			peer->tcp_ts	   = tcptw->tw_ts_recent;
+		}
+		inet_putpeer(peer);
+		return 1;
+	}
+
+	return 0;
+}
+
+const struct inet_connection_sock_af_ops ipv4_specific = {
+	.queue_xmit	   = ip_queue_xmit,
+	.send_check	   = tcp_v4_send_check,
+	.rebuild_header	   = inet_sk_rebuild_header,
+	.conn_request	   = tcp_v4_conn_request,
+	.syn_recv_sock	   = tcp_v4_syn_recv_sock,
+	.remember_stamp	   = tcp_v4_remember_stamp,
+	.net_header_len	   = sizeof(struct iphdr),
+	.setsockopt	   = ip_setsockopt,
+	.getsockopt	   = ip_getsockopt,
+	.addr2sockaddr	   = inet_csk_addr2sockaddr,
+	.sockaddr_len	   = sizeof(struct sockaddr_in),
+	.bind_conflict	   = inet_csk_bind_conflict,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_ip_setsockopt,
+	.compat_getsockopt = compat_ip_getsockopt,
+#endif
+};
+
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_sock_af_ops tcp_sock_ipv4_specific = {
+	.md5_lookup		= tcp_v4_md5_lookup,
+	.calc_md5_hash		= tcp_v4_md5_hash_skb,
+	.md5_add		= tcp_v4_md5_add_func,
+	.md5_parse		= tcp_v4_parse_md5_keys,
+};
+#endif
+
+/* NOTE: A lot of things set to zero explicitly by call to
+ *       sk_alloc() so need not be done here.
+ */
+static int tcp_v4_init_sock(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	skb_queue_head_init(&tp->out_of_order_queue);
+	tcp_init_xmit_timers(sk);
+	tcp_prequeue_init(tp);
+	INIT_LIST_HEAD(&tp->tsq_node);
+
+	icsk->icsk_rto = TCP_TIMEOUT_INIT;
+	tp->mdev = TCP_TIMEOUT_INIT;
+
+	/* So many TCP implementations out there (incorrectly) count the
+	 * initial SYN frame in their delayed-ACK and congestion control
+	 * algorithms that we must have the following bandaid to talk
+	 * efficiently to them.  -DaveM
+	 */
+	tp->snd_cwnd = TCP_INIT_CWND;
+
+	/* See draft-stevens-tcpca-spec-01 for discussion of the
+	 * initialization of these values.
+	 */
+	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
+	tp->snd_cwnd_clamp = ~0;
+	tp->mss_cache = 536;
+
+	tp->reordering = sysctl_tcp_reordering;
+	icsk->icsk_ca_ops = &tcp_init_congestion_ops;
+
+	sk->sk_state = TCP_CLOSE;
+
+	sk->sk_write_space = sk_stream_write_space;
+	sock_set_flag(sk, SOCK_USE_WRITE_QUEUE);
+
+	icsk->icsk_af_ops = &ipv4_specific;
+	icsk->icsk_sync_mss = tcp_sync_mss;
+#ifdef CONFIG_TCP_MD5SIG
+	tp->af_specific = &tcp_sock_ipv4_specific;
+#endif
+
+	sk->sk_sndbuf = sysctl_tcp_wmem[1];
+	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+
+	local_bh_disable();
+	percpu_counter_inc(&tcp_sockets_allocated);
+	local_bh_enable();
+
+	return 0;
+}
+
+void tcp_v4_destroy_sock(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	tcp_clear_xmit_timers(sk);
+
+	tcp_cleanup_congestion_control(sk);
+
+	/* Cleanup up the write buffer. */
+	tcp_write_queue_purge(sk);
+
+	/* Cleans up our, hopefully empty, out_of_order_queue. */
+	__skb_queue_purge(&tp->out_of_order_queue);
+
+#ifdef CONFIG_TCP_MD5SIG
+	/* Clean up the MD5 key list, if any */
+	if (tp->md5sig_info) {
+		tcp_v4_clear_md5_list(sk);
+		kfree(tp->md5sig_info);
+		tp->md5sig_info = NULL;
+	}
+#endif
+
+#ifdef CONFIG_NET_DMA
+	/* Cleans up our sk_async_wait_queue */
+	__skb_queue_purge(&sk->sk_async_wait_queue);
+#endif
+
+	/* Clean prequeue, it must be empty really */
+	__skb_queue_purge(&tp->ucopy.prequeue);
+
+	/* Clean up a referenced TCP bind bucket. */
+	if (inet_csk(sk)->icsk_bind_hash)
+		inet_put_port(sk);
+
+	/*
+	 * If sendmsg cached page exists, toss it.
+	 */
+	if (sk->sk_sndmsg_page) {
+		__free_page(sk->sk_sndmsg_page);
+		sk->sk_sndmsg_page = NULL;
+	}
+
+	percpu_counter_dec(&tcp_sockets_allocated);
+}
+
+EXPORT_SYMBOL(tcp_v4_destroy_sock);
+
+#ifdef CONFIG_PROC_FS
+/* Proc filesystem TCP sock list dumping. */
+
+static inline struct inet_timewait_sock *tw_head(struct hlist_nulls_head *head)
+{
+	return hlist_nulls_empty(head) ? NULL :
+		list_entry(head->first, struct inet_timewait_sock, tw_node);
+}
+
+static inline struct inet_timewait_sock *tw_next(struct inet_timewait_sock *tw)
+{
+	return !is_a_nulls(tw->tw_node.next) ?
+		hlist_nulls_entry(tw->tw_node.next, typeof(*tw), tw_node) : NULL;
+}
+
+static void *listening_get_next(struct seq_file *seq, void *cur)
+{
+	struct inet_connection_sock *icsk;
+	struct hlist_nulls_node *node;
+	struct sock *sk = cur;
+	struct inet_listen_hashbucket *ilb;
+	struct tcp_iter_state *st = seq->private;
+	struct net *net = seq_file_net(seq);
+
+	if (!sk) {
+		st->bucket = 0;
+		ilb = &tcp_hashinfo.listening_hash[0];
+		spin_lock_bh(&ilb->lock);
+		sk = sk_nulls_head(&ilb->head);
+		goto get_sk;
+	}
+	ilb = &tcp_hashinfo.listening_hash[st->bucket];
+	++st->num;
+
+	if (st->state == TCP_SEQ_STATE_OPENREQ) {
+		struct request_sock *req = cur;
+
+		icsk = inet_csk(st->syn_wait_sk);
+		req = req->dl_next;
+		while (1) {
+			while (req) {
+				if (req->rsk_ops->family == st->family) {
+					cur = req;
+					goto out;
+				}
+				req = req->dl_next;
+			}
+			if (++st->sbucket >= icsk->icsk_accept_queue.listen_opt->nr_table_entries)
+				break;
+get_req:
+			req = icsk->icsk_accept_queue.listen_opt->syn_table[st->sbucket];
+		}
+		sk	  = sk_next(st->syn_wait_sk);
+		st->state = TCP_SEQ_STATE_LISTENING;
+		read_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+	} else {
+		icsk = inet_csk(sk);
+		read_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+		if (reqsk_queue_len(&icsk->icsk_accept_queue))
+			goto start_req;
+		read_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+		sk = sk_next(sk);
+	}
+get_sk:
+	sk_nulls_for_each_from(sk, node) {
+		if (sk->sk_family == st->family && net_eq(sock_net(sk), net)) {
+			cur = sk;
+			goto out;
+		}
+		icsk = inet_csk(sk);
+		read_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+		if (reqsk_queue_len(&icsk->icsk_accept_queue)) {
+start_req:
+			st->uid		= sock_i_uid(sk);
+			st->syn_wait_sk = sk;
+			st->state	= TCP_SEQ_STATE_OPENREQ;
+			st->sbucket	= 0;
+			goto get_req;
+		}
+		read_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+	}
+	spin_unlock_bh(&ilb->lock);
+	if (++st->bucket < INET_LHTABLE_SIZE) {
+		ilb = &tcp_hashinfo.listening_hash[st->bucket];
+		spin_lock_bh(&ilb->lock);
+		sk = sk_nulls_head(&ilb->head);
+		goto get_sk;
+	}
+	cur = NULL;
+out:
+	return cur;
+}
+
+static void *listening_get_idx(struct seq_file *seq, loff_t *pos)
+{
+	void *rc = listening_get_next(seq, NULL);
+
+	while (rc && *pos) {
+		rc = listening_get_next(seq, rc);
+		--*pos;
+	}
+	return rc;
+}
+
+static inline int empty_bucket(struct tcp_iter_state *st)
+{
+	return hlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].chain) &&
+		hlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].twchain);
+}
+
+static void *established_get_first(struct seq_file *seq)
+{
+	struct tcp_iter_state *st = seq->private;
+	struct net *net = seq_file_net(seq);
+	void *rc = NULL;
+
+	for (st->bucket = 0; st->bucket < tcp_hashinfo.ehash_size; ++st->bucket) {
+		struct sock *sk;
+		struct hlist_nulls_node *node;
+		struct inet_timewait_sock *tw;
+		spinlock_t *lock = inet_ehash_lockp(&tcp_hashinfo, st->bucket);
+
+		/* Lockless fast path for the common case of empty buckets */
+		if (empty_bucket(st))
+			continue;
+
+		spin_lock_bh(lock);
+		sk_nulls_for_each(sk, node, &tcp_hashinfo.ehash[st->bucket].chain) {
+			if (sk->sk_family != st->family ||
+			    !net_eq(sock_net(sk), net)) {
+				continue;
+			}
+			rc = sk;
+			goto out;
+		}
+		st->state = TCP_SEQ_STATE_TIME_WAIT;
+		inet_twsk_for_each(tw, node,
+				   &tcp_hashinfo.ehash[st->bucket].twchain) {
+			if (tw->tw_family != st->family ||
+			    !net_eq(twsk_net(tw), net)) {
+				continue;
+			}
+			rc = tw;
+			goto out;
+		}
+		spin_unlock_bh(lock);
+		st->state = TCP_SEQ_STATE_ESTABLISHED;
+	}
+out:
+	return rc;
+}
+
+static void *established_get_next(struct seq_file *seq, void *cur)
+{
+	struct sock *sk = cur;
+	struct inet_timewait_sock *tw;
+	struct hlist_nulls_node *node;
+	struct tcp_iter_state *st = seq->private;
+	struct net *net = seq_file_net(seq);
+
+	++st->num;
+
+	if (st->state == TCP_SEQ_STATE_TIME_WAIT) {
+		tw = cur;
+		tw = tw_next(tw);
+get_tw:
+		while (tw && (tw->tw_family != st->family || !net_eq(twsk_net(tw), net))) {
+			tw = tw_next(tw);
+		}
+		if (tw) {
+			cur = tw;
+			goto out;
+		}
+		spin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));
+		st->state = TCP_SEQ_STATE_ESTABLISHED;
+
+		/* Look for next non empty bucket */
+		while (++st->bucket < tcp_hashinfo.ehash_size &&
+				empty_bucket(st))
+			;
+		if (st->bucket >= tcp_hashinfo.ehash_size)
+			return NULL;
+
+		spin_lock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));
+		sk = sk_nulls_head(&tcp_hashinfo.ehash[st->bucket].chain);
+	} else
+		sk = sk_nulls_next(sk);
+
+	sk_nulls_for_each_from(sk, node) {
+		if (sk->sk_family == st->family && net_eq(sock_net(sk), net))
+			goto found;
+	}
+
+	st->state = TCP_SEQ_STATE_TIME_WAIT;
+	tw = tw_head(&tcp_hashinfo.ehash[st->bucket].twchain);
+	goto get_tw;
+found:
+	cur = sk;
+out:
+	return cur;
+}
+
+static void *established_get_idx(struct seq_file *seq, loff_t pos)
+{
+	void *rc = established_get_first(seq);
+
+	while (rc && pos) {
+		rc = established_get_next(seq, rc);
+		--pos;
+	}
+	return rc;
+}
+
+static void *tcp_get_idx(struct seq_file *seq, loff_t pos)
+{
+	void *rc;
+	struct tcp_iter_state *st = seq->private;
+
+	st->state = TCP_SEQ_STATE_LISTENING;
+	rc	  = listening_get_idx(seq, &pos);
+
+	if (!rc) {
+		st->state = TCP_SEQ_STATE_ESTABLISHED;
+		rc	  = established_get_idx(seq, pos);
+	}
+
+	return rc;
+}
+
+static void *tcp_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	struct tcp_iter_state *st = seq->private;
+	st->state = TCP_SEQ_STATE_LISTENING;
+	st->num = 0;
+	return *pos ? tcp_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;
+}
+
+static void *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	void *rc = NULL;
+	struct tcp_iter_state *st;
+
+	if (v == SEQ_START_TOKEN) {
+		rc = tcp_get_idx(seq, 0);
+		goto out;
+	}
+	st = seq->private;
+
+	switch (st->state) {
+	case TCP_SEQ_STATE_OPENREQ:
+	case TCP_SEQ_STATE_LISTENING:
+		rc = listening_get_next(seq, v);
+		if (!rc) {
+			st->state = TCP_SEQ_STATE_ESTABLISHED;
+			rc	  = established_get_first(seq);
+		}
+		break;
+	case TCP_SEQ_STATE_ESTABLISHED:
+	case TCP_SEQ_STATE_TIME_WAIT:
+		rc = established_get_next(seq, v);
+		break;
+	}
+out:
+	++*pos;
+	return rc;
+}
+
+static void tcp_seq_stop(struct seq_file *seq, void *v)
+{
+	struct tcp_iter_state *st = seq->private;
+
+	switch (st->state) {
+	case TCP_SEQ_STATE_OPENREQ:
+		if (v) {
+			struct inet_connection_sock *icsk = inet_csk(st->syn_wait_sk);
+			read_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);
+		}
+	case TCP_SEQ_STATE_LISTENING:
+		if (v != SEQ_START_TOKEN)
+			spin_unlock_bh(&tcp_hashinfo.listening_hash[st->bucket].lock);
+		break;
+	case TCP_SEQ_STATE_TIME_WAIT:
+	case TCP_SEQ_STATE_ESTABLISHED:
+		if (v)
+			spin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));
+		break;
+	}
+}
+
+static int tcp_seq_open(struct inode *inode, struct file *file)
+{
+	struct tcp_seq_afinfo *afinfo = PDE(inode)->data;
+	struct tcp_iter_state *s;
+	int err;
+
+	err = seq_open_net(inode, file, &afinfo->seq_ops,
+			  sizeof(struct tcp_iter_state));
+	if (err < 0)
+		return err;
+
+	s = ((struct seq_file *)file->private_data)->private;
+	s->family		= afinfo->family;
+	return 0;
+}
+
+int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo)
+{
+	int rc = 0;
+	struct proc_dir_entry *p;
+
+	afinfo->seq_fops.open		= tcp_seq_open;
+	afinfo->seq_fops.read		= seq_read;
+	afinfo->seq_fops.llseek		= seq_lseek;
+	afinfo->seq_fops.release	= seq_release_net;
+
+	afinfo->seq_ops.start		= tcp_seq_start;
+	afinfo->seq_ops.next		= tcp_seq_next;
+	afinfo->seq_ops.stop		= tcp_seq_stop;
+
+	p = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,
+			     &afinfo->seq_fops, afinfo);
+	if (!p)
+		rc = -ENOMEM;
+	return rc;
+}
+
+void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo)
+{
+	proc_net_remove(net, afinfo->name);
+}
+
+static void get_openreq4(struct sock *sk, struct request_sock *req,
+			 struct seq_file *f, int i, int uid, int *len)
+{
+	const struct inet_request_sock *ireq = inet_rsk(req);
+	int ttd = req->expires - jiffies;
+
+	seq_printf(f, "%4d: %08X:%04X %08X:%04X"
+		" %02X %08X:%08X %02X:%08lX %08X %5u %8d %u %d %p%n",
+		i,
+		ireq->loc_addr,
+		ntohs(inet_sk(sk)->sport),
+		ireq->rmt_addr,
+		ntohs(ireq->rmt_port),
+		TCP_SYN_RECV,
+		0, 0, /* could print option size, but that is af dependent. */
+		1,    /* timers active (only the expire timer) */
+		jiffies_to_clock_t(ttd),
+		req->retrans,
+		uid,
+		0,  /* non standard timer */
+		0, /* open_requests have no inode */
+		atomic_read(&sk->sk_refcnt),
+		req,
+		len);
+}
+
+static void get_tcp4_sock(struct sock *sk, struct seq_file *f, int i, int *len)
+{
+	int timer_active;
+	unsigned long timer_expires;
+	struct tcp_sock *tp = tcp_sk(sk);
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+	struct inet_sock *inet = inet_sk(sk);
+	__be32 dest = inet->daddr;
+	__be32 src = inet->rcv_saddr;
+	__u16 destp = ntohs(inet->dport);
+	__u16 srcp = ntohs(inet->sport);
+
+	if (icsk->icsk_pending == ICSK_TIME_RETRANS) {
+		timer_active	= 1;
+		timer_expires	= icsk->icsk_timeout;
+	} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {
+		timer_active	= 4;
+		timer_expires	= icsk->icsk_timeout;
+	} else if (timer_pending(&sk->sk_timer)) {
+		timer_active	= 2;
+		timer_expires	= sk->sk_timer.expires;
+	} else {
+		timer_active	= 0;
+		timer_expires = jiffies;
+	}
+
+	seq_printf(f, "%4d: %08X:%04X %08X:%04X %02X %08X:%08X %02X:%08lX "
+			"%08X %5u %8d %lu %d %p %lu %lu %u %u %d%n",
+		i, src, srcp, dest, destp, sk->sk_state,
+		tp->write_seq - tp->snd_una,
+		sk->sk_state == TCP_LISTEN ? sk->sk_ack_backlog :
+					     (tp->rcv_nxt - tp->copied_seq),
+		timer_active,
+		jiffies_to_clock_t(timer_expires - jiffies),
+		icsk->icsk_retransmits,
+		sock_i_uid(sk),
+		icsk->icsk_probes_out,
+		sock_i_ino(sk),
+		atomic_read(&sk->sk_refcnt), sk,
+		jiffies_to_clock_t(icsk->icsk_rto),
+		jiffies_to_clock_t(icsk->icsk_ack.ato),
+		(icsk->icsk_ack.quick << 1) | icsk->icsk_ack.pingpong,
+		tp->snd_cwnd,
+		tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh,
+		len);
+}
+
+static void get_timewait4_sock(struct inet_timewait_sock *tw,
+			       struct seq_file *f, int i, int *len)
+{
+	__be32 dest, src;
+	__u16 destp, srcp;
+	int ttd = tw->tw_ttd - jiffies;
+
+	if (ttd < 0)
+		ttd = 0;
+
+	dest  = tw->tw_daddr;
+	src   = tw->tw_rcv_saddr;
+	destp = ntohs(tw->tw_dport);
+	srcp  = ntohs(tw->tw_sport);
+
+	seq_printf(f, "%4d: %08X:%04X %08X:%04X"
+		" %02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p%n",
+		i, src, srcp, dest, destp, tw->tw_substate, 0, 0,
+		3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,
+		atomic_read(&tw->tw_refcnt), tw, len);
+}
+
+#define TMPSZ 150
+
+static int tcp4_seq_show(struct seq_file *seq, void *v)
+{
+	struct tcp_iter_state *st;
+	int len;
+
+	if (v == SEQ_START_TOKEN) {
+		seq_printf(seq, "%-*s\n", TMPSZ - 1,
+			   "  sl  local_address rem_address   st tx_queue "
+			   "rx_queue tr tm->when retrnsmt   uid  timeout "
+			   "inode");
+		goto out;
+	}
+	st = seq->private;
+
+	switch (st->state) {
+	case TCP_SEQ_STATE_LISTENING:
+	case TCP_SEQ_STATE_ESTABLISHED:
+		get_tcp4_sock(v, seq, st->num, &len);
+		break;
+	case TCP_SEQ_STATE_OPENREQ:
+		get_openreq4(st->syn_wait_sk, v, seq, st->num, st->uid, &len);
+		break;
+	case TCP_SEQ_STATE_TIME_WAIT:
+		get_timewait4_sock(v, seq, st->num, &len);
+		break;
+	}
+	seq_printf(seq, "%*s\n", TMPSZ - 1 - len, "");
+out:
+	return 0;
+}
+
+static struct tcp_seq_afinfo tcp4_seq_afinfo = {
+	.name		= "tcp",
+	.family		= AF_INET,
+	.seq_fops	= {
+		.owner		= THIS_MODULE,
+	},
+	.seq_ops	= {
+		.show		= tcp4_seq_show,
+	},
+};
+
+static int tcp4_proc_init_net(struct net *net)
+{
+	return tcp_proc_register(net, &tcp4_seq_afinfo);
+}
+
+static void tcp4_proc_exit_net(struct net *net)
+{
+	tcp_proc_unregister(net, &tcp4_seq_afinfo);
+}
+
+static struct pernet_operations tcp4_net_ops = {
+	.init = tcp4_proc_init_net,
+	.exit = tcp4_proc_exit_net,
+};
+
+int __init tcp4_proc_init(void)
+{
+	return register_pernet_subsys(&tcp4_net_ops);
+}
+
+void tcp4_proc_exit(void)
+{
+	unregister_pernet_subsys(&tcp4_net_ops);
+}
+#endif /* CONFIG_PROC_FS */
+
+struct sk_buff **tcp4_gro_receive(struct sk_buff **head, struct sk_buff *skb)
+{
+	struct iphdr *iph = skb_gro_network_header(skb);
+	__wsum wsum;
+	__sum16 sum;
+
+	switch (skb->ip_summed) {
+	case CHECKSUM_COMPLETE:
+		if (!tcp_v4_check(skb_gro_len(skb), iph->saddr, iph->daddr,
+				  skb->csum)) {
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			break;
+		}
+flush:
+		NAPI_GRO_CB(skb)->flush = 1;
+		return NULL;
+
+	case CHECKSUM_NONE:
+		wsum = csum_tcpudp_nofold(iph->saddr, iph->daddr,
+					  skb_gro_len(skb), IPPROTO_TCP, 0);
+		sum = csum_fold(skb_checksum(skb,
+					     skb_gro_offset(skb),
+					     skb_gro_len(skb),
+					     wsum));
+		if (sum)
+			goto flush;
+
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+		break;
+	}
+
+	return tcp_gro_receive(head, skb);
+}
+EXPORT_SYMBOL(tcp4_gro_receive);
+
+int tcp4_gro_complete(struct sk_buff *skb)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	struct tcphdr *th = tcp_hdr(skb);
+
+	th->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),
+				  iph->saddr, iph->daddr, 0);
+	skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
+
+	return tcp_gro_complete(skb);
+}
+EXPORT_SYMBOL(tcp4_gro_complete);
+
+struct proto tcp_prot = {
+	.name			= "TCP",
+	.owner			= THIS_MODULE,
+	.close			= tcp_close,
+	.connect		= tcp_v4_connect,
+	.disconnect		= tcp_disconnect,
+	.accept			= inet_csk_accept,
+	.ioctl			= tcp_ioctl,
+	.init			= tcp_v4_init_sock,
+	.destroy		= tcp_v4_destroy_sock,
+	.shutdown		= tcp_shutdown,
+	.setsockopt		= tcp_setsockopt,
+	.getsockopt		= tcp_getsockopt,
+	.recvmsg		= tcp_recvmsg,
+	.backlog_rcv		= tcp_v4_do_rcv,
+	.release_cb		= tcp_release_cb,
+	.hash			= inet_hash,
+	.unhash			= inet_unhash,
+	.get_port		= inet_csk_get_port,
+	.enter_memory_pressure	= tcp_enter_memory_pressure,
+	.sockets_allocated	= &tcp_sockets_allocated,
+	.orphan_count		= &tcp_orphan_count,
+	.memory_allocated	= &tcp_memory_allocated,
+	.memory_pressure	= &tcp_memory_pressure,
+	.sysctl_mem		= sysctl_tcp_mem,
+	.sysctl_wmem		= sysctl_tcp_wmem,
+	.sysctl_rmem		= sysctl_tcp_rmem,
+	.max_header		= MAX_TCP_HEADER,
+	.obj_size		= sizeof(struct tcp_sock),
+	.slab_flags		= SLAB_DESTROY_BY_RCU | RHEL_EXTENDED_PROTO,
+	.twsk_prot		= &tcp_timewait_sock_ops,
+	.rsk_prot		= &tcp_request_sock_ops,
+	.h.hashinfo		= &tcp_hashinfo,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt	= compat_tcp_setsockopt,
+	.compat_getsockopt	= compat_tcp_getsockopt,
+#endif
+	.rhel_flags		= RHEL_PROTO_HAS_RELEASE_CB,
+};
+
+
+static int __net_init tcp_sk_init(struct net *net)
+{
+	return inet_ctl_sock_create(&net->ipv4.tcp_sock,
+				    PF_INET, SOCK_RAW, IPPROTO_TCP, net);
+}
+
+static void __net_exit tcp_sk_exit(struct net *net)
+{
+	inet_ctl_sock_destroy(net->ipv4.tcp_sock);
+	inet_twsk_purge(net, &tcp_hashinfo, &tcp_death_row, AF_INET);
+}
+
+static struct pernet_operations __net_initdata tcp_sk_ops = {
+       .init = tcp_sk_init,
+       .exit = tcp_sk_exit,
+};
+
+void __init tcp_v4_init(void)
+{
+	inet_hashinfo_init(&tcp_hashinfo);
+	if (register_pernet_subsys(&tcp_sk_ops))
+		panic("Failed to create the TCP control socket.\n");
+}
+
+EXPORT_SYMBOL(ipv4_specific);
+EXPORT_SYMBOL(tcp_hashinfo);
+EXPORT_SYMBOL(tcp_prot);
+EXPORT_SYMBOL(tcp_v4_conn_request);
+EXPORT_SYMBOL(tcp_v4_connect);
+EXPORT_SYMBOL(tcp_v4_do_rcv);
+EXPORT_SYMBOL(tcp_v4_remember_stamp);
+EXPORT_SYMBOL(tcp_v4_send_check);
+EXPORT_SYMBOL(tcp_v4_syn_recv_sock);
+
+#ifdef CONFIG_PROC_FS
+EXPORT_SYMBOL(tcp_proc_register);
+EXPORT_SYMBOL(tcp_proc_unregister);
+#endif
+EXPORT_SYMBOL(sysctl_tcp_low_latency);
+
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/af_inet6.c linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/af_inet6.c
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/af_inet6.c	2016-12-13 17:21:59.485074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/af_inet6.c	2016-12-13 17:25:15.714067919 +0800
@@ -509,7 +509,7 @@
 
 EXPORT_SYMBOL(inet6_ioctl);
 
-const struct proto_ops inet6_stream_ops = {
+struct proto_ops inet6_stream_ops = {
 	.family		   = PF_INET6,
 	.owner		   = THIS_MODULE,
 	.release	   = inet6_release,
@@ -534,6 +534,7 @@
 	.compat_getsockopt = compat_sock_common_getsockopt,
 #endif
 };
+EXPORT_SYMBOL(inet6_stream_ops);
 
 const struct proto_ops inet6_dgram_ops = {
 	.family		   = PF_INET6,
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/af_inet6.c.orig linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/af_inet6.c.orig
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/af_inet6.c.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/af_inet6.c.orig	2016-12-13 17:23:53.995074270 +0800
@@ -0,0 +1,1078 @@
+/*
+ *	PF_INET6 socket protocol family
+ *	Linux INET6 implementation
+ *
+ *	Authors:
+ *	Pedro Roque		<roque@di.fc.ul.pt>
+ *
+ *	Adapted from linux/net/ipv4/af_inet.c
+ *
+ * 	Fixes:
+ *	piggy, Karl Knutson	:	Socket protocol table
+ * 	Hideaki YOSHIFUJI	:	sin6_scope_id support
+ * 	Arnaldo Melo		: 	check proc_net_create return, cleanups
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+
+#include <linux/module.h>
+#include <linux/capability.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/kernel.h>
+#include <linux/timer.h>
+#include <linux/string.h>
+#include <linux/sockios.h>
+#include <linux/net.h>
+#include <linux/fcntl.h>
+#include <linux/mm.h>
+#include <linux/interrupt.h>
+#include <linux/proc_fs.h>
+#include <linux/stat.h>
+#include <linux/init.h>
+
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/icmpv6.h>
+#include <linux/netfilter_ipv6.h>
+
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/udp.h>
+#include <net/udplite.h>
+#include <net/tcp.h>
+#include <net/protocol.h>
+#include <net/inet_common.h>
+#include <net/route.h>
+#include <net/transp_v6.h>
+#include <net/ip6_route.h>
+#include <net/addrconf.h>
+#ifdef CONFIG_IPV6_TUNNEL
+#include <net/ip6_tunnel.h>
+#endif
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+#include <linux/mroute6.h>
+
+MODULE_AUTHOR("Cast of dozens");
+MODULE_DESCRIPTION("IPv6 protocol stack for Linux");
+MODULE_LICENSE("GPL");
+
+/* The inetsw6 table contains everything that inet6_create needs to
+ * build a new socket.
+ */
+static struct list_head inetsw6[SOCK_MAX];
+static DEFINE_SPINLOCK(inetsw6_lock);
+
+struct ipv6_params ipv6_defaults = {
+	.disable_ipv6 = 0,
+	.autoconf = 1,
+};
+
+static int disable_ipv6_mod = 0;
+
+module_param_named(disable, disable_ipv6_mod, int, 0444);
+MODULE_PARM_DESC(disable, "Disable IPv6 module such that it is non-functional");
+
+module_param_named(disable_ipv6, ipv6_defaults.disable_ipv6, int, 0444);
+MODULE_PARM_DESC(disable_ipv6, "Disable IPv6 on all interfaces");
+
+module_param_named(autoconf, ipv6_defaults.autoconf, int, 0444);
+MODULE_PARM_DESC(autoconf, "Enable IPv6 address autoconfiguration on all interfaces");
+
+static __inline__ struct ipv6_pinfo *inet6_sk_generic(struct sock *sk)
+{
+	const int offset = sk->sk_prot->obj_size - sizeof(struct ipv6_pinfo);
+
+	return (struct ipv6_pinfo *)(((u8 *)sk) + offset);
+}
+
+static int inet6_create(struct net *net, struct socket *sock, int protocol,
+			int kern)
+{
+	struct inet_sock *inet;
+	struct ipv6_pinfo *np;
+	struct sock *sk;
+	struct inet_protosw *answer;
+	struct proto *answer_prot;
+	unsigned char answer_flags;
+	char answer_no_check;
+	int try_loading_module = 0;
+	int err;
+
+	if (protocol < 0 || protocol >= IPPROTO_MAX)
+		return -EINVAL;
+
+	if (sock->type != SOCK_RAW &&
+	    sock->type != SOCK_DGRAM &&
+	    !inet_ehash_secret)
+		build_ehash_secret();
+
+	/* Look for the requested type/protocol pair. */
+lookup_protocol:
+	err = -ESOCKTNOSUPPORT;
+	rcu_read_lock();
+	list_for_each_entry_rcu(answer, &inetsw6[sock->type], list) {
+
+		err = 0;
+		/* Check the non-wild match. */
+		if (protocol == answer->protocol) {
+			if (protocol != IPPROTO_IP)
+				break;
+		} else {
+			/* Check for the two wild cases. */
+			if (IPPROTO_IP == protocol) {
+				protocol = answer->protocol;
+				break;
+			}
+			if (IPPROTO_IP == answer->protocol)
+				break;
+		}
+		err = -EPROTONOSUPPORT;
+	}
+
+	if (err) {
+		if (try_loading_module < 2) {
+			rcu_read_unlock();
+			/*
+			 * Be more specific, e.g. net-pf-10-proto-132-type-1
+			 * (net-pf-PF_INET6-proto-IPPROTO_SCTP-type-SOCK_STREAM)
+			 */
+			if (++try_loading_module == 1)
+				request_module("net-pf-%d-proto-%d-type-%d",
+						PF_INET6, protocol, sock->type);
+			/*
+			 * Fall back to generic, e.g. net-pf-10-proto-132
+			 * (net-pf-PF_INET6-proto-IPPROTO_SCTP)
+			 */
+			else
+				request_module("net-pf-%d-proto-%d",
+						PF_INET6, protocol);
+			goto lookup_protocol;
+		} else
+			goto out_rcu_unlock;
+	}
+
+	err = -EPERM;
+	if (sock->type == SOCK_RAW && !kern && !capable(CAP_NET_RAW))
+		goto out_rcu_unlock;
+
+	sock->ops = answer->ops;
+	answer_prot = answer->prot;
+	answer_no_check = answer->no_check;
+	answer_flags = answer->flags;
+	rcu_read_unlock();
+
+	WARN_ON(answer_prot->slab == NULL);
+
+	err = -ENOBUFS;
+	sk = sk_alloc(net, PF_INET6, GFP_KERNEL, answer_prot);
+	if (sk == NULL)
+		goto out;
+
+	sock_init_data(sock, sk);
+
+	err = 0;
+	sk->sk_no_check = answer_no_check;
+	if (INET_PROTOSW_REUSE & answer_flags)
+		sk->sk_reuse = 1;
+
+	inet = inet_sk(sk);
+	inet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;
+
+	if (SOCK_RAW == sock->type) {
+		inet->num = protocol;
+		if (IPPROTO_RAW == protocol)
+			inet->hdrincl = 1;
+	}
+
+	sk->sk_destruct		= inet_sock_destruct;
+	sk->sk_family		= PF_INET6;
+	sk->sk_protocol		= protocol;
+
+	sk->sk_backlog_rcv	= answer->prot->backlog_rcv;
+
+	inet_sk(sk)->pinet6 = np = inet6_sk_generic(sk);
+	np->hop_limit	= -1;
+	np->mcast_hops	= IPV6_DEFAULT_MCASTHOPS;
+	np->mc_loop	= 1;
+	np->pmtudisc	= IPV6_PMTUDISC_WANT;
+	np->ipv6only	= net->ipv6.sysctl.bindv6only;
+
+	/* Init the ipv4 part of the socket since we can have sockets
+	 * using v6 API for ipv4.
+	 */
+	inet->uc_ttl	= -1;
+
+	inet->mc_loop	= 1;
+	inet->mc_ttl	= 1;
+	inet->mc_index	= 0;
+	inet->mc_list	= NULL;
+	sk_extended(sk)->rcv_tos = 0;
+
+	if (net->sysctl_ip_no_pmtu_disc)
+		inet->pmtudisc = IP_PMTUDISC_DONT;
+	else
+		inet->pmtudisc = IP_PMTUDISC_WANT;
+	/*
+	 * Increment only the relevant sk_prot->socks debug field, this changes
+	 * the previous behaviour of incrementing both the equivalent to
+	 * answer->prot->socks (inet6_sock_nr) and inet_sock_nr.
+	 *
+	 * This allows better debug granularity as we'll know exactly how many
+	 * UDPv6, TCPv6, etc socks were allocated, not the sum of all IPv6
+	 * transport protocol socks. -acme
+	 */
+	sk_refcnt_debug_inc(sk);
+
+	if (inet->num) {
+		/* It assumes that any protocol which allows
+		 * the user to assign a number at socket
+		 * creation time automatically shares.
+		 */
+		inet->sport = htons(inet->num);
+		sk->sk_prot->hash(sk);
+	}
+	if (sk->sk_prot->init) {
+		err = sk->sk_prot->init(sk);
+		if (err) {
+			sk_common_release(sk);
+			goto out;
+		}
+	}
+out:
+	return err;
+out_rcu_unlock:
+	rcu_read_unlock();
+	goto out;
+}
+
+
+/* bind for INET6 API */
+int inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
+{
+	struct sockaddr_in6 *addr=(struct sockaddr_in6 *)uaddr;
+	struct sock *sk = sock->sk;
+	struct inet_sock *inet = inet_sk(sk);
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct net *net = sock_net(sk);
+	__be32 v4addr = 0;
+	unsigned short snum;
+	int addr_type = 0;
+	int err = 0;
+
+	/* If the socket has its own bind function then use it. */
+	if (sk->sk_prot->bind)
+		return sk->sk_prot->bind(sk, uaddr, addr_len);
+
+	if (addr_len < SIN6_LEN_RFC2133)
+		return -EINVAL;
+	addr_type = ipv6_addr_type(&addr->sin6_addr);
+	if ((addr_type & IPV6_ADDR_MULTICAST) && sock->type == SOCK_STREAM)
+		return -EINVAL;
+
+	snum = ntohs(addr->sin6_port);
+	if (snum && snum < PROT_SOCK && !capable(CAP_NET_BIND_SERVICE))
+		return -EACCES;
+
+	lock_sock(sk);
+
+	/* Check these errors (active socket, double bind). */
+	if (sk->sk_state != TCP_CLOSE || inet->num) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* Check if the address belongs to the host. */
+	if (addr_type == IPV6_ADDR_MAPPED) {
+		int chk_addr_ret;
+
+		/* Binding to v4-mapped address on a v6-only socket
+		 * makes no sense
+		 */
+		if (np->ipv6only) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		/* Reproduce AF_INET checks to make the bindings consitant */
+		v4addr = addr->sin6_addr.s6_addr32[3];
+		chk_addr_ret = inet_addr_type(net, v4addr);
+		if (!sysctl_ip_nonlocal_bind &&
+		    !(inet->freebind || inet->transparent) &&
+		    v4addr != htonl(INADDR_ANY) &&
+		    chk_addr_ret != RTN_LOCAL &&
+		    chk_addr_ret != RTN_MULTICAST &&
+		    chk_addr_ret != RTN_BROADCAST) {
+			err = -EADDRNOTAVAIL;
+			goto out;
+		}
+	} else {
+		if (addr_type != IPV6_ADDR_ANY) {
+			struct net_device *dev = NULL;
+
+			if (addr_type & IPV6_ADDR_LINKLOCAL) {
+				if (addr_len >= sizeof(struct sockaddr_in6) &&
+				    addr->sin6_scope_id) {
+					/* Override any existing binding, if another one
+					 * is supplied by user.
+					 */
+					sk->sk_bound_dev_if = addr->sin6_scope_id;
+				}
+
+				/* Binding to link-local address requires an interface */
+				if (!sk->sk_bound_dev_if) {
+					err = -EINVAL;
+					goto out;
+				}
+				dev = dev_get_by_index(net, sk->sk_bound_dev_if);
+				if (!dev) {
+					err = -ENODEV;
+					goto out;
+				}
+			}
+
+			/* ipv4 addr of the socket is invalid.  Only the
+			 * unspecified and mapped address have a v4 equivalent.
+			 */
+			v4addr = LOOPBACK4_IPV6;
+			if (!(addr_type & IPV6_ADDR_MULTICAST))	{
+				if (!inet->transparent &&
+				    !ipv6_chk_addr(net, &addr->sin6_addr,
+						   dev, 0)) {
+					if (dev)
+						dev_put(dev);
+					err = -EADDRNOTAVAIL;
+					goto out;
+				}
+			}
+			if (dev)
+				dev_put(dev);
+		}
+	}
+
+	inet->rcv_saddr = v4addr;
+	inet->saddr = v4addr;
+
+	ipv6_addr_copy(&np->rcv_saddr, &addr->sin6_addr);
+
+	if (!(addr_type & IPV6_ADDR_MULTICAST))
+		ipv6_addr_copy(&np->saddr, &addr->sin6_addr);
+
+	/* Make sure we are allowed to bind here. */
+	if (sk->sk_prot->get_port(sk, snum)) {
+		inet_reset_saddr(sk);
+		err = -EADDRINUSE;
+		goto out;
+	}
+
+	if (addr_type != IPV6_ADDR_ANY) {
+		sk->sk_userlocks |= SOCK_BINDADDR_LOCK;
+		if (addr_type != IPV6_ADDR_MAPPED)
+			np->ipv6only = 1;
+	}
+	if (snum)
+		sk->sk_userlocks |= SOCK_BINDPORT_LOCK;
+	inet->sport = htons(inet->num);
+	inet->dport = 0;
+	inet->daddr = 0;
+out:
+	release_sock(sk);
+	return err;
+}
+
+EXPORT_SYMBOL(inet6_bind);
+
+int inet6_release(struct socket *sock)
+{
+	struct sock *sk = sock->sk;
+
+	if (sk == NULL)
+		return -EINVAL;
+
+	/* Free mc lists */
+	ipv6_sock_mc_close(sk);
+
+	/* Free ac lists */
+	ipv6_sock_ac_close(sk);
+
+	return inet_release(sock);
+}
+
+EXPORT_SYMBOL(inet6_release);
+
+void inet6_destroy_sock(struct sock *sk)
+{
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct sk_buff *skb;
+	struct ipv6_txoptions *opt;
+
+	/* Release rx options */
+
+	if ((skb = xchg(&np->pktoptions, NULL)) != NULL)
+		kfree_skb(skb);
+
+	/* Free flowlabels */
+	fl6_free_socklist(sk);
+
+	/* Free tx options */
+
+	opt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);
+	if (opt) {
+		atomic_sub(opt->tot_len, &sk->sk_omem_alloc);
+		txopt_put(opt);
+	}
+}
+
+EXPORT_SYMBOL_GPL(inet6_destroy_sock);
+
+/*
+ *	This does both peername and sockname.
+ */
+
+int inet6_getname(struct socket *sock, struct sockaddr *uaddr,
+		 int *uaddr_len, int peer)
+{
+	struct sockaddr_in6 *sin=(struct sockaddr_in6 *)uaddr;
+	struct sock *sk = sock->sk;
+	struct inet_sock *inet = inet_sk(sk);
+	struct ipv6_pinfo *np = inet6_sk(sk);
+
+	sin->sin6_family = AF_INET6;
+	sin->sin6_flowinfo = 0;
+	sin->sin6_scope_id = 0;
+	if (peer) {
+		if (!inet->dport)
+			return -ENOTCONN;
+		if (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&
+		    peer == 1)
+			return -ENOTCONN;
+		sin->sin6_port = inet->dport;
+		ipv6_addr_copy(&sin->sin6_addr, &np->daddr);
+		if (np->sndflow)
+			sin->sin6_flowinfo = np->flow_label;
+	} else {
+		if (ipv6_addr_any(&np->rcv_saddr))
+			ipv6_addr_copy(&sin->sin6_addr, &np->saddr);
+		else
+			ipv6_addr_copy(&sin->sin6_addr, &np->rcv_saddr);
+
+		sin->sin6_port = inet->sport;
+	}
+	if (ipv6_addr_type(&sin->sin6_addr) & IPV6_ADDR_LINKLOCAL)
+		sin->sin6_scope_id = sk->sk_bound_dev_if;
+	*uaddr_len = sizeof(*sin);
+	return(0);
+}
+
+EXPORT_SYMBOL(inet6_getname);
+
+int inet6_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)
+{
+	struct sock *sk = sock->sk;
+	struct net *net = sock_net(sk);
+
+	switch(cmd)
+	{
+	case SIOCGSTAMP:
+		return sock_get_timestamp(sk, (struct timeval __user *)arg);
+
+	case SIOCGSTAMPNS:
+		return sock_get_timestampns(sk, (struct timespec __user *)arg);
+
+	case SIOCADDRT:
+	case SIOCDELRT:
+
+		return(ipv6_route_ioctl(net, cmd, (void __user *)arg));
+
+	case SIOCSIFADDR:
+		return addrconf_add_ifaddr(net, (void __user *) arg);
+	case SIOCDIFADDR:
+		return addrconf_del_ifaddr(net, (void __user *) arg);
+	case SIOCSIFDSTADDR:
+		return addrconf_set_dstaddr(net, (void __user *) arg);
+	default:
+		if (!sk->sk_prot->ioctl)
+			return -ENOIOCTLCMD;
+		return sk->sk_prot->ioctl(sk, cmd, arg);
+	}
+	/*NOTREACHED*/
+	return(0);
+}
+
+EXPORT_SYMBOL(inet6_ioctl);
+
+const struct proto_ops inet6_stream_ops = {
+	.family		   = PF_INET6,
+	.owner		   = THIS_MODULE,
+	.release	   = inet6_release,
+	.bind		   = inet6_bind,
+	.connect	   = inet_stream_connect,	/* ok		*/
+	.socketpair	   = sock_no_socketpair,	/* a do nothing	*/
+	.accept		   = inet_accept,		/* ok		*/
+	.getname	   = inet6_getname,
+	.poll		   = tcp_poll,			/* ok		*/
+	.ioctl		   = inet6_ioctl,		/* must change  */
+	.listen		   = inet_listen,		/* ok		*/
+	.shutdown	   = inet_shutdown,		/* ok		*/
+	.setsockopt	   = sock_common_setsockopt,	/* ok		*/
+	.getsockopt	   = sock_common_getsockopt,	/* ok		*/
+	.sendmsg	   = tcp_sendmsg,		/* ok		*/
+	.recvmsg	   = sock_common_recvmsg,	/* ok		*/
+	.mmap		   = sock_no_mmap,
+	.sendpage	   = tcp_sendpage,
+	.splice_read	   = tcp_splice_read,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_sock_common_setsockopt,
+	.compat_getsockopt = compat_sock_common_getsockopt,
+#endif
+};
+
+const struct proto_ops inet6_dgram_ops = {
+	.family		   = PF_INET6,
+	.owner		   = THIS_MODULE,
+	.release	   = inet6_release,
+	.bind		   = inet6_bind,
+	.connect	   = inet_dgram_connect,	/* ok		*/
+	.socketpair	   = sock_no_socketpair,	/* a do nothing	*/
+	.accept		   = sock_no_accept,		/* a do nothing	*/
+	.getname	   = inet6_getname,
+	.poll		   = udp_poll,			/* ok		*/
+	.ioctl		   = inet6_ioctl,		/* must change  */
+	.listen		   = sock_no_listen,		/* ok		*/
+	.shutdown	   = inet_shutdown,		/* ok		*/
+	.setsockopt	   = sock_common_setsockopt,	/* ok		*/
+	.getsockopt	   = sock_common_getsockopt,	/* ok		*/
+	.sendmsg	   = inet_sendmsg,		/* ok		*/
+	.recvmsg	   = sock_common_recvmsg,	/* ok		*/
+	.mmap		   = sock_no_mmap,
+	.sendpage	   = sock_no_sendpage,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_sock_common_setsockopt,
+	.compat_getsockopt = compat_sock_common_getsockopt,
+#endif
+};
+
+static struct net_proto_family inet6_family_ops = {
+	.family = PF_INET6,
+	.create = inet6_create,
+	.owner	= THIS_MODULE,
+};
+
+int inet6_register_protosw(struct inet_protosw *p)
+{
+	struct list_head *lh;
+	struct inet_protosw *answer;
+	struct list_head *last_perm;
+	int protocol = p->protocol;
+	int ret;
+
+	spin_lock_bh(&inetsw6_lock);
+
+	ret = -EINVAL;
+	if (p->type >= SOCK_MAX)
+		goto out_illegal;
+
+	/* If we are trying to override a permanent protocol, bail. */
+	answer = NULL;
+	ret = -EPERM;
+	last_perm = &inetsw6[p->type];
+	list_for_each(lh, &inetsw6[p->type]) {
+		answer = list_entry(lh, struct inet_protosw, list);
+
+		/* Check only the non-wild match. */
+		if (INET_PROTOSW_PERMANENT & answer->flags) {
+			if (protocol == answer->protocol)
+				break;
+			last_perm = lh;
+		}
+
+		answer = NULL;
+	}
+	if (answer)
+		goto out_permanent;
+
+	/* Add the new entry after the last permanent entry if any, so that
+	 * the new entry does not override a permanent entry when matched with
+	 * a wild-card protocol. But it is allowed to override any existing
+	 * non-permanent entry.  This means that when we remove this entry, the
+	 * system automatically returns to the old behavior.
+	 */
+	list_add_rcu(&p->list, last_perm);
+	ret = 0;
+out:
+	spin_unlock_bh(&inetsw6_lock);
+	return ret;
+
+out_permanent:
+	printk(KERN_ERR "Attempt to override permanent protocol %d.\n",
+	       protocol);
+	goto out;
+
+out_illegal:
+	printk(KERN_ERR
+	       "Ignoring attempt to register invalid socket type %d.\n",
+	       p->type);
+	goto out;
+}
+
+EXPORT_SYMBOL(inet6_register_protosw);
+
+void
+inet6_unregister_protosw(struct inet_protosw *p)
+{
+	if (INET_PROTOSW_PERMANENT & p->flags) {
+		printk(KERN_ERR
+		       "Attempt to unregister permanent protocol %d.\n",
+		       p->protocol);
+	} else {
+		spin_lock_bh(&inetsw6_lock);
+		list_del_rcu(&p->list);
+		spin_unlock_bh(&inetsw6_lock);
+
+		synchronize_net();
+	}
+}
+
+EXPORT_SYMBOL(inet6_unregister_protosw);
+
+int inet6_sk_rebuild_header(struct sock *sk)
+{
+	int err;
+	struct dst_entry *dst;
+	struct ipv6_pinfo *np = inet6_sk(sk);
+
+	dst = __sk_dst_check(sk, np->dst_cookie);
+
+	if (dst == NULL) {
+		struct inet_sock *inet = inet_sk(sk);
+		struct in6_addr *final_p, final;
+		struct flowi fl;
+
+		memset(&fl, 0, sizeof(fl));
+		fl.proto = sk->sk_protocol;
+		ipv6_addr_copy(&fl.fl6_dst, &np->daddr);
+		ipv6_addr_copy(&fl.fl6_src, &np->saddr);
+		fl.fl6_flowlabel = np->flow_label;
+		fl.oif = sk->sk_bound_dev_if;
+		fl.fl_ip_dport = inet->dport;
+		fl.fl_ip_sport = inet->sport;
+		security_sk_classify_flow(sk, &fl);
+
+		rcu_read_lock();
+		final_p = fl6_update_dst(&fl, rcu_dereference(np->opt),
+					 &final);
+		rcu_read_unlock();
+
+		err = ip6_dst_lookup(sk, &dst, &fl);
+		if (err) {
+			sk->sk_route_caps = 0;
+			return err;
+		}
+		if (final_p)
+			ipv6_addr_copy(&fl.fl6_dst, final_p);
+
+		if ((err = xfrm_lookup(sock_net(sk), &dst, &fl, sk, 0)) < 0) {
+			sk->sk_err_soft = -err;
+			return err;
+		}
+
+		__ip6_dst_store(sk, dst, NULL, NULL);
+	}
+
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(inet6_sk_rebuild_header);
+
+int ipv6_opt_accepted(struct sock *sk, struct sk_buff *skb)
+{
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct inet6_skb_parm *opt = IP6CB(skb);
+
+	if (np->rxopt.all) {
+		if ((opt->hop && (np->rxopt.bits.hopopts ||
+				  np->rxopt.bits.ohopopts)) ||
+		    ((IPV6_FLOWINFO_MASK &
+		      *(__be32 *)skb_network_header(skb)) &&
+		     np->rxopt.bits.rxflow) ||
+		    (opt->srcrt && (np->rxopt.bits.srcrt ||
+		     np->rxopt.bits.osrcrt)) ||
+		    ((opt->dst1 || opt->dst0) &&
+		     (np->rxopt.bits.dstopts || np->rxopt.bits.odstopts)))
+			return 1;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(ipv6_opt_accepted);
+
+static struct packet_type ipv6_packet_type __read_mostly = {
+	.type = cpu_to_be16(ETH_P_IPV6),
+	.func = ipv6_rcv,
+};
+
+static int __init ipv6_packet_init(void)
+{
+	dev_add_pack(&ipv6_packet_type);
+	return 0;
+}
+
+static void ipv6_packet_cleanup(void)
+{
+	dev_remove_pack(&ipv6_packet_type);
+}
+
+static int __net_init ipv6_init_mibs(struct net *net)
+{
+	if (snmp_mib_init((void **)net->mib.udp_stats_in6,
+			  sizeof (struct udp_mib)) < 0)
+		return -ENOMEM;
+	if (snmp_mib_init((void **)net->mib.udplite_stats_in6,
+			  sizeof (struct udp_mib)) < 0)
+		goto err_udplite_mib;
+	if (snmp_mib_init((void **)net->mib.ipv6_statistics,
+			  sizeof(struct ipstats_mib)) < 0)
+		goto err_ip_mib;
+	if (snmp_mib_init((void **)net->mib.icmpv6_statistics,
+			  sizeof(struct icmpv6_mib)) < 0)
+		goto err_icmp_mib;
+	if (snmp_mib_init((void **)net->mib.icmpv6msg_statistics,
+			  sizeof(struct icmpv6msg_mib)) < 0)
+		goto err_icmpmsg_mib;
+	return 0;
+
+err_icmpmsg_mib:
+	snmp_mib_free((void **)net->mib.icmpv6_statistics);
+err_icmp_mib:
+	snmp_mib_free((void **)net->mib.ipv6_statistics);
+err_ip_mib:
+	snmp_mib_free((void **)net->mib.udplite_stats_in6);
+err_udplite_mib:
+	snmp_mib_free((void **)net->mib.udp_stats_in6);
+	return -ENOMEM;
+}
+
+static void __net_exit ipv6_cleanup_mibs(struct net *net)
+{
+	snmp_mib_free((void **)net->mib.udp_stats_in6);
+	snmp_mib_free((void **)net->mib.udplite_stats_in6);
+	snmp_mib_free((void **)net->mib.ipv6_statistics);
+	snmp_mib_free((void **)net->mib.icmpv6_statistics);
+	snmp_mib_free((void **)net->mib.icmpv6msg_statistics);
+}
+
+static int __net_init inet6_net_init(struct net *net)
+{
+	int err = 0;
+
+	net->ipv6.sysctl.bindv6only = 0;
+	net->ipv6.sysctl.icmpv6_time = 1*HZ;
+
+	err = ipv6_init_mibs(net);
+	if (err)
+		return err;
+#ifdef CONFIG_PROC_FS
+	err = udp6_proc_init(net);
+	if (err)
+		goto out;
+	err = tcp6_proc_init(net);
+	if (err)
+		goto proc_tcp6_fail;
+	err = ac6_proc_init(net);
+	if (err)
+		goto proc_ac6_fail;
+#endif
+	return err;
+
+#ifdef CONFIG_PROC_FS
+proc_ac6_fail:
+	tcp6_proc_exit(net);
+proc_tcp6_fail:
+	udp6_proc_exit(net);
+out:
+	ipv6_cleanup_mibs(net);
+	return err;
+#endif
+}
+
+static void inet6_net_exit(struct net *net)
+{
+#ifdef CONFIG_PROC_FS
+	udp6_proc_exit(net);
+	tcp6_proc_exit(net);
+	ac6_proc_exit(net);
+#endif
+	ipv6_cleanup_mibs(net);
+}
+
+static struct pernet_operations inet6_net_ops = {
+	.init = inet6_net_init,
+	.exit = inet6_net_exit,
+};
+
+static int __init inet6_init(void)
+{
+	struct sk_buff *dummy_skb;
+	struct list_head *r;
+	int err = 0;
+
+	BUILD_BUG_ON(sizeof(struct inet6_skb_parm) > sizeof(dummy_skb->cb));
+
+	/* Register the socket-side information for inet6_create.  */
+	for(r = &inetsw6[0]; r < &inetsw6[SOCK_MAX]; ++r)
+		INIT_LIST_HEAD(r);
+
+	if (disable_ipv6_mod) {
+		printk(KERN_INFO
+		       "IPv6: Loaded, but administratively disabled, "
+		       "reboot required to enable\n");
+		goto out;
+	}
+
+	err = proto_register(&tcpv6_prot, 1);
+	if (err)
+		goto out;
+
+	err = proto_register(&udpv6_prot, 1);
+	if (err)
+		goto out_unregister_tcp_proto;
+
+	err = proto_register(&udplitev6_prot, 1);
+	if (err)
+		goto out_unregister_udp_proto;
+
+	err = proto_register(&rawv6_prot, 1);
+	if (err)
+		goto out_unregister_udplite_proto;
+
+
+	/* We MUST register RAW sockets before we create the ICMP6,
+	 * IGMP6, or NDISC control sockets.
+	 */
+	err = rawv6_init();
+	if (err)
+		goto out_unregister_raw_proto;
+
+	/* Register the family here so that the init calls below will
+	 * be able to create sockets. (?? is this dangerous ??)
+	 */
+	err = sock_register(&inet6_family_ops);
+	if (err)
+		goto out_sock_register_fail;
+
+#ifdef CONFIG_SYSCTL
+	err = ipv6_static_sysctl_register();
+	if (err)
+		goto static_sysctl_fail;
+#endif
+	/*
+	 *	ipngwg API draft makes clear that the correct semantics
+	 *	for TCP and UDP is to consider one TCP and UDP instance
+	 *	in a host availiable by both INET and INET6 APIs and
+	 *	able to communicate via both network protocols.
+	 */
+
+	err = register_pernet_subsys(&inet6_net_ops);
+	if (err)
+		goto register_pernet_fail;
+	err = icmpv6_init();
+	if (err)
+		goto icmp_fail;
+	err = ip6_mr_init();
+	if (err)
+		goto ipmr_fail;
+	err = ndisc_init();
+	if (err)
+		goto ndisc_fail;
+	err = igmp6_init();
+	if (err)
+		goto igmp_fail;
+	err = ipv6_netfilter_init();
+	if (err)
+		goto netfilter_fail;
+	/* Create /proc/foo6 entries. */
+#ifdef CONFIG_PROC_FS
+	err = -ENOMEM;
+	if (raw6_proc_init())
+		goto proc_raw6_fail;
+	if (udplite6_proc_init())
+		goto proc_udplite6_fail;
+	if (ipv6_misc_proc_init())
+		goto proc_misc6_fail;
+	if (if6_proc_init())
+		goto proc_if6_fail;
+#endif
+	err = ip6_route_init();
+	if (err)
+		goto ip6_route_fail;
+	err = ndisc_late_init();
+	if (err)
+		goto ndisc_late_fail;
+	err = ip6_flowlabel_init();
+	if (err)
+		goto ip6_flowlabel_fail;
+	err = addrconf_init();
+	if (err)
+		goto addrconf_fail;
+
+	/* Init v6 extension headers. */
+	err = ipv6_exthdrs_init();
+	if (err)
+		goto ipv6_exthdrs_fail;
+
+	err = ipv6_frag_init();
+	if (err)
+		goto ipv6_frag_fail;
+
+	/* Init v6 transport protocols. */
+	err = udpv6_init();
+	if (err)
+		goto udpv6_fail;
+
+	err = udplitev6_init();
+	if (err)
+		goto udplitev6_fail;
+
+	err = tcpv6_init();
+	if (err)
+		goto tcpv6_fail;
+
+	err = ipv6_packet_init();
+	if (err)
+		goto ipv6_packet_fail;
+
+#ifdef CONFIG_SYSCTL
+	err = ipv6_sysctl_register();
+	if (err)
+		goto sysctl_fail;
+#endif
+out:
+	return err;
+
+#ifdef CONFIG_SYSCTL
+sysctl_fail:
+	ipv6_packet_cleanup();
+#endif
+ipv6_packet_fail:
+	tcpv6_exit();
+tcpv6_fail:
+	udplitev6_exit();
+udplitev6_fail:
+	udpv6_exit();
+udpv6_fail:
+	ipv6_frag_exit();
+ipv6_frag_fail:
+	ipv6_exthdrs_exit();
+ipv6_exthdrs_fail:
+	addrconf_cleanup();
+addrconf_fail:
+	ip6_flowlabel_cleanup();
+ip6_flowlabel_fail:
+	ndisc_late_cleanup();
+ndisc_late_fail:
+	ip6_route_cleanup();
+ip6_route_fail:
+#ifdef CONFIG_PROC_FS
+	if6_proc_exit();
+proc_if6_fail:
+	ipv6_misc_proc_exit();
+proc_misc6_fail:
+	udplite6_proc_exit();
+proc_udplite6_fail:
+	raw6_proc_exit();
+proc_raw6_fail:
+#endif
+	ipv6_netfilter_fini();
+netfilter_fail:
+	igmp6_cleanup();
+igmp_fail:
+	ndisc_cleanup();
+ndisc_fail:
+	ip6_mr_cleanup();
+ipmr_fail:
+	icmpv6_cleanup();
+icmp_fail:
+	unregister_pernet_subsys(&inet6_net_ops);
+register_pernet_fail:
+#ifdef CONFIG_SYSCTL
+	ipv6_static_sysctl_unregister();
+static_sysctl_fail:
+#endif
+	sock_unregister(PF_INET6);
+	rtnl_unregister_all(PF_INET6);
+out_sock_register_fail:
+	rawv6_exit();
+out_unregister_raw_proto:
+	proto_unregister(&rawv6_prot);
+out_unregister_udplite_proto:
+	proto_unregister(&udplitev6_prot);
+out_unregister_udp_proto:
+	proto_unregister(&udpv6_prot);
+out_unregister_tcp_proto:
+	proto_unregister(&tcpv6_prot);
+	goto out;
+}
+module_init(inet6_init);
+
+static void __exit inet6_exit(void)
+{
+	if (disable_ipv6_mod)
+		return;
+
+	/* First of all disallow new sockets creation. */
+	sock_unregister(PF_INET6);
+	/* Disallow any further netlink messages */
+	rtnl_unregister_all(PF_INET6);
+
+#ifdef CONFIG_SYSCTL
+	ipv6_sysctl_unregister();
+#endif
+	udpv6_exit();
+	udplitev6_exit();
+	tcpv6_exit();
+
+	/* Cleanup code parts. */
+	ipv6_packet_cleanup();
+	ipv6_frag_exit();
+	ipv6_exthdrs_exit();
+	addrconf_cleanup();
+	ip6_flowlabel_cleanup();
+	ndisc_late_cleanup();
+	ip6_route_cleanup();
+#ifdef CONFIG_PROC_FS
+
+	/* Cleanup code parts. */
+	if6_proc_exit();
+	ipv6_misc_proc_exit();
+	udplite6_proc_exit();
+	raw6_proc_exit();
+#endif
+	ipv6_netfilter_fini();
+	igmp6_cleanup();
+	ndisc_cleanup();
+	ip6_mr_cleanup();
+	icmpv6_cleanup();
+	rawv6_exit();
+
+	unregister_pernet_subsys(&inet6_net_ops);
+#ifdef CONFIG_SYSCTL
+	ipv6_static_sysctl_unregister();
+#endif
+	proto_unregister(&rawv6_prot);
+	proto_unregister(&udplitev6_prot);
+	proto_unregister(&udpv6_prot);
+	proto_unregister(&tcpv6_prot);
+
+	rcu_barrier(); /* Wait for completion of call_rcu()'s */
+}
+module_exit(inet6_exit);
+
+MODULE_ALIAS_NETPROTO(PF_INET6);
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/tcp_ipv6.c linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/tcp_ipv6.c
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/tcp_ipv6.c	2016-12-13 17:21:59.491074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/tcp_ipv6.c	2016-12-13 17:25:15.714067919 +0800
@@ -78,8 +78,8 @@
 
 static int	tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
 
-static const struct inet_connection_sock_af_ops ipv6_mapped;
-static const struct inet_connection_sock_af_ops ipv6_specific;
+struct inet_connection_sock_af_ops ipv6_mapped;
+struct inet_connection_sock_af_ops ipv6_specific;
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_specific;
 static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;
@@ -1202,7 +1202,7 @@
 	return 0; /* don't send reset */
 }
 
-static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 					  struct request_sock *req,
 					  struct dst_entry *dst)
 {
@@ -1409,6 +1409,7 @@
 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
 	return NULL;
 }
+EXPORT_SYMBOL(tcp_v6_syn_recv_sock);
 
 static __sum16 tcp_v6_checksum_init(struct sk_buff *skb)
 {
@@ -1731,7 +1732,7 @@
 	return 0;
 }
 
-static const struct inet_connection_sock_af_ops ipv6_specific = {
+struct inet_connection_sock_af_ops ipv6_specific = {
 	.queue_xmit	   = inet6_csk_xmit,
 	.send_check	   = tcp_v6_send_check,
 	.rebuild_header	   = inet6_sk_rebuild_header,
@@ -1749,6 +1750,7 @@
 	.compat_getsockopt = compat_ipv6_getsockopt,
 #endif
 };
+EXPORT_SYMBOL(ipv6_specific);
 
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {
@@ -1763,7 +1765,7 @@
  *	TCP over IPv4 via INET6 API
  */
 
-static const struct inet_connection_sock_af_ops ipv6_mapped = {
+struct inet_connection_sock_af_ops ipv6_mapped = {
 	.queue_xmit	   = ip_queue_xmit,
 	.send_check	   = tcp_v4_send_check,
 	.rebuild_header	   = inet_sk_rebuild_header,
@@ -1781,6 +1783,7 @@
 	.compat_getsockopt = compat_ipv6_getsockopt,
 #endif
 };
+EXPORT_SYMBOL(ipv6_mapped);
 
 #ifdef CONFIG_TCP_MD5SIG
 static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/tcp_ipv6.c.orig linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/tcp_ipv6.c.orig
--- linux-2.6.32-642.11.1.el6.x86_64/net/ipv6/tcp_ipv6.c.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/ipv6/tcp_ipv6.c.orig	2016-12-13 17:23:54.001074270 +0800
@@ -0,0 +1,2134 @@
+/*
+ *	TCP over IPv6
+ *	Linux INET6 implementation
+ *
+ *	Authors:
+ *	Pedro Roque		<roque@di.fc.ul.pt>
+ *
+ *	Based on:
+ *	linux/net/ipv4/tcp.c
+ *	linux/net/ipv4/tcp_input.c
+ *	linux/net/ipv4/tcp_output.c
+ *
+ *	Fixes:
+ *	Hideaki YOSHIFUJI	:	sin6_scope_id support
+ *	YOSHIFUJI Hideaki @USAGI and:	Support IPV6_V6ONLY socket option, which
+ *	Alexey Kuznetsov		allow both IPv4 and IPv6 sockets to bind
+ *					a single port at the same time.
+ *	YOSHIFUJI Hideaki @USAGI:	convert /proc/net/tcp6 to seq_file.
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/bottom_half.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/sockios.h>
+#include <linux/net.h>
+#include <linux/jiffies.h>
+#include <linux/in.h>
+#include <linux/in6.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/jhash.h>
+#include <linux/ipsec.h>
+#include <linux/times.h>
+
+#include <linux/ipv6.h>
+#include <linux/icmpv6.h>
+#include <linux/random.h>
+
+#include <net/tcp.h>
+#include <net/ndisc.h>
+#include <net/inet6_hashtables.h>
+#include <net/inet6_connection_sock.h>
+#include <net/ipv6.h>
+#include <net/transp_v6.h>
+#include <net/addrconf.h>
+#include <net/ip6_route.h>
+#include <net/ip6_checksum.h>
+#include <net/inet_ecn.h>
+#include <net/protocol.h>
+#include <net/xfrm.h>
+#include <net/snmp.h>
+#include <net/dsfield.h>
+#include <net/timewait_sock.h>
+#include <net/netdma.h>
+#include <net/inet_common.h>
+#include <net/secure_seq.h>
+#include <net/busy_poll.h>
+
+#include <asm/uaccess.h>
+
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <linux/crypto.h>
+#include <linux/scatterlist.h>
+#include "ip6_offload.h"
+
+static void	tcp_v6_send_reset(struct sock *sk, struct sk_buff *skb);
+static void	tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,
+				      struct request_sock *req);
+
+static int	tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
+
+static const struct inet_connection_sock_af_ops ipv6_mapped;
+static const struct inet_connection_sock_af_ops ipv6_specific;
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_sock_af_ops tcp_sock_ipv6_specific;
+static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;
+#else
+static struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,
+						   struct in6_addr *addr)
+{
+	return NULL;
+}
+#endif
+
+static void tcp_v6_hash(struct sock *sk)
+{
+	if (sk->sk_state != TCP_CLOSE) {
+		if (inet_csk(sk)->icsk_af_ops == &ipv6_mapped) {
+			tcp_prot.hash(sk);
+			return;
+		}
+		local_bh_disable();
+		__inet6_hash(sk, NULL);
+		local_bh_enable();
+	}
+}
+
+static __u32 tcp_v6_init_sequence(struct sk_buff *skb)
+{
+	return secure_tcpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,
+					    ipv6_hdr(skb)->saddr.s6_addr32,
+					    tcp_hdr(skb)->dest,
+					    tcp_hdr(skb)->source);
+}
+
+static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
+			  int addr_len)
+{
+	struct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;
+	struct inet_sock *inet = inet_sk(sk);
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct in6_addr *saddr = NULL, *final_p, final;
+	struct ipv6_txoptions *opt;
+	struct flowi fl;
+	struct dst_entry *dst;
+	int addr_type;
+	int err;
+
+	if (addr_len < SIN6_LEN_RFC2133)
+		return -EINVAL;
+
+	if (usin->sin6_family != AF_INET6)
+		return(-EAFNOSUPPORT);
+
+	memset(&fl, 0, sizeof(fl));
+
+	if (np->sndflow) {
+		fl.fl6_flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;
+		IP6_ECN_flow_init(fl.fl6_flowlabel);
+		if (fl.fl6_flowlabel&IPV6_FLOWLABEL_MASK) {
+			struct ip6_flowlabel *flowlabel;
+			flowlabel = fl6_sock_lookup(sk, fl.fl6_flowlabel);
+			if (flowlabel == NULL)
+				return -EINVAL;
+			ipv6_addr_copy(&usin->sin6_addr, &flowlabel->dst);
+			fl6_sock_release(flowlabel);
+		}
+	}
+
+	/*
+	 *	connect() to INADDR_ANY means loopback (BSD'ism).
+	 */
+
+	if(ipv6_addr_any(&usin->sin6_addr))
+		usin->sin6_addr.s6_addr[15] = 0x1;
+
+	addr_type = ipv6_addr_type(&usin->sin6_addr);
+
+	if(addr_type & IPV6_ADDR_MULTICAST)
+		return -ENETUNREACH;
+
+	if (addr_type&IPV6_ADDR_LINKLOCAL) {
+		if (addr_len >= sizeof(struct sockaddr_in6) &&
+		    usin->sin6_scope_id) {
+			/* If interface is set while binding, indices
+			 * must coincide.
+			 */
+			if (sk->sk_bound_dev_if &&
+			    sk->sk_bound_dev_if != usin->sin6_scope_id)
+				return -EINVAL;
+
+			sk->sk_bound_dev_if = usin->sin6_scope_id;
+		}
+
+		/* Connect to link-local address requires an interface */
+		if (!sk->sk_bound_dev_if)
+			return -EINVAL;
+	}
+
+	if (tp->rx_opt.ts_recent_stamp &&
+	    !ipv6_addr_equal(&np->daddr, &usin->sin6_addr)) {
+		tp->rx_opt.ts_recent = 0;
+		tp->rx_opt.ts_recent_stamp = 0;
+		tp->write_seq = 0;
+	}
+
+	ipv6_addr_copy(&np->daddr, &usin->sin6_addr);
+	np->flow_label = fl.fl6_flowlabel;
+
+	/*
+	 *	TCP over IPv4
+	 */
+
+	if (addr_type == IPV6_ADDR_MAPPED) {
+		u32 exthdrlen = icsk->icsk_ext_hdr_len;
+		struct sockaddr_in sin;
+
+		SOCK_DEBUG(sk, "connect: ipv4 mapped\n");
+
+		if (__ipv6_only_sock(sk))
+			return -ENETUNREACH;
+
+		sin.sin_family = AF_INET;
+		sin.sin_port = usin->sin6_port;
+		sin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];
+
+		icsk->icsk_af_ops = &ipv6_mapped;
+		sk->sk_backlog_rcv = tcp_v4_do_rcv;
+#ifdef CONFIG_TCP_MD5SIG
+		tp->af_specific = &tcp_sock_ipv6_mapped_specific;
+#endif
+
+		err = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));
+
+		if (err) {
+			icsk->icsk_ext_hdr_len = exthdrlen;
+			icsk->icsk_af_ops = &ipv6_specific;
+			sk->sk_backlog_rcv = tcp_v6_do_rcv;
+#ifdef CONFIG_TCP_MD5SIG
+			tp->af_specific = &tcp_sock_ipv6_specific;
+#endif
+			goto failure;
+		} else {
+			ipv6_addr_set(&np->saddr, 0, 0, htonl(0x0000FFFF),
+				      inet->saddr);
+			ipv6_addr_set(&np->rcv_saddr, 0, 0, htonl(0x0000FFFF),
+				      inet->rcv_saddr);
+		}
+
+		return err;
+	}
+
+	if (!ipv6_addr_any(&np->rcv_saddr))
+		saddr = &np->rcv_saddr;
+
+	fl.proto = IPPROTO_TCP;
+	ipv6_addr_copy(&fl.fl6_dst, &np->daddr);
+	ipv6_addr_copy(&fl.fl6_src,
+		       (saddr ? saddr : &np->saddr));
+	fl.oif = sk->sk_bound_dev_if;
+	fl.fl_ip_dport = usin->sin6_port;
+	fl.fl_ip_sport = inet->sport;
+
+	opt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));
+	final_p = fl6_update_dst(&fl, opt, &final);
+
+	security_sk_classify_flow(sk, &fl);
+
+	err = ip6_dst_lookup(sk, &dst, &fl);
+	if (err)
+		goto failure;
+	if (final_p)
+		ipv6_addr_copy(&fl.fl6_dst, final_p);
+
+	err = __xfrm_lookup(sock_net(sk), &dst, &fl, sk, XFRM_LOOKUP_WAIT);
+	if (err < 0) {
+		if (err == -EREMOTE)
+			err = ip6_dst_blackhole(sk, &dst, &fl);
+		if (err < 0)
+			goto failure;
+	}
+
+	if (saddr == NULL) {
+		saddr = &fl.fl6_src;
+		ipv6_addr_copy(&np->rcv_saddr, saddr);
+	}
+
+	/* set the source address */
+	ipv6_addr_copy(&np->saddr, saddr);
+	inet->rcv_saddr = LOOPBACK4_IPV6;
+
+	sk->sk_gso_type = SKB_GSO_TCPV6;
+	__ip6_dst_store(sk, dst, NULL, NULL);
+
+	icsk->icsk_ext_hdr_len = 0;
+	if (opt)
+		icsk->icsk_ext_hdr_len = opt->opt_flen +
+					 opt->opt_nflen;
+
+	tp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
+
+	inet->dport = usin->sin6_port;
+
+	tcp_set_state(sk, TCP_SYN_SENT);
+	err = inet6_hash_connect(&tcp_death_row, sk);
+	if (err)
+		goto late_failure;
+
+	if (!tp->write_seq)
+		tp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,
+							     np->daddr.s6_addr32,
+							     inet->sport,
+							     inet->dport);
+
+	err = tcp_connect(sk);
+	if (err)
+		goto late_failure;
+
+	return 0;
+
+late_failure:
+	tcp_set_state(sk, TCP_CLOSE);
+	__sk_dst_reset(sk);
+failure:
+	inet->dport = 0;
+	sk->sk_route_caps = 0;
+	return err;
+}
+
+static void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
+		u8 type, u8 code, int offset, __be32 info)
+{
+	struct ipv6hdr *hdr = (struct ipv6hdr*)skb->data;
+	const struct tcphdr *th = (struct tcphdr *)(skb->data+offset);
+	struct ipv6_pinfo *np;
+	struct sock *sk;
+	int err;
+	struct tcp_sock *tp;
+	__u32 seq;
+	struct net *net = dev_net(skb->dev);
+
+	sk = inet6_lookup(net, &tcp_hashinfo, &hdr->daddr,
+			th->dest, &hdr->saddr, th->source, skb->dev->ifindex);
+
+	if (sk == NULL) {
+		ICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),
+				   ICMP6_MIB_INERRORS);
+		return;
+	}
+
+	if (sk->sk_state == TCP_TIME_WAIT) {
+		inet_twsk_put(inet_twsk(sk));
+		return;
+	}
+
+	bh_lock_sock(sk);
+	if (sock_owned_by_user(sk))
+		NET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);
+
+	if (sk->sk_state == TCP_CLOSE)
+		goto out;
+
+	if (ipv6_hdr(skb)->hop_limit < sk_get_min_hopcount(sk)) {
+		NET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);
+		goto out;
+	}
+
+	tp = tcp_sk(sk);
+	seq = ntohl(th->seq);
+	if (sk->sk_state != TCP_LISTEN &&
+	    !between(seq, tp->snd_una, tp->snd_nxt)) {
+		NET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);
+		goto out;
+	}
+
+	np = inet6_sk(sk);
+
+	if (type == ICMPV6_PKT_TOOBIG) {
+		struct dst_entry *dst = NULL;
+
+		if (sock_owned_by_user(sk))
+			goto out;
+		if ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))
+			goto out;
+
+		/* icmp should have updated the destination cache entry */
+		dst = __sk_dst_check(sk, np->dst_cookie);
+
+		if (dst == NULL) {
+			struct inet_sock *inet = inet_sk(sk);
+			struct flowi fl;
+
+			/* BUGGG_FUTURE: Again, it is not clear how
+			   to handle rthdr case. Ignore this complexity
+			   for now.
+			 */
+			memset(&fl, 0, sizeof(fl));
+			fl.proto = IPPROTO_TCP;
+			ipv6_addr_copy(&fl.fl6_dst, &np->daddr);
+			ipv6_addr_copy(&fl.fl6_src, &np->saddr);
+			fl.oif = sk->sk_bound_dev_if;
+			fl.fl_ip_dport = inet->dport;
+			fl.fl_ip_sport = inet->sport;
+			security_skb_classify_flow(skb, &fl);
+
+			if ((err = ip6_dst_lookup(sk, &dst, &fl))) {
+				sk->sk_err_soft = -err;
+				goto out;
+			}
+
+			if ((err = xfrm_lookup(net, &dst, &fl, sk, 0)) < 0) {
+				sk->sk_err_soft = -err;
+				goto out;
+			}
+
+		} else
+			dst_hold(dst);
+
+		if (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {
+			tcp_sync_mss(sk, dst_mtu(dst));
+			tcp_simple_retransmit(sk);
+		} /* else let the usual retransmit timer handle it */
+		dst_release(dst);
+		goto out;
+	}
+
+	icmpv6_err_convert(type, code, &err);
+
+	/* Might be for an request_sock */
+	switch (sk->sk_state) {
+		struct request_sock *req, **prev;
+	case TCP_LISTEN:
+		if (sock_owned_by_user(sk))
+			goto out;
+
+		req = inet6_csk_search_req(sk, &prev, th->dest, &hdr->daddr,
+					   &hdr->saddr, inet6_iif(skb));
+		if (!req)
+			goto out;
+
+		/* ICMPs are not backlogged, hence we cannot get
+		 * an established socket here.
+		 */
+		WARN_ON(req->sk != NULL);
+
+		if (seq != tcp_rsk(req)->snt_isn) {
+			NET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);
+			goto out;
+		}
+
+		inet_csk_reqsk_queue_drop(sk, req, prev);
+		goto out;
+
+	case TCP_SYN_SENT:
+	case TCP_SYN_RECV:  /* Cannot happen.
+			       It can, it SYNs are crossed. --ANK */
+		if (!sock_owned_by_user(sk)) {
+			sk->sk_err = err;
+			sk->sk_error_report(sk);		/* Wake people up to see the error (see connect in sock.c) */
+
+			tcp_done(sk);
+		} else
+			sk->sk_err_soft = err;
+		goto out;
+	}
+
+	if (!sock_owned_by_user(sk) && np->recverr) {
+		sk->sk_err = err;
+		sk->sk_error_report(sk);
+	} else
+		sk->sk_err_soft = err;
+
+out:
+	bh_unlock_sock(sk);
+	sock_put(sk);
+}
+
+
+static int tcp_v6_send_synack(struct sock *sk, struct request_sock *req)
+{
+	struct inet6_request_sock *treq = inet6_rsk(req);
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct sk_buff * skb;
+	struct in6_addr * final_p, final;
+	struct flowi fl;
+	struct dst_entry *dst;
+	int err = -1;
+
+	memset(&fl, 0, sizeof(fl));
+	fl.proto = IPPROTO_TCP;
+	ipv6_addr_copy(&fl.fl6_dst, &treq->rmt_addr);
+	ipv6_addr_copy(&fl.fl6_src, &treq->loc_addr);
+	fl.fl6_flowlabel = 0;
+	fl.oif = treq->iif;
+	fl.fl_ip_dport = inet_rsk(req)->rmt_port;
+	fl.fl_ip_sport = inet_rsk(req)->loc_port;
+	security_req_classify_flow(req, &fl);
+
+	rcu_read_lock();
+	final_p = fl6_update_dst(&fl, rcu_dereference(np->opt), &final);
+	rcu_read_unlock();
+
+	err = ip6_dst_lookup(sk, &dst, &fl);
+	if (err)
+		goto done;
+	if (final_p)
+		ipv6_addr_copy(&fl.fl6_dst, final_p);
+	if ((err = xfrm_lookup(sock_net(sk), &dst, &fl, sk, 0)) < 0)
+		goto done;
+
+	skb = tcp_make_synack(sk, dst, req);
+	if (skb) {
+		struct tcphdr *th = tcp_hdr(skb);
+
+		th->check = tcp_v6_check(skb->len,
+					 &treq->loc_addr, &treq->rmt_addr,
+					 csum_partial(th, skb->len, skb->csum));
+
+		ipv6_addr_copy(&fl.fl6_dst, &treq->rmt_addr);
+		rcu_read_lock();
+		err = ip6_xmit(sk, skb, &fl, rcu_dereference(np->opt), 0);
+		rcu_read_unlock();
+		err = net_xmit_eval(err);
+	}
+
+done:
+	dst_release(dst);
+	return err;
+}
+
+static inline void syn_flood_warning(struct sk_buff *skb)
+{
+#ifdef CONFIG_SYN_COOKIES
+	if (sysctl_tcp_syncookies)
+		printk(KERN_INFO
+		       "TCPv6: Possible SYN flooding on port %d. "
+		       "Sending cookies.\n", ntohs(tcp_hdr(skb)->dest));
+	else
+#endif
+		printk(KERN_INFO
+		       "TCPv6: Possible SYN flooding on port %d. "
+		       "Dropping request.\n", ntohs(tcp_hdr(skb)->dest));
+}
+
+static void tcp_v6_reqsk_destructor(struct request_sock *req)
+{
+	kfree_skb(inet6_rsk(req)->pktopts);
+}
+
+#ifdef CONFIG_TCP_MD5SIG
+static struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,
+						   struct in6_addr *addr)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int i;
+
+	BUG_ON(tp == NULL);
+
+	if (!tp->md5sig_info || !tp->md5sig_info->entries6)
+		return NULL;
+
+	for (i = 0; i < tp->md5sig_info->entries6; i++) {
+		if (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, addr))
+			return &tp->md5sig_info->keys6[i].base;
+	}
+	return NULL;
+}
+
+static struct tcp_md5sig_key *tcp_v6_md5_lookup(struct sock *sk,
+						struct sock *addr_sk)
+{
+	return tcp_v6_md5_do_lookup(sk, &inet6_sk(addr_sk)->daddr);
+}
+
+static struct tcp_md5sig_key *tcp_v6_reqsk_md5_lookup(struct sock *sk,
+						      struct request_sock *req)
+{
+	return tcp_v6_md5_do_lookup(sk, &inet6_rsk(req)->rmt_addr);
+}
+
+static int tcp_v6_md5_do_add(struct sock *sk, struct in6_addr *peer,
+			     char *newkey, u8 newkeylen)
+{
+	/* Add key to the list */
+	struct tcp_md5sig_key *key;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp6_md5sig_key *keys;
+
+	key = tcp_v6_md5_do_lookup(sk, peer);
+	if (key) {
+		/* modify existing entry - just update that one */
+		kfree(key->key);
+		key->key = newkey;
+		key->keylen = newkeylen;
+	} else {
+		/* reallocate new list if current one is full. */
+		if (!tp->md5sig_info) {
+			tp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info), GFP_ATOMIC);
+			if (!tp->md5sig_info) {
+				kfree(newkey);
+				return -ENOMEM;
+			}
+			sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
+		}
+		if (tcp_alloc_md5sig_pool(sk) == NULL) {
+			kfree(newkey);
+			return -ENOMEM;
+		}
+		if (tp->md5sig_info->alloced6 == tp->md5sig_info->entries6) {
+			keys = kmalloc((sizeof (tp->md5sig_info->keys6[0]) *
+				       (tp->md5sig_info->entries6 + 1)), GFP_ATOMIC);
+
+			if (!keys) {
+				tcp_free_md5sig_pool();
+				kfree(newkey);
+				return -ENOMEM;
+			}
+
+			if (tp->md5sig_info->entries6)
+				memmove(keys, tp->md5sig_info->keys6,
+					(sizeof (tp->md5sig_info->keys6[0]) *
+					 tp->md5sig_info->entries6));
+
+			kfree(tp->md5sig_info->keys6);
+			tp->md5sig_info->keys6 = keys;
+			tp->md5sig_info->alloced6++;
+		}
+
+		ipv6_addr_copy(&tp->md5sig_info->keys6[tp->md5sig_info->entries6].addr,
+			       peer);
+		tp->md5sig_info->keys6[tp->md5sig_info->entries6].base.key = newkey;
+		tp->md5sig_info->keys6[tp->md5sig_info->entries6].base.keylen = newkeylen;
+
+		tp->md5sig_info->entries6++;
+	}
+	return 0;
+}
+
+static int tcp_v6_md5_add_func(struct sock *sk, struct sock *addr_sk,
+			       u8 *newkey, __u8 newkeylen)
+{
+	return tcp_v6_md5_do_add(sk, &inet6_sk(addr_sk)->daddr,
+				 newkey, newkeylen);
+}
+
+static int tcp_v6_md5_do_del(struct sock *sk, struct in6_addr *peer)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int i;
+
+	for (i = 0; i < tp->md5sig_info->entries6; i++) {
+		if (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, peer)) {
+			/* Free the key */
+			kfree(tp->md5sig_info->keys6[i].base.key);
+			tp->md5sig_info->entries6--;
+
+			if (tp->md5sig_info->entries6 == 0) {
+				kfree(tp->md5sig_info->keys6);
+				tp->md5sig_info->keys6 = NULL;
+				tp->md5sig_info->alloced6 = 0;
+			} else {
+				/* shrink the database */
+				if (tp->md5sig_info->entries6 != i)
+					memmove(&tp->md5sig_info->keys6[i],
+						&tp->md5sig_info->keys6[i+1],
+						(tp->md5sig_info->entries6 - i)
+						* sizeof (tp->md5sig_info->keys6[0]));
+			}
+			tcp_free_md5sig_pool();
+			return 0;
+		}
+	}
+	return -ENOENT;
+}
+
+static void tcp_v6_clear_md5_list (struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int i;
+
+	if (tp->md5sig_info->entries6) {
+		for (i = 0; i < tp->md5sig_info->entries6; i++)
+			kfree(tp->md5sig_info->keys6[i].base.key);
+		tp->md5sig_info->entries6 = 0;
+		tcp_free_md5sig_pool();
+	}
+
+	kfree(tp->md5sig_info->keys6);
+	tp->md5sig_info->keys6 = NULL;
+	tp->md5sig_info->alloced6 = 0;
+
+	if (tp->md5sig_info->entries4) {
+		for (i = 0; i < tp->md5sig_info->entries4; i++)
+			kfree(tp->md5sig_info->keys4[i].base.key);
+		tp->md5sig_info->entries4 = 0;
+		tcp_free_md5sig_pool();
+	}
+
+	kfree(tp->md5sig_info->keys4);
+	tp->md5sig_info->keys4 = NULL;
+	tp->md5sig_info->alloced4 = 0;
+}
+
+static int tcp_v6_parse_md5_keys (struct sock *sk, char __user *optval,
+				  int optlen)
+{
+	struct tcp_md5sig cmd;
+	struct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)&cmd.tcpm_addr;
+	u8 *newkey;
+
+	if (optlen < sizeof(cmd))
+		return -EINVAL;
+
+	if (copy_from_user(&cmd, optval, sizeof(cmd)))
+		return -EFAULT;
+
+	if (sin6->sin6_family != AF_INET6)
+		return -EINVAL;
+
+	if (!cmd.tcpm_keylen) {
+		if (!tcp_sk(sk)->md5sig_info)
+			return -ENOENT;
+		if (ipv6_addr_v4mapped(&sin6->sin6_addr))
+			return tcp_v4_md5_do_del(sk, sin6->sin6_addr.s6_addr32[3]);
+		return tcp_v6_md5_do_del(sk, &sin6->sin6_addr);
+	}
+
+	if (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)
+		return -EINVAL;
+
+	if (!tcp_sk(sk)->md5sig_info) {
+		struct tcp_sock *tp = tcp_sk(sk);
+		struct tcp_md5sig_info *p;
+
+		p = kzalloc(sizeof(struct tcp_md5sig_info), GFP_KERNEL);
+		if (!p)
+			return -ENOMEM;
+
+		tp->md5sig_info = p;
+		sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
+	}
+
+	newkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, GFP_KERNEL);
+	if (!newkey)
+		return -ENOMEM;
+	if (ipv6_addr_v4mapped(&sin6->sin6_addr)) {
+		return tcp_v4_md5_do_add(sk, sin6->sin6_addr.s6_addr32[3],
+					 newkey, cmd.tcpm_keylen);
+	}
+	return tcp_v6_md5_do_add(sk, &sin6->sin6_addr, newkey, cmd.tcpm_keylen);
+}
+
+static int tcp_v6_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,
+					struct in6_addr *daddr,
+					struct in6_addr *saddr, int nbytes)
+{
+	struct tcp6_pseudohdr *bp;
+	struct scatterlist sg;
+
+	bp = &hp->md5_blk.ip6;
+	/* 1. TCP pseudo-header (RFC2460) */
+	ipv6_addr_copy(&bp->saddr, saddr);
+	ipv6_addr_copy(&bp->daddr, daddr);
+	bp->protocol = cpu_to_be32(IPPROTO_TCP);
+	bp->len = cpu_to_be32(nbytes);
+
+	sg_init_one(&sg, bp, sizeof(*bp));
+	return crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));
+}
+
+static int tcp_v6_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,
+			       struct in6_addr *daddr, struct in6_addr *saddr,
+			       struct tcphdr *th)
+{
+	struct tcp_md5sig_pool *hp;
+	struct hash_desc *desc;
+
+	hp = tcp_get_md5sig_pool();
+	if (!hp)
+		goto clear_hash_noput;
+	desc = &hp->md5_desc;
+
+	if (crypto_hash_init(desc))
+		goto clear_hash;
+	if (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))
+		goto clear_hash;
+	if (tcp_md5_hash_header(hp, th))
+		goto clear_hash;
+	if (tcp_md5_hash_key(hp, key))
+		goto clear_hash;
+	if (crypto_hash_final(desc, md5_hash))
+		goto clear_hash;
+
+	tcp_put_md5sig_pool();
+	return 0;
+
+clear_hash:
+	tcp_put_md5sig_pool();
+clear_hash_noput:
+	memset(md5_hash, 0, 16);
+	return 1;
+}
+
+static int tcp_v6_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
+			       struct sock *sk, struct request_sock *req,
+			       struct sk_buff *skb)
+{
+	struct in6_addr *saddr, *daddr;
+	struct tcp_md5sig_pool *hp;
+	struct hash_desc *desc;
+	struct tcphdr *th = tcp_hdr(skb);
+
+	if (sk) {
+		saddr = &inet6_sk(sk)->saddr;
+		daddr = &inet6_sk(sk)->daddr;
+	} else if (req) {
+		saddr = &inet6_rsk(req)->loc_addr;
+		daddr = &inet6_rsk(req)->rmt_addr;
+	} else {
+		struct ipv6hdr *ip6h = ipv6_hdr(skb);
+		saddr = &ip6h->saddr;
+		daddr = &ip6h->daddr;
+	}
+
+	hp = tcp_get_md5sig_pool();
+	if (!hp)
+		goto clear_hash_noput;
+	desc = &hp->md5_desc;
+
+	if (crypto_hash_init(desc))
+		goto clear_hash;
+
+	if (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))
+		goto clear_hash;
+	if (tcp_md5_hash_header(hp, th))
+		goto clear_hash;
+	if (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))
+		goto clear_hash;
+	if (tcp_md5_hash_key(hp, key))
+		goto clear_hash;
+	if (crypto_hash_final(desc, md5_hash))
+		goto clear_hash;
+
+	tcp_put_md5sig_pool();
+	return 0;
+
+clear_hash:
+	tcp_put_md5sig_pool();
+clear_hash_noput:
+	memset(md5_hash, 0, 16);
+	return 1;
+}
+
+static int tcp_v6_inbound_md5_hash (struct sock *sk, struct sk_buff *skb)
+{
+	__u8 *hash_location = NULL;
+	struct tcp_md5sig_key *hash_expected;
+	struct ipv6hdr *ip6h = ipv6_hdr(skb);
+	struct tcphdr *th = tcp_hdr(skb);
+	int genhash;
+	u8 newhash[16];
+
+	hash_expected = tcp_v6_md5_do_lookup(sk, &ip6h->saddr);
+	hash_location = tcp_parse_md5sig_option(th);
+
+	/* We've parsed the options - do we have a hash? */
+	if (!hash_expected && !hash_location)
+		return 0;
+
+	if (hash_expected && !hash_location) {
+		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);
+		return 1;
+	}
+
+	if (!hash_expected && hash_location) {
+		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);
+		return 1;
+	}
+
+	/* check the signature */
+	genhash = tcp_v6_md5_hash_skb(newhash,
+				      hash_expected,
+				      NULL, NULL, skb);
+
+	if (genhash || memcmp(hash_location, newhash, 16) != 0) {
+		if (net_ratelimit()) {
+			printk(KERN_INFO "MD5 Hash %s for (%pI6, %u)->(%pI6, %u)\n",
+			       genhash ? "failed" : "mismatch",
+			       &ip6h->saddr, ntohs(th->source),
+			       &ip6h->daddr, ntohs(th->dest));
+		}
+		return 1;
+	}
+	return 0;
+}
+#endif
+
+struct request_sock_ops tcp6_request_sock_ops __read_mostly = {
+	.family		=	AF_INET6,
+	.obj_size	=	sizeof(struct tcp6_request_sock),
+	.rtx_syn_ack	=	tcp_v6_send_synack,
+	.send_ack	=	tcp_v6_reqsk_send_ack,
+	.destructor	=	tcp_v6_reqsk_destructor,
+	.send_reset	=	tcp_v6_send_reset
+};
+
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
+	.md5_lookup	=	tcp_v6_reqsk_md5_lookup,
+	.calc_md5_hash	=	tcp_v6_md5_hash_skb,
+};
+#endif
+
+static struct timewait_sock_ops tcp6_timewait_sock_ops = {
+	.twsk_obj_size	= sizeof(struct tcp6_timewait_sock),
+	.twsk_unique	= tcp_twsk_unique,
+	.twsk_destructor= tcp_twsk_destructor,
+};
+
+static void tcp_v6_send_check(struct sock *sk, int len, struct sk_buff *skb)
+{
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct tcphdr *th = tcp_hdr(skb);
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		th->check = ~csum_ipv6_magic(&np->saddr, &np->daddr, len, IPPROTO_TCP,  0);
+		skb->csum_start = skb_transport_header(skb) - skb->head;
+		skb->csum_offset = offsetof(struct tcphdr, check);
+	} else {
+		th->check = csum_ipv6_magic(&np->saddr, &np->daddr, len, IPPROTO_TCP,
+					    csum_partial(th, th->doff<<2,
+							 skb->csum));
+	}
+}
+
+static void tcp_v6_send_response(struct sk_buff *skb, u32 seq, u32 ack, u32 win,
+				 u32 ts, int oif, struct tcp_md5sig_key *key, int rst)
+{
+	struct tcphdr *th = tcp_hdr(skb), *t1;
+	struct sk_buff *buff;
+	struct flowi fl;
+	struct net *net = dev_net(skb_dst(skb)->dev);
+	struct sock *ctl_sk = net->ipv6.tcp_sk;
+	unsigned int tot_len = sizeof(struct tcphdr);
+	struct dst_entry *dst;
+	__be32 *topt;
+
+	if (ts)
+		tot_len += TCPOLEN_TSTAMP_ALIGNED;
+#ifdef CONFIG_TCP_MD5SIG
+	if (key)
+		tot_len += TCPOLEN_MD5SIG_ALIGNED;
+#endif
+
+	buff = alloc_skb(MAX_HEADER + sizeof(struct ipv6hdr) + tot_len,
+			 GFP_ATOMIC);
+	if (buff == NULL)
+		return;
+
+	skb_reserve(buff, MAX_HEADER + sizeof(struct ipv6hdr) + tot_len);
+
+	t1 = (struct tcphdr *) skb_push(buff, tot_len);
+	skb_reset_transport_header(buff);
+
+	/* Swap the send and the receive. */
+	memset(t1, 0, sizeof(*t1));
+	t1->dest = th->source;
+	t1->source = th->dest;
+	t1->doff = tot_len / 4;
+	t1->seq = htonl(seq);
+	t1->ack_seq = htonl(ack);
+	t1->ack = !rst || !th->ack;
+	t1->rst = rst;
+	t1->window = htons(win);
+
+	topt = (__be32 *)(t1 + 1);
+
+	if (ts) {
+		*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
+				(TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);
+		*topt++ = htonl(tcp_time_stamp);
+		*topt++ = htonl(ts);
+	}
+
+#ifdef CONFIG_TCP_MD5SIG
+	if (key) {
+		*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
+				(TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);
+		tcp_v6_md5_hash_hdr((__u8 *)topt, key,
+				    &ipv6_hdr(skb)->saddr,
+				    &ipv6_hdr(skb)->daddr, t1);
+	}
+#endif
+
+	buff->csum = csum_partial(t1, tot_len, 0);
+
+	memset(&fl, 0, sizeof(fl));
+	ipv6_addr_copy(&fl.fl6_dst, &ipv6_hdr(skb)->saddr);
+	ipv6_addr_copy(&fl.fl6_src, &ipv6_hdr(skb)->daddr);
+
+	t1->check = csum_ipv6_magic(&fl.fl6_src, &fl.fl6_dst,
+				    tot_len, IPPROTO_TCP,
+				    buff->csum);
+
+	fl.proto = IPPROTO_TCP;
+	if (rt6_need_strict(&fl.fl6_dst) && !oif)
+		fl.oif = inet6_iif(skb);
+	else
+		fl.oif = oif;
+	fl.fl_ip_dport = t1->dest;
+	fl.fl_ip_sport = t1->source;
+	security_skb_classify_flow(skb, &fl);
+
+	/* Pass a socket to ip6_dst_lookup either it is for RST
+	 * Underlying function will use this to retrieve the network
+	 * namespace
+	 */
+	if (!ip6_dst_lookup(ctl_sk, &dst, &fl)) {
+		if (xfrm_lookup(net, &dst, &fl, NULL, 0) >= 0) {
+			skb_dst_set(buff, dst);
+			ip6_xmit(ctl_sk, buff, &fl, NULL, 0);
+			TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
+			if (rst)
+				TCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);
+			return;
+		}
+	}
+
+	kfree_skb(buff);
+}
+
+static void tcp_v6_send_reset(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcphdr *th = tcp_hdr(skb);
+	u32 seq = 0, ack_seq = 0;
+	struct tcp_md5sig_key *key = NULL;
+	int oif;
+
+	if (th->rst)
+		return;
+
+	if (!ipv6_unicast_destination(skb))
+		return;
+
+#ifdef CONFIG_TCP_MD5SIG
+	if (sk && sk->sk_state != TCP_TIME_WAIT)
+		key = tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr);
+#endif
+
+	if (th->ack)
+		seq = ntohl(th->ack_seq);
+	else
+		ack_seq = ntohl(th->seq) + th->syn + th->fin + skb->len -
+			  (th->doff << 2);
+
+	oif = sk ? sk->sk_bound_dev_if : 0;
+	tcp_v6_send_response(skb, seq, ack_seq, 0, 0, oif, key, 1);
+}
+
+static void tcp_v6_send_ack(struct sk_buff *skb, u32 seq, u32 ack, u32 win, u32 ts,
+			    int oif, struct tcp_md5sig_key *key)
+{
+	tcp_v6_send_response(skb, seq, ack, win, ts, oif, key, 0);
+}
+
+static void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb)
+{
+	struct inet_timewait_sock *tw = inet_twsk(sk);
+	struct tcp_timewait_sock *tcptw = tcp_twsk(sk);
+
+	tcp_v6_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,
+			tcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,
+			tcptw->tw_ts_recent, tw->tw_bound_dev_if,
+		        tcp_twsk_md5_key(tcptw));
+
+	inet_twsk_put(tw);
+}
+
+static void tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,
+				  struct request_sock *req)
+{
+	tcp_v6_send_ack(skb, tcp_rsk(req)->snt_isn + 1, tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd, req->ts_recent,
+			sk->sk_bound_dev_if,
+			tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr));
+}
+
+static struct sock *tcp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)
+{
+	struct request_sock *req, **prev;
+	const struct tcphdr *th = tcp_hdr(skb);
+	struct sock *nsk;
+
+	/* Find possible connection requests. */
+	req = inet6_csk_search_req(sk, &prev, th->source,
+				   &ipv6_hdr(skb)->saddr,
+				   &ipv6_hdr(skb)->daddr, inet6_iif(skb));
+	if (req)
+		return tcp_check_req(sk, skb, req, prev);
+
+	nsk = __inet6_lookup_established(sock_net(sk), &tcp_hashinfo,
+			&ipv6_hdr(skb)->saddr, th->source,
+			&ipv6_hdr(skb)->daddr, ntohs(th->dest), inet6_iif(skb));
+
+	if (nsk) {
+		if (nsk->sk_state != TCP_TIME_WAIT) {
+			bh_lock_sock(nsk);
+			return nsk;
+		}
+		inet_twsk_put(inet_twsk(nsk));
+		return NULL;
+	}
+
+#ifdef CONFIG_SYN_COOKIES
+	if (!th->rst && !th->syn && th->ack)
+		sk = cookie_v6_check(sk, skb);
+#endif
+	return sk;
+}
+
+/* FIXME: this is substantially similar to the ipv4 code.
+ * Can some kind of merge be done? -- erics
+ */
+static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
+{
+	struct inet6_request_sock *treq;
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct tcp_options_received tmp_opt;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct request_sock *req = NULL;
+	__u32 isn = TCP_SKB_CB(skb)->when;
+#ifdef CONFIG_SYN_COOKIES
+	int want_cookie = 0;
+#else
+#define want_cookie 0
+#endif
+
+	if (skb->protocol == htons(ETH_P_IP))
+		return tcp_v4_conn_request(sk, skb);
+
+	if (!ipv6_unicast_destination(skb))
+		goto drop;
+
+	if (inet_csk_reqsk_queue_is_full(sk) && !isn) {
+		if (net_ratelimit())
+			syn_flood_warning(skb);
+#ifdef CONFIG_SYN_COOKIES
+		if (sysctl_tcp_syncookies)
+			want_cookie = 1;
+		else
+#endif
+		goto drop;
+	}
+
+	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)
+		goto drop;
+
+	req = inet6_reqsk_alloc(&tcp6_request_sock_ops);
+	if (req == NULL)
+		goto drop;
+
+#ifdef CONFIG_TCP_MD5SIG
+	tcp_rsk(req)->af_specific = &tcp_request_sock_ipv6_ops;
+#endif
+
+	tcp_clear_options(&tmp_opt);
+	tmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
+	tmp_opt.user_mss = tp->rx_opt.user_mss;
+
+	tcp_parse_options(skb, &tmp_opt, 0);
+
+	if (want_cookie && !tmp_opt.saw_tstamp)
+		tcp_clear_options(&tmp_opt);
+
+	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+	tcp_openreq_init(req, &tmp_opt, skb);
+
+	treq = inet6_rsk(req);
+	ipv6_addr_copy(&treq->rmt_addr, &ipv6_hdr(skb)->saddr);
+	ipv6_addr_copy(&treq->loc_addr, &ipv6_hdr(skb)->daddr);
+	if (!want_cookie)
+		TCP_ECN_create_request(req, tcp_hdr(skb));
+
+	treq->iif = sk->sk_bound_dev_if;
+
+	/* So that link locals have meaning */
+	if (!sk->sk_bound_dev_if &&
+	    ipv6_addr_type(&treq->rmt_addr) & IPV6_ADDR_LINKLOCAL)
+		treq->iif = inet6_iif(skb);
+
+	if (want_cookie) {
+		isn = cookie_v6_init_sequence(sk, skb, &req->mss);
+		req->cookie_ts = tmp_opt.tstamp_ok;
+	} else if (!isn) {
+		if (ipv6_opt_accepted(sk, skb) ||
+		    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||
+		    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {
+			atomic_inc(&skb->users);
+			treq->pktopts = skb;
+		}
+
+		isn = tcp_v6_init_sequence(skb);
+	}
+
+	tcp_rsk(req)->snt_isn = isn;
+	tcp_rsk(req)->snt_synack = tcp_time_stamp;
+
+	security_inet_conn_request(sk, skb, req);
+
+	if (tcp_v6_send_synack(sk, req))
+		goto drop;
+
+	if (!want_cookie) {
+		inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+		return 0;
+	}
+
+drop:
+	if (req)
+		reqsk_free(req);
+
+	return 0; /* don't send reset */
+}
+
+static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+					  struct request_sock *req,
+					  struct dst_entry *dst)
+{
+	struct inet6_request_sock *treq;
+	struct ipv6_pinfo *newnp, *np = inet6_sk(sk);
+	struct ipv6_txoptions *opt;
+	struct tcp6_sock *newtcp6sk;
+	struct inet_sock *newinet;
+	struct tcp_sock *newtp;
+	struct sock *newsk;
+#ifdef CONFIG_TCP_MD5SIG
+	struct tcp_md5sig_key *key;
+#endif
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		/*
+		 *	v6 mapped
+		 */
+
+		newsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);
+
+		if (newsk == NULL)
+			return NULL;
+
+		newtcp6sk = (struct tcp6_sock *)newsk;
+		inet_sk(newsk)->pinet6 = &newtcp6sk->inet6;
+
+		newinet = inet_sk(newsk);
+		newnp = inet6_sk(newsk);
+		newtp = tcp_sk(newsk);
+
+		memcpy(newnp, np, sizeof(struct ipv6_pinfo));
+
+		ipv6_addr_set(&newnp->daddr, 0, 0, htonl(0x0000FFFF),
+			      newinet->daddr);
+
+		ipv6_addr_set(&newnp->saddr, 0, 0, htonl(0x0000FFFF),
+			      newinet->saddr);
+
+		ipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);
+
+		inet_csk(newsk)->icsk_af_ops = &ipv6_mapped;
+		newsk->sk_backlog_rcv = tcp_v4_do_rcv;
+#ifdef CONFIG_TCP_MD5SIG
+		newtp->af_specific = &tcp_sock_ipv6_mapped_specific;
+#endif
+
+		newnp->pktoptions  = NULL;
+		newnp->opt	   = NULL;
+		newnp->mcast_oif   = inet6_iif(skb);
+		newnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;
+		sk_extended(newsk)->rcv_tos = ipv6_get_dsfield(ipv6_hdr(skb));
+
+		/*
+		 * No need to charge this sock to the relevant IPv6 refcnt debug socks count
+		 * here, tcp_create_openreq_child now does this for us, see the comment in
+		 * that function for the gory details. -acme
+		 */
+
+		/* It is tricky place. Until this moment IPv4 tcp
+		   worked with IPv6 icsk.icsk_af_ops.
+		   Sync it now.
+		 */
+		tcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);
+
+		return newsk;
+	}
+
+	treq = inet6_rsk(req);
+
+	if (sk_acceptq_is_full(sk))
+		goto out_overflow;
+
+	if (dst == NULL) {
+		struct in6_addr *final_p, final;
+		struct flowi fl;
+
+		memset(&fl, 0, sizeof(fl));
+		fl.proto = IPPROTO_TCP;
+		ipv6_addr_copy(&fl.fl6_dst, &treq->rmt_addr);
+		final_p = fl6_update_dst(&fl, rcu_dereference(np->opt), &final);
+		ipv6_addr_copy(&fl.fl6_src, &treq->loc_addr);
+		fl.oif = sk->sk_bound_dev_if;
+		fl.fl_ip_dport = inet_rsk(req)->rmt_port;
+		fl.fl_ip_sport = inet_rsk(req)->loc_port;
+		security_req_classify_flow(req, &fl);
+
+		if (ip6_dst_lookup(sk, &dst, &fl))
+			goto out;
+
+		if (final_p)
+			ipv6_addr_copy(&fl.fl6_dst, final_p);
+
+		if ((xfrm_lookup(sock_net(sk), &dst, &fl, sk, 0)) < 0)
+			goto out;
+	}
+
+	newsk = tcp_create_openreq_child(sk, req, skb);
+	if (newsk == NULL)
+		goto out_nonewsk;
+
+	/*
+	 * No need to charge this sock to the relevant IPv6 refcnt debug socks
+	 * count here, tcp_create_openreq_child now does this for us, see the
+	 * comment in that function for the gory details. -acme
+	 */
+
+	newsk->sk_gso_type = SKB_GSO_TCPV6;
+	__ip6_dst_store(newsk, dst, NULL, NULL);
+
+	newtcp6sk = (struct tcp6_sock *)newsk;
+	inet_sk(newsk)->pinet6 = &newtcp6sk->inet6;
+
+	newtp = tcp_sk(newsk);
+	newinet = inet_sk(newsk);
+	newnp = inet6_sk(newsk);
+
+	memcpy(newnp, np, sizeof(struct ipv6_pinfo));
+
+	ipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);
+	ipv6_addr_copy(&newnp->saddr, &treq->loc_addr);
+	ipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);
+	newsk->sk_bound_dev_if = treq->iif;
+
+	/* Now IPv6 options...
+
+	   First: no IPv4 options.
+	 */
+	newinet->opt = NULL;
+	newnp->ipv6_fl_list = NULL;
+
+	/* Clone RX bits */
+	newnp->rxopt.all = np->rxopt.all;
+
+	/* Clone pktoptions received with SYN */
+	newnp->pktoptions = NULL;
+	if (treq->pktopts != NULL) {
+		newnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);
+		kfree_skb(treq->pktopts);
+		treq->pktopts = NULL;
+		if (newnp->pktoptions)
+			skb_set_owner_r(newnp->pktoptions, newsk);
+	}
+	newnp->opt	  = NULL;
+	newnp->mcast_oif  = inet6_iif(skb);
+	newnp->mcast_hops = ipv6_hdr(skb)->hop_limit;
+	sk_extended(newsk)->rcv_tos = ipv6_get_dsfield(ipv6_hdr(skb));
+
+	/* Clone native IPv6 options from listening socket (if any)
+
+	   Yes, keeping reference count would be much more clever,
+	   but we make one more one thing there: reattach optmem
+	   to newsk.
+	 */
+	opt = rcu_dereference(np->opt);
+	if (opt) {
+		opt = ipv6_dup_options(newsk, opt);
+		RCU_INIT_POINTER(newnp->opt, opt);
+	}
+	inet_csk(newsk)->icsk_ext_hdr_len = 0;
+	if (opt)
+		inet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +
+						    opt->opt_flen;
+
+	tcp_mtup_init(newsk);
+	tcp_sync_mss(newsk, dst_mtu(dst));
+	newtp->advmss = dst_metric_advmss(dst);
+	tcp_initialize_rcv_mss(newsk);
+	if (tcp_rsk(req)->snt_synack)
+		tcp_valid_rtt_meas(newsk,
+		    tcp_time_stamp - tcp_rsk(req)->snt_synack);
+	newtp->total_retrans = req->retrans;
+
+	newinet->daddr = newinet->saddr = newinet->rcv_saddr = LOOPBACK4_IPV6;
+
+#ifdef CONFIG_TCP_MD5SIG
+	/* Copy over the MD5 key from the original socket */
+	if ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {
+		/* We're using one, so create a matching key
+		 * on the newsk structure. If we fail to get
+		 * memory, then we end up not copying the key
+		 * across. Shucks.
+		 */
+		char *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);
+		if (newkey != NULL)
+			tcp_v6_md5_do_add(newsk, &newnp->daddr,
+					  newkey, key->keylen);
+	}
+#endif
+
+	if (__inet_inherit_port(sk, newsk) < 0) {
+		sock_put(newsk);
+		goto out;
+	}
+	__inet6_hash(newsk, NULL);
+
+	return newsk;
+
+out_overflow:
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+out_nonewsk:
+	dst_release(dst);
+out:
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+	return NULL;
+}
+
+static __sum16 tcp_v6_checksum_init(struct sk_buff *skb)
+{
+	if (skb->ip_summed == CHECKSUM_COMPLETE) {
+		if (!tcp_v6_check(skb->len, &ipv6_hdr(skb)->saddr,
+				  &ipv6_hdr(skb)->daddr, skb->csum)) {
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return 0;
+		}
+	}
+
+	skb->csum = ~csum_unfold(tcp_v6_check(skb->len,
+					      &ipv6_hdr(skb)->saddr,
+					      &ipv6_hdr(skb)->daddr, 0));
+
+	if (skb->len <= 76) {
+		return __skb_checksum_complete(skb);
+	}
+	return 0;
+}
+
+/* The socket must have it's spinlock held when we get
+ * here.
+ *
+ * We have a potential double-lock case here, so even when
+ * doing backlog processing we use the BH locking scheme.
+ * This is because we cannot sleep with the original spinlock
+ * held.
+ */
+static int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	struct ipv6_pinfo *np = inet6_sk(sk);
+	struct tcp_sock *tp;
+	struct sk_buff *opt_skb = NULL;
+
+	/* Imagine: socket is IPv6. IPv4 packet arrives,
+	   goes to IPv4 receive handler and backlogged.
+	   From backlog it always goes here. Kerboom...
+	   Fortunately, tcp_rcv_established and rcv_established
+	   handle them correctly, but it is not case with
+	   tcp_v6_hnd_req and tcp_v6_send_reset().   --ANK
+	 */
+
+	if (skb->protocol == htons(ETH_P_IP))
+		return tcp_v4_do_rcv(sk, skb);
+
+#ifdef CONFIG_TCP_MD5SIG
+	if (tcp_v6_inbound_md5_hash (sk, skb))
+		goto discard;
+#endif
+
+	if (sk_filter(sk, skb))
+		goto discard;
+
+	/*
+	 *	socket locking is here for SMP purposes as backlog rcv
+	 *	is currently called with bh processing disabled.
+	 */
+
+	/* Do Stevens' IPV6_PKTOPTIONS.
+
+	   Yes, guys, it is the only place in our code, where we
+	   may make it not affecting IPv4.
+	   The rest of code is protocol independent,
+	   and I do not like idea to uglify IPv4.
+
+	   Actually, all the idea behind IPV6_PKTOPTIONS
+	   looks not very well thought. For now we latch
+	   options, received in the last packet, enqueued
+	   by tcp. Feel free to propose better solution.
+					       --ANK (980728)
+	 */
+	if (np->rxopt.all)
+		opt_skb = skb_clone(skb, GFP_ATOMIC);
+
+	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
+		TCP_CHECK_TIMER(sk);
+		sock_rps_save_rxhash(sk, skb->rxhash);
+		if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len))
+			goto reset;
+		TCP_CHECK_TIMER(sk);
+		if (opt_skb)
+			goto ipv6_pktoptions;
+		return 0;
+	}
+
+	if (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))
+		goto csum_err;
+
+	if (sk->sk_state == TCP_LISTEN) {
+		struct sock *nsk = tcp_v6_hnd_req(sk, skb);
+		if (!nsk)
+			goto discard;
+
+		/*
+		 * Queue it on the new socket if the new socket is active,
+		 * otherwise we just shortcircuit this and continue with
+		 * the new socket..
+		 */
+		if(nsk != sk) {
+			sock_rps_save_rxhash(nsk, skb->rxhash);
+			if (tcp_child_process(sk, nsk, skb))
+				goto reset;
+			if (opt_skb)
+				__kfree_skb(opt_skb);
+			return 0;
+		}
+	} else
+		sock_rps_save_rxhash(sk, skb->rxhash);
+
+	TCP_CHECK_TIMER(sk);
+	if (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len))
+		goto reset;
+	TCP_CHECK_TIMER(sk);
+	if (opt_skb)
+		goto ipv6_pktoptions;
+	return 0;
+
+reset:
+	tcp_v6_send_reset(sk, skb);
+discard:
+	if (opt_skb)
+		__kfree_skb(opt_skb);
+	kfree_skb(skb);
+	return 0;
+csum_err:
+	TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+	goto discard;
+
+
+ipv6_pktoptions:
+	/* Do you ask, what is it?
+
+	   1. skb was enqueued by tcp.
+	   2. skb is added to tail of read queue, rather than out of order.
+	   3. socket is not in passive state.
+	   4. Finally, it really contains options, which user wants to receive.
+	 */
+	tp = tcp_sk(sk);
+	if (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt &&
+	    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {
+		if (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)
+			np->mcast_oif = inet6_iif(opt_skb);
+		if (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)
+			np->mcast_hops = ipv6_hdr(opt_skb)->hop_limit;
+		if (np->rxopt.bits.rxtclass)
+			sk_extended(sk)->rcv_tos = ipv6_get_dsfield(ipv6_hdr(opt_skb));
+		if (ipv6_opt_accepted(sk, opt_skb)) {
+			skb_set_owner_r(opt_skb, sk);
+			opt_skb = xchg(&np->pktoptions, opt_skb);
+		} else {
+			__kfree_skb(opt_skb);
+			opt_skb = xchg(&np->pktoptions, NULL);
+		}
+	}
+
+	kfree_skb(opt_skb);
+	return 0;
+}
+
+static int tcp_v6_rcv(struct sk_buff *skb)
+{
+	struct tcphdr *th;
+	struct ipv6hdr *hdr;
+	struct sock *sk;
+	int ret;
+	struct net *net = dev_net(skb->dev);
+
+	if (skb->pkt_type != PACKET_HOST)
+		goto discard_it;
+
+	/*
+	 *	Count it even if it's bad.
+	 */
+	TCP_INC_STATS_BH(net, TCP_MIB_INSEGS);
+
+	if (!pskb_may_pull(skb, sizeof(struct tcphdr)))
+		goto discard_it;
+
+	th = tcp_hdr(skb);
+	hdr = ipv6_hdr(skb);
+
+	if (th->doff < sizeof(struct tcphdr)/4)
+		goto bad_packet;
+	if (!pskb_may_pull(skb, th->doff*4))
+		goto discard_it;
+
+	if (!skb_csum_unnecessary(skb) && tcp_v6_checksum_init(skb))
+		goto bad_packet;
+
+	th = tcp_hdr(skb);
+	TCP_SKB_CB(skb)->seq = ntohl(th->seq);
+	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
+				    skb->len - th->doff*4);
+	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
+	TCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);
+	TCP_SKB_CB(skb)->when = 0;
+	TCP_SKB_CB(skb)->ip_dsfield = ipv6_get_dsfield(hdr);
+	TCP_SKB_CB(skb)->sacked = 0;
+
+	sk = __inet6_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);
+	if (!sk)
+		goto no_tcp_socket;
+
+process:
+	if (sk->sk_state == TCP_TIME_WAIT)
+		goto do_time_wait;
+
+	if (hdr->hop_limit < sk_get_min_hopcount(sk)) {
+		NET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);
+		goto discard_and_relse;
+	}
+
+	if (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))
+		goto discard_and_relse;
+
+	if (sk_filter(sk, skb))
+		goto discard_and_relse;
+
+	sk_mark_napi_id(sk, skb);
+	skb->dev = NULL;
+
+	bh_lock_sock_nested(sk);
+	tcp_sk(sk)->segs_in += max_t(u16, 1, skb_shinfo(skb)->gso_segs);
+	ret = 0;
+	if (!sock_owned_by_user(sk)) {
+#ifdef CONFIG_NET_DMA
+		struct tcp_sock *tp = tcp_sk(sk);
+		if (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)
+			tp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);
+		if (tp->ucopy.dma_chan)
+			ret = tcp_v6_do_rcv(sk, skb);
+		else
+#endif
+		{
+			if (!tcp_prequeue(sk, skb))
+				ret = tcp_v6_do_rcv(sk, skb);
+		}
+	} else if (unlikely(sk_add_backlog(sk, skb,
+					   sk->sk_rcvbuf + sk->sk_sndbuf))) {
+		bh_unlock_sock(sk);
+		NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);
+		goto discard_and_relse;
+	}
+	bh_unlock_sock(sk);
+
+	sock_put(sk);
+	return ret ? -1 : 0;
+
+no_tcp_socket:
+	if (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))
+		goto discard_it;
+
+	if (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {
+bad_packet:
+		TCP_INC_STATS_BH(net, TCP_MIB_INERRS);
+	} else {
+		tcp_v6_send_reset(NULL, skb);
+	}
+
+discard_it:
+
+	/*
+	 *	Discard frame
+	 */
+
+	kfree_skb(skb);
+	return 0;
+
+discard_and_relse:
+	sock_put(sk);
+	goto discard_it;
+
+do_time_wait:
+	if (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	}
+
+	if (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {
+		TCP_INC_STATS_BH(net, TCP_MIB_INERRS);
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	}
+
+	switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
+	case TCP_TW_SYN:
+	{
+		struct sock *sk2;
+
+		sk2 = inet6_lookup_listener(dev_net(skb->dev), &tcp_hashinfo,
+					    &ipv6_hdr(skb)->saddr, th->source,
+					    &ipv6_hdr(skb)->daddr,
+					    ntohs(th->dest), inet6_iif(skb));
+		if (sk2 != NULL) {
+			struct inet_timewait_sock *tw = inet_twsk(sk);
+			inet_twsk_deschedule(tw, &tcp_death_row);
+			inet_twsk_put(tw);
+			sk = sk2;
+			goto process;
+		}
+		/* Fall through to ACK */
+	}
+	case TCP_TW_ACK:
+		tcp_v6_timewait_ack(sk, skb);
+		break;
+	case TCP_TW_RST:
+		tcp_v6_send_reset(sk, skb);
+		inet_twsk_deschedule(inet_twsk(sk), &tcp_death_row);
+		inet_twsk_put(inet_twsk(sk));
+		goto discard_it;
+	case TCP_TW_SUCCESS:;
+	}
+	goto discard_it;
+}
+
+static int tcp_v6_remember_stamp(struct sock *sk)
+{
+	/* Alas, not yet... */
+	return 0;
+}
+
+static const struct inet_connection_sock_af_ops ipv6_specific = {
+	.queue_xmit	   = inet6_csk_xmit,
+	.send_check	   = tcp_v6_send_check,
+	.rebuild_header	   = inet6_sk_rebuild_header,
+	.conn_request	   = tcp_v6_conn_request,
+	.syn_recv_sock	   = tcp_v6_syn_recv_sock,
+	.remember_stamp	   = tcp_v6_remember_stamp,
+	.net_header_len	   = sizeof(struct ipv6hdr),
+	.setsockopt	   = ipv6_setsockopt,
+	.getsockopt	   = ipv6_getsockopt,
+	.addr2sockaddr	   = inet6_csk_addr2sockaddr,
+	.sockaddr_len	   = sizeof(struct sockaddr_in6),
+	.bind_conflict	   = inet6_csk_bind_conflict,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_ipv6_setsockopt,
+	.compat_getsockopt = compat_ipv6_getsockopt,
+#endif
+};
+
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {
+	.md5_lookup	=	tcp_v6_md5_lookup,
+	.calc_md5_hash	=	tcp_v6_md5_hash_skb,
+	.md5_add	=	tcp_v6_md5_add_func,
+	.md5_parse	=	tcp_v6_parse_md5_keys,
+};
+#endif
+
+/*
+ *	TCP over IPv4 via INET6 API
+ */
+
+static const struct inet_connection_sock_af_ops ipv6_mapped = {
+	.queue_xmit	   = ip_queue_xmit,
+	.send_check	   = tcp_v4_send_check,
+	.rebuild_header	   = inet_sk_rebuild_header,
+	.conn_request	   = tcp_v6_conn_request,
+	.syn_recv_sock	   = tcp_v6_syn_recv_sock,
+	.remember_stamp	   = tcp_v4_remember_stamp,
+	.net_header_len	   = sizeof(struct iphdr),
+	.setsockopt	   = ipv6_setsockopt,
+	.getsockopt	   = ipv6_getsockopt,
+	.addr2sockaddr	   = inet6_csk_addr2sockaddr,
+	.sockaddr_len	   = sizeof(struct sockaddr_in6),
+	.bind_conflict	   = inet6_csk_bind_conflict,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt = compat_ipv6_setsockopt,
+	.compat_getsockopt = compat_ipv6_getsockopt,
+#endif
+};
+
+#ifdef CONFIG_TCP_MD5SIG
+static const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {
+	.md5_lookup	=	tcp_v4_md5_lookup,
+	.calc_md5_hash	=	tcp_v4_md5_hash_skb,
+	.md5_add	=	tcp_v6_md5_add_func,
+	.md5_parse	=	tcp_v6_parse_md5_keys,
+};
+#endif
+
+/* NOTE: A lot of things set to zero explicitly by call to
+ *       sk_alloc() so need not be done here.
+ */
+static int tcp_v6_init_sock(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	skb_queue_head_init(&tp->out_of_order_queue);
+	tcp_init_xmit_timers(sk);
+	tcp_prequeue_init(tp);
+	INIT_LIST_HEAD(&tp->tsq_node);
+
+	icsk->icsk_rto = TCP_TIMEOUT_INIT;
+	tp->mdev = TCP_TIMEOUT_INIT;
+
+	/* So many TCP implementations out there (incorrectly) count the
+	 * initial SYN frame in their delayed-ACK and congestion control
+	 * algorithms that we must have the following bandaid to talk
+	 * efficiently to them.  -DaveM
+	 */
+	tp->snd_cwnd = 2;
+
+	/* See draft-stevens-tcpca-spec-01 for discussion of the
+	 * initialization of these values.
+	 */
+	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
+	tp->snd_cwnd_clamp = ~0;
+	tp->mss_cache = 536;
+
+	tp->reordering = sysctl_tcp_reordering;
+
+	sk->sk_state = TCP_CLOSE;
+
+	icsk->icsk_af_ops = &ipv6_specific;
+	icsk->icsk_ca_ops = &tcp_init_congestion_ops;
+	icsk->icsk_sync_mss = tcp_sync_mss;
+	sk->sk_write_space = sk_stream_write_space;
+	sock_set_flag(sk, SOCK_USE_WRITE_QUEUE);
+
+#ifdef CONFIG_TCP_MD5SIG
+	tp->af_specific = &tcp_sock_ipv6_specific;
+#endif
+
+	sk->sk_sndbuf = sysctl_tcp_wmem[1];
+	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+
+	local_bh_disable();
+	percpu_counter_inc(&tcp_sockets_allocated);
+	local_bh_enable();
+
+	return 0;
+}
+
+static void tcp_v6_destroy_sock(struct sock *sk)
+{
+#ifdef CONFIG_TCP_MD5SIG
+	/* Clean up the MD5 key list */
+	if (tcp_sk(sk)->md5sig_info)
+		tcp_v6_clear_md5_list(sk);
+#endif
+	tcp_v4_destroy_sock(sk);
+	inet6_destroy_sock(sk);
+}
+
+#ifdef CONFIG_PROC_FS
+/* Proc filesystem TCPv6 sock list dumping. */
+static void get_openreq6(struct seq_file *seq,
+			 struct sock *sk, struct request_sock *req, int i, int uid)
+{
+	int ttd = req->expires - jiffies;
+	struct in6_addr *src = &inet6_rsk(req)->loc_addr;
+	struct in6_addr *dest = &inet6_rsk(req)->rmt_addr;
+
+	if (ttd < 0)
+		ttd = 0;
+
+	seq_printf(seq,
+		   "%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X "
+		   "%02X %08X:%08X %02X:%08lX %08X %5u %8d %d %d %p\n",
+		   i,
+		   src->s6_addr32[0], src->s6_addr32[1],
+		   src->s6_addr32[2], src->s6_addr32[3],
+		   ntohs(inet_rsk(req)->loc_port),
+		   dest->s6_addr32[0], dest->s6_addr32[1],
+		   dest->s6_addr32[2], dest->s6_addr32[3],
+		   ntohs(inet_rsk(req)->rmt_port),
+		   TCP_SYN_RECV,
+		   0,0, /* could print option size, but that is af dependent. */
+		   1,   /* timers active (only the expire timer) */
+		   jiffies_to_clock_t(ttd),
+		   req->retrans,
+		   uid,
+		   0,  /* non standard timer */
+		   0, /* open_requests have no inode */
+		   0, req);
+}
+
+static void get_tcp6_sock(struct seq_file *seq, struct sock *sp, int i)
+{
+	struct in6_addr *dest, *src;
+	__u16 destp, srcp;
+	int timer_active;
+	unsigned long timer_expires;
+	struct inet_sock *inet = inet_sk(sp);
+	struct tcp_sock *tp = tcp_sk(sp);
+	const struct inet_connection_sock *icsk = inet_csk(sp);
+	struct ipv6_pinfo *np = inet6_sk(sp);
+
+	dest  = &np->daddr;
+	src   = &np->rcv_saddr;
+	destp = ntohs(inet->dport);
+	srcp  = ntohs(inet->sport);
+
+	if (icsk->icsk_pending == ICSK_TIME_RETRANS) {
+		timer_active	= 1;
+		timer_expires	= icsk->icsk_timeout;
+	} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {
+		timer_active	= 4;
+		timer_expires	= icsk->icsk_timeout;
+	} else if (timer_pending(&sp->sk_timer)) {
+		timer_active	= 2;
+		timer_expires	= sp->sk_timer.expires;
+	} else {
+		timer_active	= 0;
+		timer_expires = jiffies;
+	}
+
+	seq_printf(seq,
+		   "%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X "
+		   "%02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %p %lu %lu %u %u %d\n",
+		   i,
+		   src->s6_addr32[0], src->s6_addr32[1],
+		   src->s6_addr32[2], src->s6_addr32[3], srcp,
+		   dest->s6_addr32[0], dest->s6_addr32[1],
+		   dest->s6_addr32[2], dest->s6_addr32[3], destp,
+		   sp->sk_state,
+		   tp->write_seq-tp->snd_una,
+		   (sp->sk_state == TCP_LISTEN) ? sp->sk_ack_backlog : (tp->rcv_nxt - tp->copied_seq),
+		   timer_active,
+		   jiffies_to_clock_t(timer_expires - jiffies),
+		   icsk->icsk_retransmits,
+		   sock_i_uid(sp),
+		   icsk->icsk_probes_out,
+		   sock_i_ino(sp),
+		   atomic_read(&sp->sk_refcnt), sp,
+		   jiffies_to_clock_t(icsk->icsk_rto),
+		   jiffies_to_clock_t(icsk->icsk_ack.ato),
+		   (icsk->icsk_ack.quick << 1 ) | icsk->icsk_ack.pingpong,
+		   tp->snd_cwnd,
+		   tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh
+		   );
+}
+
+static void get_timewait6_sock(struct seq_file *seq,
+			       struct inet_timewait_sock *tw, int i)
+{
+	struct in6_addr *dest, *src;
+	__u16 destp, srcp;
+	struct inet6_timewait_sock *tw6 = inet6_twsk((struct sock *)tw);
+	int ttd = tw->tw_ttd - jiffies;
+
+	if (ttd < 0)
+		ttd = 0;
+
+	dest = &tw6->tw_v6_daddr;
+	src  = &tw6->tw_v6_rcv_saddr;
+	destp = ntohs(tw->tw_dport);
+	srcp  = ntohs(tw->tw_sport);
+
+	seq_printf(seq,
+		   "%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X "
+		   "%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p\n",
+		   i,
+		   src->s6_addr32[0], src->s6_addr32[1],
+		   src->s6_addr32[2], src->s6_addr32[3], srcp,
+		   dest->s6_addr32[0], dest->s6_addr32[1],
+		   dest->s6_addr32[2], dest->s6_addr32[3], destp,
+		   tw->tw_substate, 0, 0,
+		   3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,
+		   atomic_read(&tw->tw_refcnt), tw);
+}
+
+static int tcp6_seq_show(struct seq_file *seq, void *v)
+{
+	struct tcp_iter_state *st;
+
+	if (v == SEQ_START_TOKEN) {
+		seq_puts(seq,
+			 "  sl  "
+			 "local_address                         "
+			 "remote_address                        "
+			 "st tx_queue rx_queue tr tm->when retrnsmt"
+			 "   uid  timeout inode\n");
+		goto out;
+	}
+	st = seq->private;
+
+	switch (st->state) {
+	case TCP_SEQ_STATE_LISTENING:
+	case TCP_SEQ_STATE_ESTABLISHED:
+		get_tcp6_sock(seq, v, st->num);
+		break;
+	case TCP_SEQ_STATE_OPENREQ:
+		get_openreq6(seq, st->syn_wait_sk, v, st->num, st->uid);
+		break;
+	case TCP_SEQ_STATE_TIME_WAIT:
+		get_timewait6_sock(seq, v, st->num);
+		break;
+	}
+out:
+	return 0;
+}
+
+static struct tcp_seq_afinfo tcp6_seq_afinfo = {
+	.name		= "tcp6",
+	.family		= AF_INET6,
+	.seq_fops	= {
+		.owner		= THIS_MODULE,
+	},
+	.seq_ops	= {
+		.show		= tcp6_seq_show,
+	},
+};
+
+int tcp6_proc_init(struct net *net)
+{
+	return tcp_proc_register(net, &tcp6_seq_afinfo);
+}
+
+void tcp6_proc_exit(struct net *net)
+{
+	tcp_proc_unregister(net, &tcp6_seq_afinfo);
+}
+#endif
+
+struct proto tcpv6_prot = {
+	.name			= "TCPv6",
+	.owner			= THIS_MODULE,
+	.close			= tcp_close,
+	.connect		= tcp_v6_connect,
+	.disconnect		= tcp_disconnect,
+	.accept			= inet_csk_accept,
+	.ioctl			= tcp_ioctl,
+	.init			= tcp_v6_init_sock,
+	.destroy		= tcp_v6_destroy_sock,
+	.shutdown		= tcp_shutdown,
+	.setsockopt		= tcp_setsockopt,
+	.getsockopt		= tcp_getsockopt,
+	.recvmsg		= tcp_recvmsg,
+	.backlog_rcv		= tcp_v6_do_rcv,
+	.release_cb		= tcp_release_cb,
+	.hash			= tcp_v6_hash,
+	.unhash			= inet_unhash,
+	.get_port		= inet_csk_get_port,
+	.enter_memory_pressure	= tcp_enter_memory_pressure,
+	.sockets_allocated	= &tcp_sockets_allocated,
+	.memory_allocated	= &tcp_memory_allocated,
+	.memory_pressure	= &tcp_memory_pressure,
+	.orphan_count		= &tcp_orphan_count,
+	.sysctl_mem		= sysctl_tcp_mem,
+	.sysctl_wmem		= sysctl_tcp_wmem,
+	.sysctl_rmem		= sysctl_tcp_rmem,
+	.max_header		= MAX_TCP_HEADER,
+	.obj_size		= sizeof(struct tcp6_sock),
+	.slab_flags		= SLAB_DESTROY_BY_RCU | RHEL_EXTENDED_PROTO,
+	.twsk_prot		= &tcp6_timewait_sock_ops,
+	.rsk_prot		= &tcp6_request_sock_ops,
+	.h.hashinfo		= &tcp_hashinfo,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt	= compat_tcp_setsockopt,
+	.compat_getsockopt	= compat_tcp_getsockopt,
+#endif
+	.rhel_flags		= RHEL_PROTO_HAS_RELEASE_CB,
+};
+
+static const struct inet6_protocol tcpv6_protocol = {
+	.handler	=	tcp_v6_rcv,
+	.err_handler	=	tcp_v6_err,
+	.flags		=	INET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,
+};
+
+static struct inet_protosw tcpv6_protosw = {
+	.type		=	SOCK_STREAM,
+	.protocol	=	IPPROTO_TCP,
+	.prot		=	&tcpv6_prot,
+	.ops		=	&inet6_stream_ops,
+	.no_check	=	0,
+	.flags		=	INET_PROTOSW_PERMANENT |
+				INET_PROTOSW_ICSK,
+};
+
+static int tcpv6_net_init(struct net *net)
+{
+	return inet_ctl_sock_create(&net->ipv6.tcp_sk, PF_INET6,
+				    SOCK_RAW, IPPROTO_TCP, net);
+}
+
+static void tcpv6_net_exit(struct net *net)
+{
+	inet_ctl_sock_destroy(net->ipv6.tcp_sk);
+	inet_twsk_purge(net, &tcp_hashinfo, &tcp_death_row, AF_INET6);
+}
+
+static struct pernet_operations tcpv6_net_ops = {
+	.init = tcpv6_net_init,
+	.exit = tcpv6_net_exit,
+};
+
+int __init tcpv6_init(void)
+{
+	int ret;
+
+	ret = inet6_add_protocol(&tcpv6_protocol, IPPROTO_TCP);
+	if (ret)
+		goto out;
+
+	/* register inet6 protocol */
+	ret = inet6_register_protosw(&tcpv6_protosw);
+	if (ret)
+		goto out_tcpv6_protocol;
+
+	ret = register_pernet_subsys(&tcpv6_net_ops);
+	if (ret)
+		goto out_tcpv6_protosw;
+out:
+	return ret;
+
+out_tcpv6_protosw:
+	inet6_unregister_protosw(&tcpv6_protosw);
+out_tcpv6_protocol:
+	inet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);
+	goto out;
+}
+
+void tcpv6_exit(void)
+{
+	unregister_pernet_subsys(&tcpv6_net_ops);
+	inet6_unregister_protosw(&tcpv6_protosw);
+	inet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);
+}
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/Kconfig linux-2.6.32-642.11.1.el6.toa.x86_64/net/Kconfig
--- linux-2.6.32-642.11.1.el6.x86_64/net/Kconfig	2016-12-13 17:21:59.444074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/Kconfig	2016-12-13 17:25:05.517074987 +0800
@@ -49,6 +49,7 @@
 source "net/unix/Kconfig"
 source "net/xfrm/Kconfig"
 source "net/iucv/Kconfig"
+source "net/toa/Kconfig"
 
 config INET
 	bool "TCP/IP networking"
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/Makefile linux-2.6.32-642.11.1.el6.toa.x86_64/net/Makefile
--- linux-2.6.32-642.11.1.el6.x86_64/net/Makefile	2016-12-13 17:21:59.492074340 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/Makefile	2016-12-13 17:25:05.518074939 +0800
@@ -62,6 +62,7 @@
 obj-y				+= ieee802154/
 
 ifeq ($(CONFIG_NET),y)
+obj-$(CONFIG_TOA)               += toa/
 obj-$(CONFIG_SYSCTL)		+= sysctl_net.o
 endif
 obj-$(CONFIG_WIMAX)		+= wimax/
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/Makefile.orig linux-2.6.32-642.11.1.el6.toa.x86_64/net/Makefile.orig
--- linux-2.6.32-642.11.1.el6.x86_64/net/Makefile.orig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/Makefile.orig	2016-12-13 17:23:54.001074270 +0800
@@ -0,0 +1,68 @@
+#
+# Makefile for the linux networking.
+#
+# 2 Sep 2000, Christoph Hellwig <hch@infradead.org>
+# Rewritten to use lists instead of if-statements.
+#
+
+obj-y	:= nonet.o
+
+obj-$(CONFIG_NET)		:= socket.o core/
+
+tmp-$(CONFIG_COMPAT) 		:= compat.o
+obj-$(CONFIG_NET)		+= $(tmp-y)
+
+# LLC has to be linked before the files in net/802/
+obj-$(CONFIG_LLC)		+= llc/
+obj-$(CONFIG_NET)		+= ethernet/ 802/ sched/ netlink/
+obj-$(CONFIG_NETFILTER)		+= netfilter/
+obj-$(CONFIG_INET)		+= ipv4/
+obj-$(CONFIG_XFRM)		+= xfrm/
+obj-$(CONFIG_UNIX)		+= unix/
+ifneq ($(CONFIG_IPV6),)
+obj-y				+= ipv6/
+endif
+obj-$(CONFIG_PACKET)		+= packet/
+obj-$(CONFIG_NET_KEY)		+= key/
+obj-$(CONFIG_BRIDGE)		+= bridge/
+obj-$(CONFIG_NET_DSA)		+= dsa/
+obj-$(CONFIG_IPX)		+= ipx/
+obj-$(CONFIG_ATALK)		+= appletalk/
+obj-$(CONFIG_WAN_ROUTER)	+= wanrouter/
+obj-$(CONFIG_X25)		+= x25/
+obj-$(CONFIG_LAPB)		+= lapb/
+obj-$(CONFIG_NETROM)		+= netrom/
+obj-$(CONFIG_ROSE)		+= rose/
+obj-$(CONFIG_AX25)		+= ax25/
+obj-$(CONFIG_CAN)		+= can/
+obj-$(CONFIG_IRDA)		+= irda/
+obj-$(CONFIG_BT)		+= bluetooth/
+obj-$(CONFIG_SUNRPC)		+= sunrpc/
+obj-$(CONFIG_AF_RXRPC)		+= rxrpc/
+obj-$(CONFIG_ATM)		+= atm/
+obj-$(CONFIG_DECNET)		+= decnet/
+obj-$(CONFIG_ECONET)		+= econet/
+obj-$(CONFIG_PHONET)		+= phonet/
+ifneq ($(CONFIG_VLAN_8021Q),)
+obj-y				+= 8021q/
+endif
+obj-$(CONFIG_IP_DCCP)		+= dccp/
+obj-$(CONFIG_IP_SCTP)		+= sctp/
+obj-$(CONFIG_RDS)		+= rds/
+obj-y				+= wireless/
+obj-$(CONFIG_MAC80211)		+= mac80211/
+obj-$(CONFIG_TIPC)		+= tipc/
+obj-$(CONFIG_NETLABEL)		+= netlabel/
+obj-$(CONFIG_IUCV)		+= iucv/
+obj-$(CONFIG_RFKILL)		+= rfkill/
+obj-$(CONFIG_NET_9P)		+= 9p/
+ifneq ($(CONFIG_DCB),)
+obj-y				+= dcb/
+endif
+obj-y				+= ieee802154/
+
+ifeq ($(CONFIG_NET),y)
+obj-$(CONFIG_SYSCTL)		+= sysctl_net.o
+endif
+obj-$(CONFIG_WIMAX)		+= wimax/
+obj-$(CONFIG_OPENVSWITCH)	+= openvswitch/
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/toa/Kconfig linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/Kconfig
--- linux-2.6.32-642.11.1.el6.x86_64/net/toa/Kconfig	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/Kconfig	2016-12-13 17:25:05.519074892 +0800
@@ -0,0 +1,9 @@
+config	TOA
+	tristate "The private TCP option for support Taobao LVS full-NAT feature"
+	default m
+	depends on HOOKERS
+	---help---
+	  This option saves the original IP address and source port of a TCP segment
+	  after LVS performed NAT on it. So far, this module supports IPv4 and IPv6.
+
+	  Say m if unsure.
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/toa/Makefile linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/Makefile
--- linux-2.6.32-642.11.1.el6.x86_64/net/toa/Makefile	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/Makefile	2016-12-13 17:25:05.519074892 +0800
@@ -0,0 +1,4 @@
+#
+# Makefile for TOA module.
+#
+obj-$(CONFIG_TOA) += toa.o
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/toa/toa.c linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/toa.c
--- linux-2.6.32-642.11.1.el6.x86_64/net/toa/toa.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/toa.c	2016-12-13 17:25:05.520074850 +0800
@@ -0,0 +1,393 @@
+#include "toa.h"
+
+/*
+ *	TOA: Address is a new TCP Option
+ *	Address include ip+port, Now support IPv4/IPv6
+ */
+
+
+/*
+ * Statistics of toa in proc /proc/net/toa_stats
+ */
+
+struct toa_stats_entry toa_stats[] = {
+	TOA_STAT_ITEM("syn_recv_sock_toa", SYN_RECV_SOCK_TOA_CNT),
+	TOA_STAT_ITEM("syn_recv_sock_no_toa", SYN_RECV_SOCK_NO_TOA_CNT),
+	TOA_STAT_ITEM("getname_toa_ok_v4", GETNAME_TOA_OK_CNT_V4),
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	TOA_STAT_ITEM("getname_toa_ok_v6", GETNAME_TOA_OK_CNT_V6),
+	TOA_STAT_ITEM("getname_toa_ok_mapped", GETNAME_TOA_OK_CNT_MAPPED),
+#endif
+	TOA_STAT_ITEM("getname_toa_mismatch", GETNAME_TOA_MISMATCH_CNT),
+	TOA_STAT_ITEM("getname_toa_bypass", GETNAME_TOA_BYPASS_CNT),
+	TOA_STAT_ITEM("getname_toa_empty", GETNAME_TOA_EMPTY_CNT),
+	TOA_STAT_END
+};
+
+struct toa_stat_mib *ext_stats;
+/*
+ * Funcs for toa hooks
+ */
+
+/* Parse TCP options in skb, try to get client ip, port
+ * @param skb [in] received skb, it should be a ack/get-ack packet.
+ * @return NULL if we don't get client ip/port;
+ *         value of toa_data in ret_ptr if we get client ip/port.
+ */
+static int get_toa_data(struct sk_buff *skb, void *sk_toa_data)
+{
+	struct tcphdr *th;
+	int length;
+	unsigned char *ptr;
+
+	struct toa_data *tdata;
+
+	TOA_DBG("get_toa_data called\n");
+
+	if (NULL != skb) {
+		th = tcp_hdr(skb);
+		length = (th->doff * 4) - sizeof(struct tcphdr);
+		ptr = (unsigned char *) (th + 1);
+
+		while (length > 0) {
+			int opcode = *ptr++;
+			int opsize;
+			switch (opcode) {
+			case TCPOPT_EOL:
+				return 0;
+			case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+				length--;
+				continue;
+			default:
+				opsize = *ptr++;
+				if (opsize < 2)	/* "silly options" */
+					return 0;
+				if (opsize > length)
+					/* don't parse partial options */
+					return 0;
+				if ((TCPOPT_TOA == opcode &&
+						TCPOLEN_TOA == opsize)) {
+					memset(sk_toa_data, 0,
+							sizeof(struct toa_data));
+					memcpy(sk_toa_data, ptr - 2,
+							TCPOLEN_TOA);
+					tdata = (struct toa_data*)sk_toa_data;
+					TOA_DBG("find toa data: ip = " \
+						"%u.%u.%u.%u, port = %u\n",
+						NIPQUAD(tdata->ip),
+						ntohs(tdata->port));
+					return 1;
+				}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+				else if (TCPOPT_TOA_V6 == opcode &&
+						TCPOLEN_TOA_V6 == opsize){
+					memset(sk_toa_data, 0,
+							sizeof(struct toa_data));
+					memcpy(sk_toa_data, ptr - 2,
+							TCPOLEN_TOA_V6);
+					tdata = (struct toa_data*)sk_toa_data;
+					TOA_DBG("find toa data: ipv6 = " \
+						"%pI6, port = %u\n",
+						&tdata->in6,
+						ntohs(tdata->port));
+					return 1;
+				}
+#endif
+				ptr += opsize - 2;
+				length -= opsize;
+			}
+		}
+	}
+	return 0;
+}
+
+/* get client ip from socket
+ * @param sock [in] the socket to getpeername() or getsockname()
+ * @param uaddr [out] the place to put client ip, port
+ * @param uaddr_len [out] lenth of @uaddr
+ * @peer [in] if(peer), try to get remote address; if(!peer),
+ *  try to get local address
+ * @return return what the original inet_getname() returns.
+ */
+static int
+inet_getname_toa(struct socket *sock, struct sockaddr *uaddr,
+		int *uaddr_len, int peer, int *p_retval)
+{
+	int retval = *p_retval;
+	struct sock *sk = sock->sk;
+	struct sockaddr_in *sin = (struct sockaddr_in *) uaddr;
+	struct toa_data tdata;
+
+	TOA_DBG("inet_getname_toa called\n");
+
+	/* set our value if need */
+	if (retval == 0 && peer) {
+		memcpy(&tdata, sk->sk_toa_data, sizeof(tdata));
+		if (TCPOPT_TOA == tdata.opcode &&
+		    TCPOLEN_TOA == tdata.opsize) {
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_V4);
+			TOA_DBG("inet_getname_toa: set new sockaddr, " \
+				"ip %u.%u.%u.%u -> %u.%u.%u.%u, port "
+				"%u -> %u\n",
+				NIPQUAD(sin->sin_addr.s_addr),
+				NIPQUAD(tdata.ip), ntohs(sin->sin_port),
+				ntohs(tdata.port));
+				sin->sin_port = tdata.port;
+				sin->sin_addr.s_addr = tdata.ip;
+		} else { /* doesn't belong to us */
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_MISMATCH_CNT);
+			TOA_DBG("inet_getname_toa: invalid toa data, " \
+				"ip %u.%u.%u.%u port %u opcode %u "
+				"opsize %u\n",
+				NIPQUAD(tdata.ip), ntohs(tdata.port),
+				tdata.opcode, tdata.opsize);
+		}
+	} else { /* no need to get client ip */
+		TOA_INC_STATS(ext_stats, GETNAME_TOA_EMPTY_CNT);
+	}
+
+	return retval;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static int
+inet6_getname_toa(struct socket *sock, struct sockaddr *uaddr,
+		  int *uaddr_len, int peer, int *p_retval)
+{
+	int retval = *p_retval;
+	struct sock *sk = sock->sk;
+	struct sockaddr_in6 *sin = (struct sockaddr_in6 *) uaddr;
+	struct toa_data tdata;
+
+	TOA_DBG("inet6_getname_toa called\n");
+
+	/* set our value if need */
+	if (retval == 0 && peer) {
+		memcpy(&tdata, sk->sk_toa_data, sizeof(tdata));
+		if (TCPOPT_TOA_V6 == tdata.opcode &&
+				TCPOLEN_TOA_V6 == tdata.opsize){
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_V6);
+			sin->sin6_port = tdata.port;
+			sin->sin6_addr = tdata.in6;
+			TOA_DBG("inet6_getname_toa: ipv6 = " \
+						"%pI6, port = %u\n",
+						&sin->sin6_addr,
+						ntohs(sin->sin6_port));
+		}else if (TCPOPT_TOA == tdata.opcode &&
+				TCPOLEN_TOA == tdata.opsize) {
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_OK_CNT_MAPPED);
+			sin->sin6_port = tdata.port;
+			ipv6_addr_set(&sin->sin6_addr, 0, 0,
+					htonl(0x0000FFFF), tdata.ip);
+			TOA_DBG("inet6_getname_toa: ipv6_mapped = " \
+						"%pI6, port = %u\n",
+						&sin->sin6_addr,
+						ntohs(sin->sin6_port));
+		} else { /* doesn't belong to us */
+			TOA_INC_STATS(ext_stats, GETNAME_TOA_MISMATCH_CNT);
+		}
+	} else { /* no need to get client ip */
+		TOA_INC_STATS(ext_stats, GETNAME_TOA_EMPTY_CNT);
+	}
+
+	return retval;
+}
+#endif
+
+/* The three way handshake has completed - we got a valid synack -
+ * now create the new socket.
+ * We need to save toa data into the new socket.
+ * @param sk [out]  the socket
+ * @param skb [in] the ack/ack-get packet
+ * @param req [in] the open request for this connection
+ * @param dst [out] route cache entry
+ * @return NULL if fail new socket if succeed.
+ */
+static struct sock *
+tcp_v4_syn_recv_sock_toa(struct sock *sk, struct sk_buff *skb,
+			struct request_sock *req, struct dst_entry *dst,
+						struct sock **p_newsock)
+{
+	struct sock *newsock = *p_newsock;
+
+	TOA_DBG("tcp_v4_syn_recv_sock_toa called\n");
+
+	if (!sk || !skb)
+		return NULL;
+
+	/* set our value if need */
+	if (NULL != newsock) {
+		if (get_toa_data(skb, newsock->sk_toa_data))
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_TOA_CNT);
+		else
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_NO_TOA_CNT);
+		TOA_DBG("tcp_v4_syn_recv_sock_toa: set " \
+			"sk->sk_toa_data\n");
+	}
+	return newsock;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static struct sock *
+tcp_v6_syn_recv_sock_toa(struct sock *sk, struct sk_buff *skb,
+			 struct request_sock *req, struct dst_entry *dst,
+					struct sock **p_newsock)
+{
+	struct sock *newsock = *p_newsock;
+
+	TOA_DBG("tcp_v6_syn_recv_sock_toa called\n");
+
+	if (!sk || !skb)
+		return NULL;
+
+	/* set our value if need */
+	if (NULL != newsock) {
+		if (get_toa_data(skb, newsock->sk_toa_data))
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_TOA_CNT);
+		else
+			TOA_INC_STATS(ext_stats, SYN_RECV_SOCK_NO_TOA_CNT);
+	}
+	return newsock;
+}
+#endif
+
+/*
+ * HOOK FUNCS
+ */
+
+static struct hooker inet_getname_hooker = {
+	.func = inet_getname_toa,
+};
+
+static struct hooker inet_tcp_hooker = {
+	.func = tcp_v4_syn_recv_sock_toa,
+};
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static struct hooker inet6_getname_hooker = {
+	.func = inet6_getname_toa,
+};
+
+static struct hooker inet6_tcp_hooker = {
+	.func = tcp_v6_syn_recv_sock_toa,
+};
+#endif
+
+/* replace the functions with our functions */
+static inline int
+hook_toa_functions(void)
+{
+	int ret;
+
+	ret = hooker_install(&inet_stream_ops.getname, &inet_getname_hooker);
+	ret |= hooker_install(&ipv4_specific.syn_recv_sock, &inet_tcp_hooker);
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	ret |= hooker_install(&inet6_stream_ops.getname, &inet6_getname_hooker);
+	ret |= hooker_install(&ipv6_specific.syn_recv_sock, &inet6_tcp_hooker);
+#endif
+	return ret;
+}
+
+/* replace the functions to original ones */
+static void
+unhook_toa_functions(void)
+{
+	hooker_uninstall(&inet_getname_hooker);
+	hooker_uninstall(&inet_tcp_hooker);
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	hooker_uninstall(&inet6_getname_hooker);
+	hooker_uninstall(&inet6_tcp_hooker);
+#endif
+}
+
+/*
+ * Statistics of toa in proc /proc/net/toa_stats
+ */
+static int toa_stats_show(struct seq_file *seq, void *v)
+{
+	int i, j, cpu_nr;
+
+	/* print CPU first */
+	seq_printf(seq, "                                  ");
+	cpu_nr = num_possible_cpus();
+	for (i = 0; i < cpu_nr; i++)
+		if (cpu_online(i))
+			seq_printf(seq, "CPU%d       ", i);
+	seq_putc(seq, '\n');
+
+	i = 0;
+	while (NULL != toa_stats[i].name) {
+		seq_printf(seq, "%-25s:", toa_stats[i].name);
+		for (j = 0; j < cpu_nr; j++) {
+			if (cpu_online(j)) {
+				seq_printf(seq, "%10lu ", *(
+					((unsigned long *) per_cpu_ptr(
+					ext_stats, j)) + toa_stats[i].entry
+					));
+			}
+		}
+		seq_putc(seq, '\n');
+		i++;
+	}
+	return 0;
+}
+
+static int toa_stats_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, toa_stats_show, NULL);
+}
+
+static const struct file_operations toa_stats_fops = {
+	.owner = THIS_MODULE,
+	.open = toa_stats_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * TOA module init and destory
+ */
+
+/* module init */
+static int __init
+toa_init(void)
+{
+	/* alloc statistics array for toa */
+	ext_stats = alloc_percpu(struct toa_stat_mib);
+	if (!ext_stats)
+		return -ENOMEM;
+
+	if (!proc_net_fops_create(&init_net, "toa_stats", 0, &toa_stats_fops)) {
+		TOA_INFO("cannot create procfs /proc/net/toa_stats.\n");
+		goto err_percpu;
+	}
+
+	/* hook funcs for parse and get toa */
+	if (hook_toa_functions())
+		goto err_proc;
+
+	return 0;
+
+err_proc:
+	proc_net_remove(&init_net, "toa_stats");
+err_percpu:
+	free_percpu(ext_stats);
+	return -ENODEV;
+}
+
+/* module cleanup*/
+static void __exit
+toa_exit(void)
+{
+	unhook_toa_functions();
+
+	proc_net_remove(&init_net, "toa_stats");
+	free_percpu(ext_stats);
+}
+
+module_init(toa_init);
+module_exit(toa_exit);
+MODULE_LICENSE("GPL");
diff -uNr linux-2.6.32-642.11.1.el6.x86_64/net/toa/toa.h linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/toa.h
--- linux-2.6.32-642.11.1.el6.x86_64/net/toa/toa.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-642.11.1.el6.toa.x86_64/net/toa/toa.h	2016-12-13 17:25:05.520074850 +0800
@@ -0,0 +1,105 @@
+#ifndef __NET__TOA_H__
+#define __NET__TOA_H__
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/err.h>
+#include <linux/time.h>
+#include <linux/skbuff.h>
+#include <net/tcp.h>
+#include <net/inet_common.h>
+#include <linux/uaccess.h>
+#include <linux/netdevice.h>
+#include <net/net_namespace.h>
+#include <linux/fs.h>
+#include <linux/sysctl.h>
+#include <linux/proc_fs.h>
+#include <linux/kallsyms.h>
+
+#include <linux/hookers.h>
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#include <net/ipv6.h>
+#include <net/transp_v6.h>
+#endif
+
+#define TOA_VERSION "1.0.0.2"
+
+#ifdef TOA_DEBUG
+#define TOA_DBG(msg...)				\
+	do {					\
+		printk(KERN_DEBUG "[DEBUG] TOA: " msg); \
+	} while (0)
+#else
+#define TOA_DBG(msg...)
+#endif
+
+#define TOA_INFO(msg...)				\
+	do {						\
+		if (net_ratelimit())			\
+			printk(KERN_INFO "TOA: " msg);	\
+	} while (0)
+
+#define TCPOPT_TOA  254
+
+/* MUST be 4n !!!! */
+#define TCPOLEN_TOA 8		/* |opcode|size|ip+port| = 1 + 1 + 6 */
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#define TCPOPT_TOA_V6	253
+#define TCPOLEN_TOA_V6	20	/* |opcode|size|port|ipv6| = 1 + 1 + 2 + 16 */
+#endif
+
+/* MUST be 4 bytes alignment */
+struct toa_data {
+	__u8 opcode;
+	__u8 opsize;
+	__be16 port;
+	union {
+		__be32 ip;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		struct in6_addr in6;
+#endif
+	};
+};
+
+/* statistics about toa in proc /proc/net/toa_stat */
+enum {
+	SYN_RECV_SOCK_TOA_CNT = 1,
+	SYN_RECV_SOCK_NO_TOA_CNT,
+	GETNAME_TOA_OK_CNT_V4,
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	GETNAME_TOA_OK_CNT_V6,
+	GETNAME_TOA_OK_CNT_MAPPED,
+#endif
+	GETNAME_TOA_MISMATCH_CNT,
+	GETNAME_TOA_BYPASS_CNT,
+	GETNAME_TOA_EMPTY_CNT,
+	TOA_STAT_LAST
+};
+
+struct toa_stats_entry {
+	char *name;
+	int entry;
+};
+
+#define TOA_STAT_ITEM(_name, _entry) { \
+	.name = _name,		\
+	.entry = _entry,	\
+}
+
+#define TOA_STAT_END {	\
+	NULL,		\
+	0,		\
+}
+
+struct toa_stat_mib {
+	unsigned long mibs[TOA_STAT_LAST];
+};
+
+#define TOA_INC_STATS(mib, field)         \
+	(per_cpu_ptr(mib, smp_processor_id())->mibs[field]++)
+
+#endif
